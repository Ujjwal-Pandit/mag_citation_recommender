{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YvEvhy9LFcPB",
    "outputId": "c28ad4b1-3b6b-4ade-dee2-e9d297da8086"
   },
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "# import subprocess; subprocess.run(['pip', 'install', 'torch-geometric', 'sentence-transformers', 'tqdm'], check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "a_DZSAgyGMsP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import HeteroConv, GCNConv, SAGEConv, Linear\n",
    "from torch_geometric.utils import train_test_split_edges, negative_sampling\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "GNN_config = {\n",
    "    'epoch': 10,\n",
    "    'batch': 128,\n",
    "    'dropout': 0.2,\n",
    "    'early_stopping': 2,\n",
    "    'learning_rate': 1e-3\n",
    "}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BziXeQtCNmzg",
    "outputId": "35d05389-3b78-4f60-a691-09d4beaacf11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: /home/upandit/mag_citation_recommender/data\n",
      "Checkpoints: /home/upandit/mag_citation_recommender/checkpoints\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "base_dir = Path('.')\n",
    "data_dir = base_dir / 'data'\n",
    "checkpoints_dir = base_dir / 'checkpoints'\n",
    "\n",
    "# Fix nested directory structure if files were transferred from Turing\n",
    "if (base_dir / 'data' / 'data').exists():\n",
    "    print(\"Moving files from data/data/ to data/...\")\n",
    "    for item in (base_dir / 'data' / 'data').iterdir():\n",
    "        if item.is_file():\n",
    "            shutil.move(str(item), str(data_dir / item.name))\n",
    "    print(\"[OK] Done\")\n",
    "\n",
    "if (base_dir / 'checkpoints' / 'checkpoints').exists():\n",
    "    print(\"Moving files from checkpoints/checkpoints/ to checkpoints/...\")\n",
    "    for item in (base_dir / 'checkpoints' / 'checkpoints').iterdir():\n",
    "        if item.is_file():\n",
    "            shutil.move(str(item), str(checkpoints_dir / item.name))\n",
    "    print(\"[OK] Done\")\n",
    "\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "checkpoints_dir.mkdir(exist_ok=True)\n",
    "print(f\"Data: {data_dir.absolute()}\\nCheckpoints: {checkpoints_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G2TALZbfLWu_",
    "outputId": "0635df32-989c-45bd-d035-e31f576e7974"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset files...\n",
      "⊙ train.txt.zip.001 (exists)\n",
      "⊙ train.txt.zip.002 (exists)\n",
      "⊙ test.txt (exists)\n",
      "⊙ val.txt (exists)\n",
      "Download complete!\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "urls = {\n",
    "    'train.txt.zip.001': 'https://github.com/QianWangWPI/Released-Microsoft-dataset/raw/main/train.txt.zip.001',\n",
    "    'train.txt.zip.002': 'https://github.com/QianWangWPI/Released-Microsoft-dataset/raw/main/train.txt.zip.002',\n",
    "    'test.txt': 'https://raw.githubusercontent.com/QianWangWPI/Released-Microsoft-dataset/refs/heads/main/test.txt',\n",
    "    'val.txt': 'https://raw.githubusercontent.com/QianWangWPI/Released-Microsoft-dataset/refs/heads/main/val.txt'\n",
    "}\n",
    "\n",
    "print(\"Downloading dataset files...\")\n",
    "for filename, url in urls.items():\n",
    "    filepath = data_dir / filename\n",
    "    if not filepath.exists():\n",
    "        urllib.request.urlretrieve(url, filepath)\n",
    "        print(f\"[OK] {filename}\")\n",
    "    else:\n",
    "        print(f\"⊙ {filename} (exists)\")\n",
    "\n",
    "print(\"Download complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_k1Hd2vkLkLb",
    "outputId": "66bd566e-de0a-4047-bb2b-9d1b2cb9406e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.txt already exists\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import glob\n",
    "\n",
    "train_txt_path = data_dir / \"train.txt\"\n",
    "train_zip_path = data_dir / \"train.txt.zip\"\n",
    "\n",
    "if not train_txt_path.exists():\n",
    "    zip_parts = sorted(glob.glob(str(data_dir / \"train.txt.zip.*\")))\n",
    "    if zip_parts:\n",
    "        print(\"Combining zip parts...\")\n",
    "        with open(train_zip_path, 'wb') as outfile:\n",
    "            for part in zip_parts:\n",
    "                with open(part, 'rb') as infile:\n",
    "                    outfile.write(infile.read())\n",
    "        print(\"[OK] Combined\")\n",
    "    \n",
    "    if train_zip_path.exists():\n",
    "        print(\"Extracting...\")\n",
    "        with zipfile.ZipFile(train_zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(data_dir)\n",
    "        print(\"[OK] Extracted\")\n",
    "else:\n",
    "    print(\"train.txt already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I4ySpwb2OXCv",
    "outputId": "2aa18895-11a2-4cb6-a27d-37274d2eb343"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 42000, Val: 9000, Test: 9000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(data_dir / \"train.txt\", encoding=\"utf-8\") as f:\n",
    "    train_data = json.load(f)\n",
    "with open(data_dir / \"val.txt\", encoding=\"utf-8\") as f:\n",
    "    val_data = json.load(f)\n",
    "with open(data_dir / \"test.txt\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "16WFCPlZOmmz",
    "outputId": "9f63c64a-0af7-44da-ad85-16793fcf0c62"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['publication_ID', 'Citations', 'pubDate', 'language', 'title', 'journal', 'abstract', 'keywords', 'authors', 'venue', 'doi'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "HKutAA3VOfHq",
    "outputId": "f27e859e-ab99-4807-a377-9ec24695ecdb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publication_ID</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>Citations</th>\n",
       "      <th>author_ids</th>\n",
       "      <th>keyword_list</th>\n",
       "      <th>venue</th>\n",
       "      <th>pubDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23641233</td>\n",
       "      <td>Local functional connectivity as a pre surgica...</td>\n",
       "      <td>Local functional connectivity as a pre surgica...</td>\n",
       "      <td>[23847586, 24073391, 26204264]</td>\n",
       "      <td>[560cdf1545ce1e5960a19851, 560cdf1545ce1e5960a...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>Frontiers in neurology', 'id': '5451a5cae0cf0b...</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17157189</td>\n",
       "      <td>Heat shock response and acute lung injury</td>\n",
       "      <td>Heat shock response and acute lung injury. All...</td>\n",
       "      <td>[20465849, 23536968, 24524071, 22140545, 21543...</td>\n",
       "      <td>[53f4442cdabfaee43ec75166, 54096d37dabfae8faa6...</td>\n",
       "      <td>[animals, heat-shock proteins, genetics, metab...</td>\n",
       "      <td>Free Radical Biology and Medicine', 'id': '545...</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15872007</td>\n",
       "      <td>STa and cGMP stimulate CFTR translocation to t...</td>\n",
       "      <td>STa and cGMP stimulate CFTR translocation to t...</td>\n",
       "      <td>[21347269, 22069681, 24275951, 21347269, 22069...</td>\n",
       "      <td>[53f467c8dabfaeecd6a126b7, 5608ce0745cedb3396d...</td>\n",
       "      <td>[animals, bacterial toxins, pharmacology, biot...</td>\n",
       "      <td>American journal of physiology. Cell physiolog...</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20360276</td>\n",
       "      <td>Stigma and depression treatment utilization am...</td>\n",
       "      <td>Stigma and depression treatment utilization am...</td>\n",
       "      <td>[27473569, 26576680, 24938081, 28774339, 29536...</td>\n",
       "      <td>[53f42d05dabfaedf43511829, 53f4263edabfaeb2acf...</td>\n",
       "      <td>[adolescent, adult, aged, antidepressive agent...</td>\n",
       "      <td>Psychiatric services (Washington, D.C.)</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15963034</td>\n",
       "      <td>Increased incidence and severity of diabetic k...</td>\n",
       "      <td>Increased incidence and severity of diabetic k...</td>\n",
       "      <td>[31086620, 24355514, 34188679, 35511179]</td>\n",
       "      <td>[53f43fb3dabfaee4dc7be511, 53f4d409dabfaeedd17...</td>\n",
       "      <td>[adolescent, child, child, preschool, colorado...</td>\n",
       "      <td>Pediatric Diabetes', 'id': '5451a5c4e0cf0b02b5...</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publication_ID                                              title  \\\n",
       "0        23641233  Local functional connectivity as a pre surgica...   \n",
       "1        17157189          Heat shock response and acute lung injury   \n",
       "2        15872007  STa and cGMP stimulate CFTR translocation to t...   \n",
       "3        20360276  Stigma and depression treatment utilization am...   \n",
       "4        15963034  Increased incidence and severity of diabetic k...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Local functional connectivity as a pre surgica...   \n",
       "1  Heat shock response and acute lung injury. All...   \n",
       "2  STa and cGMP stimulate CFTR translocation to t...   \n",
       "3  Stigma and depression treatment utilization am...   \n",
       "4  Increased incidence and severity of diabetic k...   \n",
       "\n",
       "                                           Citations  \\\n",
       "0                     [23847586, 24073391, 26204264]   \n",
       "1  [20465849, 23536968, 24524071, 22140545, 21543...   \n",
       "2  [21347269, 22069681, 24275951, 21347269, 22069...   \n",
       "3  [27473569, 26576680, 24938081, 28774339, 29536...   \n",
       "4           [31086620, 24355514, 34188679, 35511179]   \n",
       "\n",
       "                                          author_ids  \\\n",
       "0  [560cdf1545ce1e5960a19851, 560cdf1545ce1e5960a...   \n",
       "1  [53f4442cdabfaee43ec75166, 54096d37dabfae8faa6...   \n",
       "2  [53f467c8dabfaeecd6a126b7, 5608ce0745cedb3396d...   \n",
       "3  [53f42d05dabfaedf43511829, 53f4263edabfaeb2acf...   \n",
       "4  [53f43fb3dabfaee4dc7be511, 53f4d409dabfaeedd17...   \n",
       "\n",
       "                                        keyword_list  \\\n",
       "0                                                [0]   \n",
       "1  [animals, heat-shock proteins, genetics, metab...   \n",
       "2  [animals, bacterial toxins, pharmacology, biot...   \n",
       "3  [adolescent, adult, aged, antidepressive agent...   \n",
       "4  [adolescent, child, child, preschool, colorado...   \n",
       "\n",
       "                                               venue  pubDate  \n",
       "0  Frontiers in neurology', 'id': '5451a5cae0cf0b...     2013  \n",
       "1  Free Radical Biology and Medicine', 'id': '545...     2007  \n",
       "2  American journal of physiology. Cell physiolog...     2005  \n",
       "3            Psychiatric services (Washington, D.C.)     2010  \n",
       "4  Pediatric Diabetes', 'id': '5451a5c4e0cf0b02b5...     2005  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def preprocess(data):\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Split citations into list\n",
    "    df[\"Citations\"] = df[\"Citations\"].apply(lambda x: x.split(\";\") if isinstance(x, str) else [])\n",
    "\n",
    "    df[\"title\"] = df[\"title\"].astype(str)\n",
    "    df[\"abstract\"] = df[\"abstract\"].astype(str)\n",
    "    df[\"keywords\"] = df[\"keywords\"].astype(str)\n",
    "    # Extract doi for reporting\n",
    "    df[\"doi\"] = df[\"doi\"].apply(lambda x: x.strip() if isinstance(x, str) else None)\n",
    "\n",
    "    # Combine title + abstract + keywords as paper text\n",
    "    df[\"text\"] = df[\"title\"] + \". \" + df[\"abstract\"] + \". \" + df[\"keywords\"] + '. doi:' + df['doi']\n",
    "\n",
    "    # Extract authors as list of IDs\n",
    "    df[\"author_ids\"] = df[\"authors\"].apply(lambda authors: [a.get('id', a.get('name')) for a in authors] if isinstance(authors, list) else [])\n",
    "\n",
    "\n",
    "    # Extract keywords as lowercase list\n",
    "    df[\"keyword_list\"] = df[\"keywords\"].apply(lambda x: [kw.strip().lower() for kw in x.split(';')] if isinstance(x, str) else [])\n",
    "\n",
    "    # Extract venues as lowercase list (will be node)\n",
    "    df[\"venue\"] = df[\"venue\"].apply(lambda x: x.split(\"name':\")[-1].strip()[1:-2] if isinstance(x, str) else None)\n",
    "\n",
    "    # Extract pub date for attributes\n",
    "    df[\"pubDate\"] = df[\"pubDate\"].apply(lambda x: x.strip() if isinstance(x, str) else None)\n",
    "\n",
    "    # Comment out the following if we want to consider the full date.\n",
    "    # My reasoning for only using the year is that it compresses the graph down significantly if multiple papers share a year node. May cause more noise tho\n",
    "    df[\"pubDate\"] = df[\"pubDate\"].apply(lambda date: int(date.split(' ')[0]) if date[0].isdigit() else None)\n",
    "\n",
    "    return df[[\"publication_ID\", \"title\", \"text\", \"Citations\", \"author_ids\", \"keyword_list\", \"venue\", \"pubDate\"]] #Added title for inference later\n",
    "\n",
    "train_df = preprocess(train_data)\n",
    "val_df = preprocess(val_data)\n",
    "test_df = preprocess(test_data)\n",
    "\n",
    "train_df.head(2)\n",
    "val_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kmX3yPCBe-3q",
    "outputId": "9e657279-e53c-4d00-bbd9-89ab787999a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#papers=424279 #authors=307266 #keywords=17207 #venues=5501 #pubDates=35\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "all_df = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
    "\n",
    "# All papers (in data or cited)\n",
    "all_papers = set(all_df['publication_ID'].astype(str))\n",
    "for cits in all_df['Citations']:\n",
    "    all_papers.update(cits)\n",
    "paper2idx = {pid: i for i, pid in enumerate(sorted(all_papers))}\n",
    "\n",
    "# All authors\n",
    "all_authors = set(itertools.chain.from_iterable(all_df['author_ids']))\n",
    "# Filter out None values before sorting\n",
    "all_authors = {aid for aid in all_authors if aid is not None}\n",
    "author2idx = {aid: i for i, aid in enumerate(sorted(all_authors))}\n",
    "\n",
    "# All keywords\n",
    "all_keywords = set(itertools.chain.from_iterable(all_df['keyword_list']))\n",
    "keyword2idx = {kw: i for i, kw in enumerate(sorted(all_keywords))}\n",
    "\n",
    "# All venues\n",
    "all_venues = set(all_df['venue'].dropna())\n",
    "venue2idx = {v: i for i, v in enumerate(sorted(all_venues))}\n",
    "\n",
    "# All pub dates\n",
    "all_pubDates = set(all_df['pubDate'].dropna())\n",
    "pubDate2idx = {pd: i for i, pd in enumerate(sorted(all_pubDates))}\n",
    "\n",
    "print(f\"#papers={len(paper2idx)} #authors={len(author2idx)} #keywords={len(keyword2idx)} #venues={len(venue2idx)} #pubDates={len(pubDate2idx)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_AsFvZgFqqF3"
   },
   "source": [
    "# Upgrading the graph where paper node features come from real text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8QEe92r0qzX3"
   },
   "outputs": [],
   "source": [
    "# Install sentence-transformers if needed (uncomment if required)\n",
    "# import subprocess; subprocess.run(['pip', 'install', '-q', 'sentence-transformers'], check=True)\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369,
     "referenced_widgets": [
      "0360e5b4587d498ca89b86ff2e2627eb",
      "1937e59c1cc544dea15e8be6e9ec1ca1",
      "fc99d2bf92a2458eac7fb4980cd5f757",
      "3be7f5eaca0544388784367f2ce1a060",
      "6a9bba08586b494694abab655f49206c",
      "72a54e97a5054b709bcbf72f4dd85fe3",
      "ab619af6632945939eebf8be0338b8e0",
      "e3ffc470112c4727991e2a8bb004dd0f",
      "fb21fa57fba941dda7d460f27343d7dd",
      "37887e11ef214661bd6f74a32766274b",
      "a1056d170b63463db495247c0679851f",
      "eb91300c22514b27b5350e6808460b52",
      "1f97fc92dfcc436ebad18f9e1d4f016a",
      "25c406ed33ed4cf49116cb20ddb50256",
      "614c75ab42f44852aa81a85b2236c92c",
      "b837acaa30664304ae8786d38d794c84",
      "24182075e4974186a6bed5072f67601c",
      "4e845b095fa04a8e8f5017af6f1fbee4",
      "fe2e3ea421364debbc32539439311f8f",
      "3e7d07734800439c9798b91545b94004",
      "24f5e74118dc40d5a6f0371a447b8bce",
      "c6cf4ef1267d47d09e2d20d9894e63fb",
      "d714eaa9296d461ab37f0e3e69c65bc0",
      "7f726a871fcf40a78160fe8e3964c063",
      "b4341b18cee846bcb73ca665705464cf",
      "20aadd9f73464590ad8eb87f9b3830f8",
      "0b6c70e751cf457ea318988e335e61c4",
      "30a9eeefa4f9475393dfcce81dd8e67b",
      "06991822b23d417496e937f977d45eee",
      "65afb9893b9d4feca7456726dc890ba1",
      "b73c8ac3c072401dbdd65fcea79b9af4",
      "0e156ad0bafc491da0f24570a8216c86",
      "4c8fe2e1058945fb80dd41cf4eb5df49",
      "5527913dc1d644bc8b00ec1b7f0a17f5",
      "fd5d944f1dde4da28bfe38bd36780fe2",
      "569cbef7cc634d72875ef40731022e83",
      "357ba5868921480684827c0a1ea2dd30",
      "017455a50103431abe967ce3edd08634",
      "984531e0cc5f4125b5464805e8976cc5",
      "3f42509651084f00b9496ce73f800248",
      "61d1ecc3c8a44351b7c25174c487cbf7",
      "f5bdbe6669ec43c08b70e34b1a01c430",
      "971fd59a043946588ebe2b6de97a8826",
      "7975159d86fb4893a9151e72fb5bbc9d",
      "91d13cc582be4b988fd958bea1450329",
      "3e523b4a8d59446cb216b324bef1d013",
      "0cf3809d637342a984191c9b50596e1d",
      "ebf22edcfced42a695ed615d42ada934",
      "e70be053fa334155879e66718c90339a",
      "9e79df59bb11405393af04047f9a36fe",
      "2b248e547bbb4b628f9db01278ec77e6",
      "fd902d39b5b54f8980c3e04229fcab6b",
      "4d4e4172637142289c8d73894ddb9f74",
      "19d93a1f31714989bf347beba37d22e1",
      "8405d2700fe748d183cf8df08cab2f50",
      "11a3b79c2d5d46968ae3c81a790f6fb7",
      "686e2ecbc65d4050b74710b68c5fef6a",
      "aa701f7b487640fdbb063f02a3f49ec5",
      "939e1a0773b249c18bc7b033a828733d",
      "5b23b0c34399491aa548b0a040b7d868",
      "a446cbe7ec7042c885433d78ac42948b",
      "977accd634cd479f89e0800da49bae1d",
      "40aae11b392142f2abf6b38e1d06e6dd",
      "3f7d30cdae054944ab41b17f9cf3f8e6",
      "df289a4a4aeb41e4922f98d2677eba00",
      "9ea91c6d49b344189431e2602904a375",
      "7ece3b1aeecb46df8d295f2a2fcb4c1a",
      "df7ebab2501d454583a478dd79f5cb89",
      "1df0a564094142a8992835bdc0ba74d1",
      "65fe6277c7bd4911b779b9d3eae1cb80",
      "5b9fed553c5b4c0d83ff80a09d048a19",
      "e868b25a6a9b43b0a8aecdef8794006c",
      "ff5d1103c2e1427a87183a9fbb6f6f0b",
      "891ee13ba6ec4f81a78e5b8e67aba3f3",
      "5101f6c585da4e7db8e5e548f0d9ec01",
      "460e13836be34983bd59a6539147d614",
      "640aa67869574234acd8ec7655bb9ce2",
      "38355623054d4841851240b8c70d3974",
      "4767f185a682488b82304a7613a43aa9",
      "4ab9f52cb4a241a6a00f5ea5d3e5de45",
      "913c20aadf9e41ec8c315f68edca9a9c",
      "ab4c744c07e74f4a99839d6fa2eaa6b0",
      "7d8bd1eb3cf640d69c332b52bb21e71b",
      "4d089500a48f49d78fe1b247e222d0cf",
      "6e6a0a87b09e44e0abb1ce7b3cc9ae38",
      "859d78a445264529979b61a291283f70",
      "9d01ac43fc0c4888bfeed063e73e1df1",
      "0aa9b9cbccda4016949857b1274affc7",
      "44958190405b4ba6b0ff0ff7302a3e4d",
      "bcabc31828684b17b882ec17978d45c9",
      "1f95847ea47e4f0b852366d5d599d62e",
      "453eea4d42b842f1b9fea4ecd3b6904d",
      "ddfb5aab135945f1bccfb624d794dff7",
      "41d096868a964d0cab765f0326d8d4c1",
      "0af323a9de65474b848871745f3b25bf",
      "64a5b02f63b24a2b8cdebb7a519df606",
      "6a05296b063e4fc890c82e9d56446e85",
      "c0276f556e424d3187b32e271dcec9f8",
      "bf83fe16987243d39854e5a8e30aa6da",
      "29019e98cfbc46ef99417ae67b7ced63",
      "b4ea04a3bdb745b1941d3d1572c974c0",
      "02555f1425ab46b8baa9e22850916f4d",
      "8532ab20f3164335b698ce83849e773b",
      "67e708f3806a4d15aaeeb6ed5fdf2e2a",
      "a4d93ac76a6a401e99d8ef262a908c37",
      "58ef2ad1f1004bfbb1e2c15b25818de8",
      "0cd24ded990047c8ae343b8880533320",
      "5b63ec87dc9943d497afe1c8b46b99c9",
      "6417d9300f894bbd91f9561755cee093",
      "a0ad946dc86e4d159361dc78f6987e6c",
      "33a12990d5614c688524a08e8c1f7db7",
      "7ce1faec9e74445384c9e7b1de703fca",
      "d7cfac6c24834ebbb533b8ca430d6a45",
      "067c551f00aa4990a09cb142a00f7e95",
      "d8b3611eb2ec41d18c0c8ff2cfa3d047",
      "429ebf9726104edda7d71ca248d16667",
      "61fd545ee64545c3b19aa462d1dfd3e6",
      "e82ea687032b4778b87cc9e6037fe0ac",
      "1439582734794fa68ba068ca39e94914",
      "0cb8cb1304e444a9acd83297793c0f4c",
      "47742bb797fb42ad956c3a2cdbea641c"
     ]
    },
    "id": "ZoQuSi-jq2ar",
    "outputId": "780b4aa5-663b-4273-e783-9905838f314f"
   },
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # 384-dim embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "XIkykVn2r4gb"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "paper_texts = []\n",
    "valid_pids = [pid for pid in paper2idx.keys() if str(pid).isdigit()]  # keep only numeric strings\n",
    "\n",
    "for pid in sorted(valid_pids, key=lambda x: int(x)):\n",
    "    pid_int = int(pid)\n",
    "    text = all_df.loc[all_df[\"publication_ID\"] == pid_int, \"text\"]\n",
    "\n",
    "    if len(text) > 0 and isinstance(text.values[0], str):\n",
    "        paper_texts.append(text.values[0])\n",
    "    else:\n",
    "        paper_texts.append(\" \")  # placeholder if no text found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MHUIzd1msTCh",
    "outputId": "f582c282-f427-4f6b-b9d4-cb20d44ec185"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total paper nodes: 424279\n",
      "Total valid papers with text: 424278\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total paper nodes: {len(paper2idx)}\")\n",
    "print(f\"Total valid papers with text: {len(paper_texts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "6a4c68b9acc84133bab07a8fbf33b4e9",
      "f3fca04e4dd44c97ad4312fdec3f5884",
      "d4cb152409894f6aa39434bfa1f6c3de",
      "79bf615cfdff43eda64c409bd547f375",
      "358f31c9a08247c29827809f618d0430",
      "fcdb2ebed9ce4da284d429dd54e74cde",
      "828a177687cd41ea913f43852be16943",
      "ce226ae29fe148808b9b7ecb2858fa2c",
      "36eac627bd2544f48fb79cce1e21228f",
      "7908bdd58499455bbf3bdaa064333370",
      "157b9cc8da2f4236bb40a63cffee6f1e"
     ]
    },
    "id": "IKRylrx7sYtA",
    "outputId": "be3f3c72-1ff4-439f-e3bd-b42bf0c42eb2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e84d64614a455ea044677ec50a058b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/6630 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "paper_embs = model.encode(\n",
    "    paper_texts,\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "-8Z3mYyivXuN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper embeddings saved to data/paper_embeddings.npy\n"
     ]
    }
   ],
   "source": [
    "# Save paper embeddings to data directory\n",
    "np.save(data_dir / \"paper_embeddings.npy\", paper_embs)\n",
    "print(f\"Paper embeddings saved to {data_dir / 'paper_embeddings.npy'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "m5yo0N9ANMoF"
   },
   "outputs": [],
   "source": [
    "# Load the embeddings\n",
    "# import numpy as np\n",
    "# paper_embs = np.load(data_dir / \"paper_embeddings.npy\")\n",
    "# print(\"Loaded paper embeddings:\", paper_embs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "IXOmfYcSWsqC"
   },
   "outputs": [],
   "source": [
    "# Jacob Question: Why are only author and keyword learnable embeddings? what about paper?\n",
    "\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch\n",
    "\n",
    "data = HeteroData()\n",
    "\n",
    "# Paper node features\n",
    "embedding_dim = 384 #this is because the transformer has 384-dim embeddings\n",
    "\n",
    "data['paper'].x = torch.tensor(paper_embs, dtype=torch.float) #the paper node is now embeddings from transformer\n",
    "data['author'].x = torch.randn(len(author2idx), embedding_dim)\n",
    "data['keyword'].x = torch.randn(len(keyword2idx), embedding_dim)\n",
    "data['venue'].x = torch.randn(len(venue2idx), embedding_dim)\n",
    "data['pubDate'].x = torch.randn(len(pubDate2idx), embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MrsL3MUHNq65",
    "outputId": "7f0ffe52-9494-409a-c80b-0b5e25e1015a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([424278, 384])\n",
      "torch.Size([307266, 384])\n",
      "torch.Size([17207, 384])\n"
     ]
    }
   ],
   "source": [
    "print(data['paper'].x.shape)\n",
    "print(data['author'].x.shape)\n",
    "print(data['keyword'].x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R6srFgzXJRtS",
    "outputId": "f059f7d0-6a8c-4131-fd24-c7628e9bf628"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building edges: 100%|██████████████████| 42000/42000 [00:03<00:00, 13580.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# Saw time inefficiency in the above code, the below is a fix\n",
    "#-----Shik Fixed here -------------\n",
    "# Fixed edge overwriting (previously only kept last record)\n",
    "\n",
    "edge_store = {\n",
    "    ('paper', 'cites', 'paper'): [[], []],\n",
    "    ('paper', 'written_by', 'author'): [[], []],\n",
    "    ('author', 'authored', 'paper'): [[], []],\n",
    "    ('paper', 'mentions', 'keyword'): [[], []],\n",
    "    ('keyword', 'appears_in', 'paper'): [[], []],\n",
    "    ('paper', 'published_in', 'venue'): [[], []],\n",
    "    ('venue', 'published', 'paper'): [[], []],\n",
    "    ('paper', 'publication_date', 'pubDate'): [[], []],\n",
    "}\n",
    "\n",
    "for _, row in tqdm(train_df.iterrows(), total=len(train_df), desc=\"Building edges\"):\n",
    "    pid = str(row['publication_ID'])\n",
    "    if pid not in paper2idx:\n",
    "        continue\n",
    "    pidx = paper2idx[pid]\n",
    "\n",
    "    # --- (a) Citations: paper → paper ---\n",
    "    for cited in row['Citations']:\n",
    "        cited = str(cited)\n",
    "        if cited in paper2idx:\n",
    "            edge_store[('paper', 'cites', 'paper')][0].append(pidx)\n",
    "            edge_store[('paper', 'cites', 'paper')][1].append(paper2idx[cited])\n",
    "\n",
    "    # --- (b) Authors: paper ↔ author ---\n",
    "    for aid in row['author_ids']:\n",
    "        if aid in author2idx:\n",
    "            edge_store[('paper', 'written_by', 'author')][0].append(pidx)\n",
    "            edge_store[('paper', 'written_by', 'author')][1].append(author2idx[aid])\n",
    "            edge_store[('author', 'authored', 'paper')][0].append(author2idx[aid])\n",
    "            edge_store[('author', 'authored', 'paper')][1].append(pidx)\n",
    "\n",
    "    # --- (c) Keywords: paper ↔ keyword ---\n",
    "    for kw in row['keyword_list']:\n",
    "        if kw in keyword2idx:\n",
    "            edge_store[('paper', 'mentions', 'keyword')][0].append(pidx)\n",
    "            edge_store[('paper', 'mentions', 'keyword')][1].append(keyword2idx[kw])\n",
    "            edge_store[('keyword', 'appears_in', 'paper')][0].append(keyword2idx[kw])\n",
    "            edge_store[('keyword', 'appears_in', 'paper')][1].append(pidx)\n",
    "\n",
    "    # --- (d) Venue: paper ↔ venue ---\n",
    "    venue = row['venue']\n",
    "    if venue in venue2idx:\n",
    "        edge_store[('paper', 'published_in', 'venue')][0].append(pidx)\n",
    "        edge_store[('paper', 'published_in', 'venue')][1].append(venue2idx[venue])\n",
    "        edge_store[('venue', 'published', 'paper')][0].append(venue2idx[venue])\n",
    "        edge_store[('venue', 'published', 'paper')][1].append(pidx)\n",
    "\n",
    "    # --- (e) Publication Date: paper → pubDate ---\n",
    "    pubDate = row['pubDate']\n",
    "    if pubDate in pubDate2idx:\n",
    "        edge_store[('paper', 'publication_date', 'pubDate')][0].append(pidx)\n",
    "        edge_store[('paper', 'publication_date', 'pubDate')][1].append(pubDate2idx[pubDate])\n",
    "\n",
    "for rel, (src, dst) in edge_store.items():\n",
    "    if len(src) > 0:\n",
    "        data[rel].edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
    "    else:\n",
    "        print(f\"No edges found for relation {rel}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EMGF3w33Yp-D",
    "outputId": "3ccb0360-5376-4e14-f1bd-dbccdb0dc451"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Node Counts ===\n",
      "paper        → 424,278 nodes | Feature dim: 384\n",
      "author       → 307,266 nodes | Feature dim: 384\n",
      "keyword      → 17,207 nodes | Feature dim: 384\n",
      "venue        → 5,501 nodes | Feature dim: 384\n",
      "pubDate      → 35 nodes | Feature dim: 384\n",
      "\n",
      "=== Edge Counts ===\n",
      "('paper', 'cites', 'paper') → 486,632 edges (shape=(2, 486632))\n",
      "('paper', 'written_by', 'author') → 291,666 edges (shape=(2, 291666))\n",
      "('author', 'authored', 'paper') → 291,666 edges (shape=(2, 291666))\n",
      "('paper', 'mentions', 'keyword') → 948,766 edges (shape=(2, 948766))\n",
      "('keyword', 'appears_in', 'paper') → 948,766 edges (shape=(2, 948766))\n",
      "('paper', 'published_in', 'venue') → 42,000 edges (shape=(2, 42000))\n",
      "('venue', 'published', 'paper') → 42,000 edges (shape=(2, 42000))\n",
      "('paper', 'publication_date', 'pubDate') → 42,000 edges (shape=(2, 42000))\n"
     ]
    }
   ],
   "source": [
    "def summarize_heterodata(data):\n",
    "    print(\"=== Node Counts ===\")\n",
    "    for ntype in data.node_types:\n",
    "        print(f\"{ntype:<12} → {data[ntype].num_nodes:,} nodes | \"\n",
    "              f\"Feature dim: {data[ntype].x.shape[1] if 'x' in data[ntype] else 'N/A'}\")\n",
    "\n",
    "    print(\"\\n=== Edge Counts ===\")\n",
    "    for etype in data.edge_types:\n",
    "        e = data[etype].edge_index\n",
    "        print(f\"{etype} → {e.shape[1]:,} edges (shape={tuple(e.shape)})\")\n",
    "\n",
    "summarize_heterodata(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rHvHU1-pN4-2",
    "outputId": "af5c05cb-0c7d-4961-ab80-d63a8ee859ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-loops in cites relation: 0\n",
      "Duplicate edges: 63994\n"
     ]
    }
   ],
   "source": [
    "# Checking for self-loops\n",
    "num_self_loops = (data['paper', 'cites', 'paper'].edge_index[0] ==\n",
    "                  data['paper', 'cites', 'paper'].edge_index[1]).sum().item()\n",
    "print(f\"Self-loops in cites relation: {num_self_loops}\")\n",
    "\n",
    "# Checking for duplicate edges\n",
    "ei = data['paper', 'cites', 'paper'].edge_index\n",
    "num_unique = torch.unique(ei, dim=1).shape[1]\n",
    "print(f\"Duplicate edges: {ei.shape[1] - num_unique}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eKmf9_-VODuW",
    "outputId": "25b7365a-a121-4cc2-c2fd-a27e6d76584d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 63994 duplicate citation edges. New total: 422638\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Remove duplicate edges from (paper, cites, paper)\n",
    "ei = data['paper', 'cites', 'paper'].edge_index\n",
    "\n",
    "# Sort columns so [src, dst] and [dst, src] duplicates align\n",
    "ei_unique = torch.unique(ei, dim=1)\n",
    "num_removed = ei.shape[1] - ei_unique.shape[1]\n",
    "\n",
    "data['paper', 'cites', 'paper'].edge_index = ei_unique\n",
    "\n",
    "print(f\"Removed {num_removed} duplicate citation edges. New total: {ei_unique.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H17-8lh2OIob",
    "outputId": "120aaf62-f0e2-4d88-9168-334dad8b9e27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New edge count: 422638\n",
      "Remaining duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "ei = data['paper', 'cites', 'paper'].edge_index\n",
    "print(f\"New edge count: {ei.shape[1]}\")\n",
    "num_unique = torch.unique(ei, dim=1).shape[1]\n",
    "print(f\"Remaining duplicates: {ei.shape[1] - num_unique}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "MpzDsyazZMtz"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.nn import HeteroConv, GATConv, Linear\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "class HeteroGNN(nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = HeteroConv({\n",
    "            ('paper', 'cites', 'paper'): GATConv(-1, hidden_channels, residual=True, add_self_loops=True),\n",
    "            ('paper', 'written_by', 'author'): GATConv(-1, hidden_channels, residual=True, add_self_loops=False),\n",
    "            ('author', 'authored', 'paper'): GATConv(-1, hidden_channels, residual=True, add_self_loops=False),\n",
    "            ('paper', 'mentions', 'keyword'): GATConv(-1, hidden_channels, residual=True, add_self_loops=False),\n",
    "            ('keyword', 'appears_in', 'paper'): GATConv(-1, hidden_channels, residual=True, add_self_loops=False),\n",
    "            ('paper', 'published_in', 'venue'): GATConv(-1, hidden_channels, residual=True, add_self_loops=False),\n",
    "            ('venue', 'published', 'paper'): GATConv(-1, hidden_channels, residual=True, add_self_loops=False),\n",
    "            ('paper','publication_date', 'pubDate'): GATConv(-1, hidden_channels, residual=True, add_self_loops=False)\n",
    "        }, aggr='sum')\n",
    "\n",
    "        self.lin = nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x_dict = self.conv1(x_dict, edge_index_dict)\n",
    "        x_dict = {key: F.relu(x) for key, x in x_dict.items()}\n",
    "        x_dict = {key: self.lin(x) for key, x in x_dict.items()}\n",
    "        return x_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GOAMFOtNboaw",
    "outputId": "899513a7-1f48-4924-db60-b86c9475df26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroGNN(\n",
      "  (conv1): HeteroConv(num_relations=8)\n",
      "  (lin): Linear(in_features=64, out_features=64, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = HeteroGNN(hidden_channels=64, out_channels=64).to(device)\n",
    "data = data.to(device) #Needed for all values to be on same device\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "iEjdSKw7bZd_"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "def loss_fn(emb_src, emb_dst, pos_edges, neg_edges):\n",
    "    pos_score = (emb_src[pos_edges[0]] * emb_dst[pos_edges[1]]).sum(dim=1)\n",
    "    neg_score = (emb_src[neg_edges[0]] * emb_dst[neg_edges[1]]).sum(dim=1)\n",
    "    labels = torch.cat([torch.ones_like(pos_score), torch.zeros_like(neg_score)])\n",
    "    scores = torch.cat([pos_score, neg_score])\n",
    "    return F.binary_cross_entropy_with_logits(scores, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sPRB9tqxOtCX",
    "outputId": "58f68c94-3930-42f8-8196-173a056ca9b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 48 invalid edges from ('paper', 'cites', 'paper')\n"
     ]
    }
   ],
   "source": [
    "def clean_invalid_edges(data):\n",
    "    for etype in data.edge_types:\n",
    "        if 'edge_index' not in data[etype]:\n",
    "            continue\n",
    "\n",
    "        ei = data[etype].edge_index\n",
    "        src_type, _, dst_type = etype\n",
    "        src_nodes = data[src_type].num_nodes\n",
    "        dst_nodes = data[dst_type].num_nodes\n",
    "\n",
    "        mask = (ei[0] < src_nodes) & (ei[1] < dst_nodes)\n",
    "        valid_count = mask.sum().item()\n",
    "\n",
    "        if valid_count < ei.shape[1]:\n",
    "            print(f\"Removing {ei.shape[1] - valid_count} invalid edges from {etype}\")\n",
    "            data[etype].edge_index = ei[:, mask]\n",
    "\n",
    "clean_invalid_edges(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "seDAihxhO7HM",
    "outputId": "4ad6a163-3f68-41fd-d050-18f31baa4baf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sanity Check for HeteroData ===\n",
      "('paper', 'cites', 'paper') OK (422590 edges)\n",
      "('paper', 'written_by', 'author') OK (291666 edges)\n",
      "('author', 'authored', 'paper') OK (291666 edges)\n",
      "('paper', 'mentions', 'keyword') OK (948766 edges)\n",
      "('keyword', 'appears_in', 'paper') OK (948766 edges)\n",
      "('paper', 'published_in', 'venue') OK (42000 edges)\n",
      "('venue', 'published', 'paper') OK (42000 edges)\n",
      "('paper', 'publication_date', 'pubDate') OK (42000 edges)\n"
     ]
    }
   ],
   "source": [
    "def check_hetero_integrity(data):\n",
    "    print(\"=== Sanity Check for HeteroData ===\")\n",
    "    for etype in data.edge_types:\n",
    "        ei = data[etype].edge_index\n",
    "        src_type, _, dst_type = etype\n",
    "\n",
    "        if ei is None or ei.numel() == 0:\n",
    "            print(f\"{etype} has no edges\")\n",
    "            continue\n",
    "\n",
    "        src_nodes, dst_nodes = data[src_type].num_nodes, data[dst_type].num_nodes\n",
    "        max_src, max_dst = ei[0].max().item(), ei[1].max().item()\n",
    "        min_src, min_dst = ei[0].min().item(), ei[1].min().item()\n",
    "\n",
    "        if max_src >= src_nodes or max_dst >= dst_nodes:\n",
    "            print(f\"{etype} has invalid indices: \"\n",
    "                  f\"src max {max_src}/{src_nodes}, dst max {max_dst}/{dst_nodes}\")\n",
    "        elif min_src < 0 or min_dst < 0:\n",
    "            print(f\"{etype} has negative indices!\")\n",
    "        else:\n",
    "            print(f\"{etype} OK ({ei.shape[1]} edges)\")\n",
    "\n",
    "check_hetero_integrity(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "HL3F18TUPA__"
   },
   "outputs": [],
   "source": [
    "# Split Citation Edges\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "\n",
    "# Randomly split only the citation edges for link prediction\n",
    "transform = RandomLinkSplit(\n",
    "    num_val=0.1,                   # 10% validation\n",
    "    num_test=0.1,                  # 10% test\n",
    "    is_undirected=False,           # citations are directional\n",
    "    add_negative_train_samples=True,\n",
    "    edge_types=[('paper', 'cites', 'paper')],  # focus only on citation edges\n",
    "    rev_edge_types=[None]          # no reverse relation\n",
    ")\n",
    "\n",
    "train_data, val_data, test_data = transform(data)\n",
    "train_data = train_data.to(device)\n",
    "val_data = val_data.to(device)\n",
    "test_data = test_data.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DM49m10322V0",
    "outputId": "60cf8c15-2cb5-4184-d94a-870e8f90db06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Loss: 0.7390\n",
      "Epoch 2/10 | Loss: 0.7063\n",
      "Epoch 3/10 | Loss: 0.6940\n",
      "Epoch 4/10 | Loss: 0.6880\n",
      "Epoch 5/10 | Loss: 0.6846\n",
      "Epoch 6/10 | Loss: 0.6809\n",
      "Epoch 7/10 | Loss: 0.6784\n",
      "Epoch 8/10 | Loss: 0.6756\n",
      "Epoch 9/10 | Loss: 0.6730\n",
      "Epoch 10/10 | Loss: 0.6706\n"
     ]
    }
   ],
   "source": [
    "# Fixed to focus only on paper, cites, paper edges\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=GNN_config[\"learning_rate\"])\n",
    "\n",
    "for epoch in range(GNN_config['epoch']):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    out_dict = model(train_data.x_dict, train_data.edge_index_dict)\n",
    "\n",
    "    # --- Focus only on (paper, cites, paper) edges ---\n",
    "    etype = ('paper', 'cites', 'paper')\n",
    "    pos_edges = train_data[etype].edge_label_index  # edges for training\n",
    "\n",
    "    # Negative sampling for link prediction\n",
    "    neg_edges = negative_sampling(\n",
    "        edge_index=pos_edges,\n",
    "        num_nodes=(train_data['paper'].num_nodes, train_data['paper'].num_nodes),\n",
    "        num_neg_samples=pos_edges.size(1)\n",
    "    )\n",
    "\n",
    "\n",
    "    emb_src = out_dict['paper']\n",
    "    emb_dst = out_dict['paper']\n",
    "\n",
    "    total_loss = loss_fn(emb_src, emb_dst, pos_edges, neg_edges)\n",
    "\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{GNN_config['epoch']} | Loss: {total_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "HDQtE6d0bKVh"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"hetero_gnn_checkpoint.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jfiZ3Avsp79s",
    "outputId": "a5ae3d37-0681-4ef7-9b83-bb3abd00a8c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at /home/upandit/mag_citation_recommender/checkpoints/hetero_gnn_checkpoint.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Use local checkpoints directory (defined in cell 2)\n",
    "# checkpoint_dir is already defined as checkpoints_dir from earlier setup\n",
    "checkpoint_path = checkpoints_dir / 'hetero_gnn_checkpoint.pt'\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), checkpoint_path)\n",
    "print(f\"Model saved at {checkpoint_path.absolute()}\")\n",
    "\n",
    "# Load:\n",
    "# model.load_state_dict(torch.load(checkpoint_path))\n",
    "# model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUCN0JRdPcBX"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1cbWhC7MPi5e",
    "outputId": "41074205-a12c-4aec-fa57-af3d113dfeee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroGNN(\n",
       "  (conv1): HeteroConv(num_relations=8)\n",
       "  (lin): Linear(in_features=64, out_features=64, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set model to evaluation mode\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Load model from local checkpoints directory\n",
    "checkpoint_path = checkpoints_dir / 'hetero_gnn_checkpoint.pt'\n",
    "\n",
    "model = HeteroGNN(hidden_channels=64, out_channels=64)\n",
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Vl2mFtV90W5Y"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out_dict = model(data.x_dict, data.edge_index_dict)\n",
    "    paper_emb = out_dict['paper'].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6f7qD2yf0xmX",
    "outputId": "2e909c1d-adbd-4e7a-b057-5ec6d8b8ca5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Paper:\n",
      "ID: 17396995\n",
      "Title: Herpes simplex virus type 2 infection does not influence viral dynamics during early HIV 1 infection\n",
      "Year: 2007\n",
      "Venue: The Journal of infectious diseases\n",
      "Abstract: Herpes simplex virus type 2 infection does not influence viral dynamics during early HIV 1 infection. We sought to compare baseline and longitudinal plasma HIV-1 loads between herpes simplex virus type 2 (HSV-2)-seropositive and -seronegative individuals who are enrolled in a primary HIV-1 infection cohort in San Diego, California.. Adult;California;epidemiology;Cohort Studies;HIV Infections;blood...\n",
      "\n",
      "Top 5 similar papers:\n",
      "--------------------------------------------------\n",
      "17264332 — Clinicopathologic features of osteosarcoma in patients with Rothmund Thomson syndrome\n",
      "15372107 — PDX 1 haploinsufficiency limits the compensatory islet hyperplasia that occurs in response to insulin resistance\n",
      "19759291 — Medial prefrontal cortex secondary hyperalgesia and the default mode network\n",
      "15983384 — High resolution genomic profiles of human lung cancer\n",
      "15858822 — Relationship of serotonin transporter gene polymorphisms and haplotypes to mRNA transcription\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "query_pid = \"17396995\"  # Example paper ID\n",
    "query_idx = paper2idx[query_pid]\n",
    "\n",
    "if isinstance(paper_embs, torch.Tensor):\n",
    "    paper_embs = paper_embs.cpu().numpy()\n",
    "\n",
    "# Extract query vector\n",
    "query_vec = paper_embs[query_idx].reshape(1, -1)\n",
    "\n",
    "# Compute cosine similarity across all papers\n",
    "sims = cosine_similarity(query_vec, paper_embs)[0]\n",
    "topk = np.argsort(sims)[::-1][1:6]  # Skip itself, take top 5\n",
    "\n",
    "# Show info about the query paper itself\n",
    "query_row = all_df.loc[all_df[\"publication_ID\"] == int(query_pid)]\n",
    "if len(query_row) > 0:\n",
    "    print(\"Query Paper:\")\n",
    "    print(f\"ID: {query_pid}\")\n",
    "    print(f\"Title: {query_row['title'].values[0] if 'title' in query_row.columns else 'Unknown'}\")\n",
    "    print(f\"Year: {query_row['pubDate'].values[0]}\")\n",
    "    print(f\"Venue: {query_row['venue'].values[0]}\")\n",
    "    print(f\"Abstract: {query_row['text'].values[0][:400]}...\")\n",
    "else:\n",
    "    print(\"Query paper not found in all_df.\")\n",
    "\n",
    "print(\"\\nTop 5 similar papers:\")\n",
    "print(\"-\" * 50)\n",
    "for i in topk:\n",
    "    pid = list(paper2idx.keys())[i]\n",
    "    row = all_df.loc[all_df[\"publication_ID\"] == int(pid)]\n",
    "    if len(row) > 0:\n",
    "        title = row[\"title\"].values[0] if \"title\" in row.columns else \"Unknown\"\n",
    "        print(f\"{pid} — {title}\")\n",
    "    else:\n",
    "        print(f\"{pid} — Not found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "PlquSAQO3i1q"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data, device):\n",
    "    model.eval()\n",
    "    out = model(data.x_dict, data.edge_index_dict)\n",
    "    src_emb = out['paper']\n",
    "    dst_emb = out['paper']\n",
    "\n",
    "    # Positive and negative edges from test split\n",
    "    pos_edges = data['paper', 'cites', 'paper'].edge_label_index[:, data['paper', 'cites', 'paper'].edge_label == 1]\n",
    "    neg_edges = data['paper', 'cites', 'paper'].edge_label_index[:, data['paper', 'cites', 'paper'].edge_label == 0]\n",
    "\n",
    "    # Compute scores (dot product similarity)\n",
    "    pos_scores = (src_emb[pos_edges[0]] * dst_emb[pos_edges[1]]).sum(dim=1).cpu().numpy()\n",
    "    neg_scores = (src_emb[neg_edges[0]] * dst_emb[neg_edges[1]]).sum(dim=1).cpu().numpy()\n",
    "\n",
    "    y_true = np.concatenate([np.ones_like(pos_scores), np.zeros_like(neg_scores)])\n",
    "    y_scores = np.concatenate([pos_scores, neg_scores])\n",
    "\n",
    "    auc = roc_auc_score(y_true, y_scores)\n",
    "    ap = average_precision_score(y_true, y_scores)\n",
    "\n",
    "    return auc, ap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pzJawyJGQDXo",
    "outputId": "5d167d52-2b99-42b5-a899-98abf8b3b391"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AUC: 0.9093, AP: 0.8504\n",
      "Test AUC: 0.9050, AP: 0.8441\n"
     ]
    }
   ],
   "source": [
    "val_auc, val_ap = evaluate(model, val_data, device)\n",
    "print(f\"Validation AUC: {val_auc:.4f}, AP: {val_ap:.4f}\")\n",
    "\n",
    "test_auc, test_ap = evaluate(model, test_data, device)\n",
    "print(f\"Test AUC: {test_auc:.4f}, AP: {test_ap:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========================================\n",
    "# PART 2: GRIL ALGORITHMS IMPLEMENTATION\n",
    "# =========================================\n",
    "\n",
    "This section implements the GRIL algorithms for citation recommendation:\n",
    "- Algorithm 1: Attention-based Graph Retriever (with Entity Updates and Gumbel-Softmax)\n",
    "- Algorithm 3: SAG Pooling Layer\n",
    "- Algorithm 4: Joint Training Framework\n",
    "- Algorithm 5: Graph Supervision\n",
    "- LLM Integration: Llama3-8B with LoRA\n",
    "- Verbalization: Triple-to-text conversion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " model: HeteroGNN\n",
      " data: HeteroData\n",
      " paper2idx: dict\n",
      " all_df: DataFrame\n",
      " device: device\n",
      " checkpoints_dir: PosixPath\n",
      "\n",
      " All required variables from Part 1 are available!\n",
      "   Model: HeteroGNN\n",
      "   Data: HeteroData with 5 node types\n",
      "   Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Verify Part 1 outputs are available for Part 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "# Check required variables from Part 1\n",
    "required_vars = {\n",
    "    'model': 'HeteroGNN model (loaded from checkpoint)',\n",
    "    'data': 'HeteroData graph',\n",
    "    'paper2idx': 'Paper ID to index mapping',\n",
    "    'all_df': 'DataFrame with paper metadata',\n",
    "    'device': 'torch.device (cuda/cpu)',\n",
    "    'checkpoints_dir': 'Path to checkpoints directory'\n",
    "}\n",
    "\n",
    "missing = []\n",
    "for var_name, description in required_vars.items():\n",
    "    if var_name not in globals():\n",
    "        missing.append(f\"{var_name} ({description})\")\n",
    "    else:\n",
    "        print(f\"[OK] {var_name}: {type(globals()[var_name]).__name__}\")\n",
    "\n",
    "if missing:\n",
    "    print(f\"\\n[WARNING]  Missing variables: {', '.join(missing)}\")\n",
    "    print(\"Please run Part 1 cells first (especially model loading cell).\")\n",
    "else:\n",
    "    print(\"\\nAll required variables from Part 1 are available!\")\n",
    "    print(f\"   Model: {type(model).__name__}\")\n",
    "    print(f\"   Data: {type(data).__name__} with {len(data.node_types)} node types\")\n",
    "    print(f\"   Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Encoder (SentenceTransformer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QueryEncoder initialized with all-MiniLM-L6-v2, embedding_dim=384\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# QUERY ENCODER\n",
    "# =========================================\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List\n",
    "import torch\n",
    "\n",
    "class QueryEncoder:\n",
    "    \"\"\"\n",
    "    Encodes natural language queries into dense vectors using SentenceTransformer.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2', device=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_name: SentenceTransformer model name (default: 'all-MiniLM-L6-v2' - 384 dim)\n",
    "            device: Device to run the model on\n",
    "        \"\"\"\n",
    "        self.device = device if device is not None else torch.device('cpu')\n",
    "        self.model = SentenceTransformer(model_name, device=self.device)\n",
    "        self.embedding_dim = self.model.get_sentence_embedding_dimension()\n",
    "        print(f\"QueryEncoder initialized with {model_name}, embedding_dim={self.embedding_dim}\")\n",
    "    \n",
    "    def encode(self, query_text: str, convert_to_tensor: bool = True) -> torch.Tensor:\n",
    "        \"\"\"Encode a single query text into a dense vector.\"\"\"\n",
    "        embedding = self.model.encode(query_text, convert_to_tensor=convert_to_tensor, device=self.device)\n",
    "        return embedding\n",
    "    \n",
    "    def encode_batch(self, query_texts: List[str], batch_size: int = 32) -> torch.Tensor:\n",
    "        \"\"\"Encode a batch of query texts.\"\"\"\n",
    "        embeddings = self.model.encode(query_texts, batch_size=batch_size, \n",
    "                                      convert_to_tensor=True, device=self.device)\n",
    "        return embeddings\n",
    "\n",
    "# Initialize query encoder\n",
    "query_encoder = QueryEncoder(device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention-based Relevance Scorer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RelevanceScorer initialized (query_dim=384, node_dim=64)\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# ATTENTION-BASED RELEVANCE SCORER\n",
    "# =========================================\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RelevanceScorer(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes relevance scores between query, source node, and destination node embeddings.\n",
    "    This is the core attention mechanism for Algorithm 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, query_dim: int, node_dim: int, hidden_dim: int = 128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Project query to node dimension if needed\n",
    "        if query_dim != node_dim:\n",
    "            self.query_proj = nn.Linear(query_dim, node_dim)\n",
    "        else:\n",
    "            self.query_proj = nn.Identity()\n",
    "        \n",
    "        # MLP for computing relevance: f(query, src, dst) -> score\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(node_dim * 3, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, query_emb: torch.Tensor, src_embs: torch.Tensor, \n",
    "                dst_embs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute relevance scores for edges.\"\"\"\n",
    "        query_proj = self.query_proj(query_emb)\n",
    "        if query_proj.dim() == 1:\n",
    "            query_proj = query_proj.unsqueeze(0)\n",
    "        \n",
    "        num_edges = src_embs.size(0)\n",
    "        query_expanded = query_proj.expand(num_edges, -1)\n",
    "        combined = torch.cat([query_expanded, src_embs, dst_embs], dim=1)\n",
    "        scores = self.mlp(combined).squeeze(-1)\n",
    "        scores = torch.sigmoid(scores)\n",
    "        return scores\n",
    "\n",
    "# Initialize relevance scorer\n",
    "query_dim = query_encoder.embedding_dim  # 384\n",
    "node_dim = 64  # From HeteroGNN output\n",
    "relevance_scorer = RelevanceScorer(query_dim=query_dim, node_dim=node_dim).to(device)\n",
    "print(f\"RelevanceScorer initialized (query_dim={query_dim}, node_dim={node_dim})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 1: Attention-based Growing and Pruning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Algorithm 1 functions defined\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# ALGORITHM 1: ATTENTION-BASED GROWING AND PRUNING\n",
    "# =========================================\n",
    "from typing import Dict, List, Tuple, Set, Optional\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "def attention_based_graph_retriever(\n",
    "    query_text: str,\n",
    "    query_seed_entities: Dict[str, List[int]],\n",
    "    gnn_model: nn.Module,\n",
    "    full_data: HeteroData,\n",
    "    query_encoder: QueryEncoder,\n",
    "    relevance_scorer: RelevanceScorer,\n",
    "    max_hops: int = 2,\n",
    "    relevance_threshold: float = 0.1,\n",
    "    max_nodes_per_hop: Optional[int] = None,\n",
    "    device: torch.device = None\n",
    ") -> HeteroData:\n",
    "    \"\"\"\n",
    "    Implements Algorithm 1: Attention-based Growing and Pruning for dynamic subgraph retrieval.\n",
    "    \n",
    "    Args:\n",
    "        query_text: Natural language query (e.g., paper title + abstract)\n",
    "        query_seed_entities: Dict mapping node types to lists of node indices to start from\n",
    "        gnn_model: Trained HeteroGNN model\n",
    "        full_data: Full HeteroData graph\n",
    "        query_encoder: QueryEncoder instance\n",
    "        relevance_scorer: RelevanceScorer instance\n",
    "        max_hops: Maximum number of hops to expand (default: 2)\n",
    "        relevance_threshold: Minimum relevance score to retain an edge (default: 0.1)\n",
    "        max_nodes_per_hop: Maximum number of nodes to retain per hop (None = no limit)\n",
    "        device: Device to run computation on\n",
    "    \n",
    "    Returns:\n",
    "        HeteroData: Retrieved subgraph containing only relevant nodes and edges\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = next(gnn_model.parameters()).device\n",
    "    \n",
    "    # 1. Encode query\n",
    "    query_emb = query_encoder.encode(query_text).to(device)\n",
    "    \n",
    "    # 2. Get GNN embeddings for all nodes\n",
    "    gnn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        out_dict = gnn_model(full_data.x_dict, full_data.edge_index_dict)\n",
    "    \n",
    "    # 3. Initialize current nodes (seed entities)\n",
    "    current_nodes: Dict[str, Set[int]] = {k: set(v) for k, v in query_seed_entities.items()}\n",
    "    \n",
    "    # 4. Track all retained edges and nodes across hops\n",
    "    retained_edges: Dict[Tuple[str, str, str], List[Tuple[int, int, float]]] = {}\n",
    "    all_retained_nodes: Dict[str, Set[int]] = {k: set(v) for k, v in current_nodes.items()}\n",
    "    \n",
    "    # 5. Multi-hop expansion\n",
    "    for hop in range(max_hops):\n",
    "        newly_retained_nodes: Dict[str, Set[int]] = {}\n",
    "        \n",
    "        # Process each edge type\n",
    "        for etype in full_data.edge_types:\n",
    "            src_type, rel_type, dst_type = etype\n",
    "            edge_index = full_data[etype].edge_index\n",
    "            \n",
    "            # 5.1. Grow: Filter edges starting from current nodes\n",
    "            if src_type not in current_nodes or len(current_nodes[src_type]) == 0:\n",
    "                continue\n",
    "            \n",
    "            src_nodes_tensor = torch.tensor(list(current_nodes[src_type]), device=device)\n",
    "            src_mask = torch.isin(edge_index[0], src_nodes_tensor)\n",
    "            potential_edge_indices = torch.where(src_mask)[0]\n",
    "            \n",
    "            if potential_edge_indices.numel() == 0:\n",
    "                continue\n",
    "            \n",
    "            # Get source and destination indices for potential edges\n",
    "            potential_src = edge_index[0, potential_edge_indices]\n",
    "            potential_dst = edge_index[1, potential_edge_indices]\n",
    "            \n",
    "            # 5.2. Score: Calculate relevance for each potential edge\n",
    "            # Clone tensors to detach from inference mode (allows use in autograd if needed)\n",
    "            src_node_embs = out_dict[src_type][potential_src].clone()\n",
    "            dst_node_embs = out_dict[dst_type][potential_dst].clone()\n",
    "            # For inference, wrap in no_grad to avoid gradient computation\n",
    "            with torch.no_grad():\n",
    "                relevance_scores = relevance_scorer(query_emb, src_node_embs, dst_node_embs)\n",
    "            \n",
    "            # 5.3. Prune: Filter edges below threshold\n",
    "            relevant_mask = relevance_scores > relevance_threshold\n",
    "            \n",
    "            if relevant_mask.sum() == 0:\n",
    "                continue\n",
    "            \n",
    "            # Get retained edges\n",
    "            retained_src = potential_src[relevant_mask].cpu().numpy()\n",
    "            retained_dst = potential_dst[relevant_mask].cpu().numpy()\n",
    "            retained_scores = relevance_scores[relevant_mask].cpu().numpy()\n",
    "            \n",
    "            # Optionally limit nodes per hop\n",
    "            if max_nodes_per_hop is not None:\n",
    "                top_k = min(max_nodes_per_hop, len(retained_scores))\n",
    "                top_indices = np.argsort(retained_scores)[::-1][:top_k]\n",
    "                retained_src = retained_src[top_indices]\n",
    "                retained_dst = retained_dst[top_indices]\n",
    "                retained_scores = retained_scores[top_indices]\n",
    "            \n",
    "            # Store retained edges\n",
    "            edge_triples = list(zip(retained_src.tolist(), retained_dst.tolist(), retained_scores.tolist()))\n",
    "            if etype not in retained_edges:\n",
    "                retained_edges[etype] = []\n",
    "            retained_edges[etype].extend(edge_triples)\n",
    "            \n",
    "            # 5.4. Update: Add destination nodes for next hop\n",
    "            newly_retained_nodes.setdefault(dst_type, set()).update(retained_dst.tolist())\n",
    "            all_retained_nodes.setdefault(dst_type, set()).update(retained_dst.tolist())\n",
    "        \n",
    "        # Merge newly retained nodes for next iteration\n",
    "        for node_type, new_nodes in newly_retained_nodes.items():\n",
    "            current_nodes.setdefault(node_type, set()).update(new_nodes)\n",
    "    \n",
    "    # 6. Construct subgraph from retained nodes and edges\n",
    "    subgraph_data = _construct_subgraph(full_data, all_retained_nodes, retained_edges, device)\n",
    "    return subgraph_data\n",
    "\n",
    "\n",
    "def _construct_subgraph(\n",
    "    full_data: HeteroData,\n",
    "    retained_nodes: Dict[str, Set[int]],\n",
    "    retained_edges: Dict[Tuple[str, str, str], List[Tuple[int, int, float]]],\n",
    "    device: torch.device\n",
    ") -> HeteroData:\n",
    "    \"\"\"Construct a HeteroData subgraph from retained nodes and edges.\"\"\"\n",
    "    subgraph_data = HeteroData().to(device)\n",
    "    \n",
    "    # Add node features for retained nodes\n",
    "    for node_type in retained_nodes:\n",
    "        if node_type not in full_data.node_types:\n",
    "            continue\n",
    "        \n",
    "        node_indices = sorted(list(retained_nodes[node_type]))\n",
    "        if len(node_indices) == 0:\n",
    "            continue\n",
    "        \n",
    "        node_mapping = {orig_idx: new_idx for new_idx, orig_idx in enumerate(node_indices)}\n",
    "        node_tensor = torch.tensor(node_indices, device=device)\n",
    "        subgraph_data[node_type].x = full_data[node_type].x[node_tensor]\n",
    "        subgraph_data[node_type]._node_mapping = node_mapping\n",
    "    \n",
    "    # Add edges\n",
    "    for etype, edge_list in retained_edges.items():\n",
    "        if len(edge_list) == 0:\n",
    "            continue\n",
    "        \n",
    "        src_type, rel_type, dst_type = etype\n",
    "        src_mapping = subgraph_data[src_type]._node_mapping\n",
    "        dst_mapping = subgraph_data[dst_type]._node_mapping\n",
    "        \n",
    "        edge_src = []\n",
    "        edge_dst = []\n",
    "        \n",
    "        for src, dst, score in edge_list:\n",
    "            if src in src_mapping and dst in dst_mapping:\n",
    "                edge_src.append(src_mapping[src])\n",
    "                edge_dst.append(dst_mapping[dst])\n",
    "        \n",
    "        if len(edge_src) > 0:\n",
    "            edge_index = torch.tensor([edge_src, edge_dst], dtype=torch.long, device=device)\n",
    "            subgraph_data[etype].edge_index = edge_index\n",
    "    \n",
    "    return subgraph_data\n",
    "\n",
    "print(\"Algorithm 1 functions defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Embedding Update (Eq. 4) and Gumbel-Softmax Sampling (Eq. 5)\n",
    "\n",
    "This section implements:\n",
    "- **Entity Embedding Update (Eq. 4)**: Message-passing mechanism that updates entity embeddings after each growing/pruning step\n",
    "- **Gumbel-Softmax Sampling (Eq. 5)**: Differentiable subgraph sampling for end-to-end training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " EntityEmbeddingUpdater initialized (node_dim=64, hidden_dim=64)\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# ENTITY EMBEDDING UPDATE MODULE (Eq. 4)\n",
    "# =========================================\n",
    "class EntityEmbeddingUpdater(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements Equation 4 from GRIL paper:\n",
    "    h'_ei = W₁h_ei + W₂ Σ_{j∈N(vi)} α_ji h_ej\n",
    "    \n",
    "    Updates entity embeddings through attention-weighted message passing.\n",
    "    \"\"\"\n",
    "    def __init__(self, node_dim: int, hidden_dim: int = None):\n",
    "        super().__init__()\n",
    "        self.node_dim = node_dim\n",
    "        self.hidden_dim = hidden_dim if hidden_dim is not None else node_dim\n",
    "        \n",
    "        # W₁: Linear transformation for self-embedding\n",
    "        self.W1 = nn.Linear(node_dim, self.hidden_dim)\n",
    "        \n",
    "        # W₂: Linear transformation for aggregated neighbor embeddings\n",
    "        self.W2 = nn.Linear(node_dim, self.hidden_dim)\n",
    "        \n",
    "        # Optional: Layer normalization\n",
    "        self.layer_norm = nn.LayerNorm(self.hidden_dim)\n",
    "    \n",
    "    def forward(self, \n",
    "                node_embeddings: torch.Tensor,\n",
    "                neighbor_embeddings: torch.Tensor,\n",
    "                attention_weights: torch.Tensor,\n",
    "                node_indices: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Update entity embeddings using attention-weighted message passing.\n",
    "        \n",
    "        Args:\n",
    "            node_embeddings: Current node embeddings [num_nodes, node_dim]\n",
    "            neighbor_embeddings: Neighbor embeddings [num_edges, node_dim]\n",
    "            attention_weights: Attention scores α_ji [num_edges]\n",
    "            node_indices: Optional mapping from edges to nodes [num_edges] (source node for each edge)\n",
    "        \n",
    "        Returns:\n",
    "            Updated embeddings [num_nodes, hidden_dim]\n",
    "        \"\"\"\n",
    "        # W₁h_ei: Self-embedding transformation\n",
    "        self_emb = self.W1(node_embeddings)  # [num_nodes, hidden_dim]\n",
    "        \n",
    "        # W₂ Σ_{j∈N(vi)} α_ji h_ej: Aggregated neighbor embeddings\n",
    "        if node_indices is not None:\n",
    "            # Aggregate neighbors for each node using attention weights\n",
    "            # neighbor_embeddings: [num_edges, node_dim]\n",
    "            # attention_weights: [num_edges]\n",
    "            weighted_neighbors = neighbor_embeddings * attention_weights.unsqueeze(-1)  # [num_edges, node_dim]\n",
    "            \n",
    "            # Aggregate by node (sum neighbors for each node)\n",
    "            num_nodes = node_embeddings.size(0)\n",
    "            aggregated = torch.zeros(num_nodes, self.node_dim, \n",
    "                                    device=node_embeddings.device, \n",
    "                                    dtype=node_embeddings.dtype)\n",
    "            aggregated.index_add_(0, node_indices, weighted_neighbors)\n",
    "            \n",
    "            # Transform aggregated neighbors\n",
    "            neighbor_emb = self.W2(aggregated)  # [num_nodes, hidden_dim]\n",
    "        else:\n",
    "            # Simple case: if node_indices not provided, assume direct aggregation\n",
    "            # This is a fallback - in practice, node_indices should be provided\n",
    "            weighted_neighbors = neighbor_embeddings * attention_weights.unsqueeze(-1)\n",
    "            neighbor_emb = self.W2(weighted_neighbors.mean(dim=0, keepdim=True).expand_as(self_emb))\n",
    "        \n",
    "        # Combine: h'_ei = W₁h_ei + W₂ Σ_{j∈N(vi)} α_ji h_ej\n",
    "        updated_emb = self_emb + neighbor_emb\n",
    "        \n",
    "        # Apply layer normalization\n",
    "        updated_emb = self.layer_norm(updated_emb)\n",
    "        \n",
    "        return updated_emb\n",
    "\n",
    "# Initialize entity embedding updater\n",
    "entity_updater = EntityEmbeddingUpdater(node_dim=64, hidden_dim=64).to(device)\n",
    "print(f\"EntityEmbeddingUpdater initialized (node_dim=64, hidden_dim=64)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Gumbel-Softmax sampling functions defined\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# GUMBEL-SOFTMAX SAMPLING (Eq. 5)\n",
    "# =========================================\n",
    "def gumbel_softmax_sampling(\n",
    "    logits: torch.Tensor,\n",
    "    temperature: float = 1.0,\n",
    "    hard: bool = False,\n",
    "    training: bool = True\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Implements Equation 5 from GRIL paper:\n",
    "    Mi = σ((log(ϵ/(1-ϵ)) + log(Pi/(1-Pi))) / τ)\n",
    "    \n",
    "    Differentiable subgraph sampling using Gumbel-Softmax reparameterization trick.\n",
    "    \n",
    "    Args:\n",
    "        logits: Probability logits for each edge/triplet [num_edges] or [batch_size, num_edges]\n",
    "        temperature: Temperature parameter τ (default: 1.0)\n",
    "        hard: If True, returns hard (one-hot) samples, but gradients flow through soft samples\n",
    "        training: If False, uses hard sampling (deterministic)\n",
    "    \n",
    "    Returns:\n",
    "        Sampled mask M [num_edges] or [batch_size, num_edges]\n",
    "    \"\"\"\n",
    "    if not training:\n",
    "        # During inference, use hard thresholding\n",
    "        probs = torch.sigmoid(logits)\n",
    "        return (probs > 0.5).float()\n",
    "    \n",
    "    # Convert logits to probabilities\n",
    "    # Pi = sigmoid(logits) for binary case\n",
    "    probs = torch.sigmoid(logits)\n",
    "    \n",
    "    # Avoid numerical issues\n",
    "    eps = 1e-10\n",
    "    probs = torch.clamp(probs, eps, 1.0 - eps)\n",
    "    \n",
    "    # Sample Gumbel noise: ϵ ~ Uniform(0, 1)\n",
    "    # Gumbel(0, 1) = -log(-log(U)) where U ~ Uniform(0, 1)\n",
    "    uniform_noise = torch.rand_like(probs)\n",
    "    uniform_noise = torch.clamp(uniform_noise, eps, 1.0 - eps)\n",
    "    gumbel_noise = -torch.log(-torch.log(uniform_noise))\n",
    "    \n",
    "    # Compute log(Pi / (1 - Pi))\n",
    "    log_odds = torch.log(probs / (1 - probs))\n",
    "    \n",
    "    # Apply Gumbel-Softmax: Mi = σ((log(ϵ/(1-ϵ)) + log(Pi/(1-Pi))) / τ)\n",
    "    # Note: log(ϵ/(1-ϵ)) is the Gumbel noise\n",
    "    y = (log_odds + gumbel_noise) / temperature\n",
    "    soft_samples = torch.sigmoid(y)\n",
    "    \n",
    "    if hard:\n",
    "        # Hard sampling: return one-hot, but gradients flow through soft\n",
    "        hard_samples = (soft_samples > 0.5).float()\n",
    "        # Straight-through estimator: use hard in forward, soft in backward\n",
    "        return hard_samples + soft_samples - soft_samples.detach()\n",
    "    else:\n",
    "        return soft_samples\n",
    "\n",
    "\n",
    "def compute_triplet_probabilities(\n",
    "    relevance_scores: torch.Tensor,\n",
    "    attention_weights: torch.Tensor = None\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute probability scores P on triplets for Gumbel-Softmax sampling.\n",
    "    \n",
    "    Args:\n",
    "        relevance_scores: Relevance scores from RelevanceScorer [num_edges]\n",
    "        attention_weights: Optional attention weights α_ij [num_edges]\n",
    "    \n",
    "    Returns:\n",
    "        Probability logits for triplets [num_edges]\n",
    "    \"\"\"\n",
    "    if attention_weights is not None:\n",
    "        # Combine relevance and attention: P = relevance * attention\n",
    "        probs = relevance_scores * attention_weights\n",
    "    else:\n",
    "        probs = relevance_scores\n",
    "    \n",
    "    # Convert to logits for Gumbel-Softmax\n",
    "    # Use logit transform: logit(p) = log(p / (1-p))\n",
    "    eps = 1e-10\n",
    "    probs = torch.clamp(probs, eps, 1.0 - eps)\n",
    "    logits = torch.log(probs / (1 - probs))\n",
    "    \n",
    "    return logits\n",
    "\n",
    "print(\"Gumbel-Softmax sampling functions defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Algorithm 1: With Entity Updates and Gumbel-Softmax\n",
    "\n",
    "This enhanced version integrates:\n",
    "- Entity embedding updates after each hop (Eq. 4)\n",
    "- Gumbel-Softmax sampling for differentiable subgraph selection (Eq. 5)\n",
    "- Support for both training and inference modes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Enhanced Algorithm 1 with Entity Updates and Gumbel-Softmax defined\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# ENHANCED ALGORITHM 1: WITH ENTITY UPDATES AND GUMBEL-SOFTMAX\n",
    "# =========================================\n",
    "def attention_based_graph_retriever_enhanced(\n",
    "    query_text: str,\n",
    "    query_seed_entities: Dict[str, List[int]],\n",
    "    gnn_model: nn.Module,\n",
    "    full_data: HeteroData,\n",
    "    query_encoder: QueryEncoder,\n",
    "    relevance_scorer: RelevanceScorer,\n",
    "    entity_updater: EntityEmbeddingUpdater = None,\n",
    "    max_hops: int = 2,\n",
    "    relevance_threshold: float = 0.1,\n",
    "    max_nodes_per_hop: Optional[int] = None,\n",
    "    use_gumbel_softmax: bool = True,\n",
    "    gumbel_temperature: float = 1.0,\n",
    "    training: bool = False,\n",
    "    device: torch.device = None\n",
    ") -> Tuple[HeteroData, Dict]:\n",
    "    \"\"\"\n",
    "    Enhanced Algorithm 1 with Entity Embedding Updates (Eq. 4) and Gumbel-Softmax Sampling (Eq. 5).\n",
    "    \n",
    "    Args:\n",
    "        query_text: Natural language query (e.g., paper title + abstract)\n",
    "        query_seed_entities: Dict mapping node types to lists of node indices to start from\n",
    "        gnn_model: Trained HeteroGNN model\n",
    "        full_data: Full HeteroData graph\n",
    "        query_encoder: QueryEncoder instance\n",
    "        relevance_scorer: RelevanceScorer instance\n",
    "        entity_updater: EntityEmbeddingUpdater instance (optional, for Eq. 4)\n",
    "        max_hops: Maximum number of hops to expand (default: 2)\n",
    "        relevance_threshold: Minimum relevance score to retain an edge (default: 0.1)\n",
    "        max_nodes_per_hop: Maximum number of nodes to retain per hop (None = no limit)\n",
    "        use_gumbel_softmax: Whether to use Gumbel-Softmax sampling (default: True)\n",
    "        gumbel_temperature: Temperature for Gumbel-Softmax (default: 1.0)\n",
    "        training: Whether in training mode (affects Gumbel-Softmax behavior)\n",
    "        device: Device to run computation on\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of:\n",
    "        - HeteroData: Retrieved subgraph\n",
    "        - Dict: Additional info (updated embeddings, triplet probabilities, etc.)\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = next(gnn_model.parameters()).device\n",
    "    \n",
    "    # 1. Encode query\n",
    "    # Ensure encoding is done outside inference mode\n",
    "    if training:\n",
    "        # During training, we want regular tensors\n",
    "        query_emb = query_encoder.encode(query_text, convert_to_tensor=True).to(device)\n",
    "        # Clone to ensure it's a regular tensor (not inference mode)\n",
    "        query_emb = query_emb.clone()\n",
    "    else:\n",
    "        # During inference, use no_grad and detach\n",
    "        with torch.no_grad():\n",
    "            query_emb = query_encoder.encode(query_text, convert_to_tensor=True).to(device)\n",
    "        query_emb = query_emb.clone().detach()\n",
    "    \n",
    "    # 2. Get initial GNN embeddings for all nodes\n",
    "    gnn_model.eval()\n",
    "    if training:\n",
    "        # During training, we freeze GNN (no gradients through GNN)\n",
    "        # Entity updater will create new tensors with gradients\n",
    "        with torch.no_grad():\n",
    "            out_dict = gnn_model(full_data.x_dict, full_data.edge_index_dict)\n",
    "        # Clone to make them regular tensors, but don't require grad on initial embeddings\n",
    "        # The entity updater will create new tensors with gradients\n",
    "        updated_embeddings = {node_type: emb.clone().detach() \n",
    "                            for node_type, emb in out_dict.items()}\n",
    "    else:\n",
    "        # During inference, use no_grad\n",
    "        with torch.no_grad():\n",
    "            out_dict = gnn_model(full_data.x_dict, full_data.edge_index_dict)\n",
    "        # Clone to ensure they're regular tensors (not inference mode)\n",
    "        updated_embeddings = {node_type: emb.clone().detach() \n",
    "                            for node_type, emb in out_dict.items()}\n",
    "    \n",
    "    # 3. Initialize current nodes (seed entities)\n",
    "    current_nodes: Dict[str, Set[int]] = {k: set(v) for k, v in query_seed_entities.items()}\n",
    "    \n",
    "    # 4. Track all retained edges and nodes across hops\n",
    "    retained_edges: Dict[Tuple[str, str, str], List[Tuple[int, int, float]]] = {}\n",
    "    all_retained_nodes: Dict[str, Set[int]] = {k: set(v) for k, v in current_nodes.items()}\n",
    "    \n",
    "    # Track triplet probabilities for Gumbel-Softmax\n",
    "    all_triplet_logits: Dict[Tuple[str, str, str], torch.Tensor] = {}\n",
    "    all_attention_weights: Dict[Tuple[str, str, str], torch.Tensor] = {}\n",
    "    \n",
    "    # 5. Multi-hop expansion with entity embedding updates\n",
    "    for hop in range(max_hops):\n",
    "        newly_retained_nodes: Dict[str, Set[int]] = {}\n",
    "        hop_edges: Dict[Tuple[str, str, str], List[Tuple[int, int, float, torch.Tensor]]] = {}\n",
    "        \n",
    "        # Process each edge type\n",
    "        for etype in full_data.edge_types:\n",
    "            src_type, rel_type, dst_type = etype\n",
    "            edge_index = full_data[etype].edge_index\n",
    "            \n",
    "            # 5.1. Grow: Filter edges starting from current nodes\n",
    "            if src_type not in current_nodes or len(current_nodes[src_type]) == 0:\n",
    "                continue\n",
    "            \n",
    "            src_nodes_tensor = torch.tensor(list(current_nodes[src_type]), device=device)\n",
    "            src_mask = torch.isin(edge_index[0], src_nodes_tensor)\n",
    "            potential_edge_indices = torch.where(src_mask)[0]\n",
    "            \n",
    "            if potential_edge_indices.numel() == 0:\n",
    "                continue\n",
    "            \n",
    "            # Get source and destination indices for potential edges\n",
    "            potential_src = edge_index[0, potential_edge_indices]\n",
    "            potential_dst = edge_index[1, potential_edge_indices]\n",
    "            \n",
    "            # 5.2. Score: Calculate relevance using UPDATED embeddings\n",
    "            # Use updated embeddings from previous hop (or initial if first hop)\n",
    "            src_node_embs = updated_embeddings[src_type][potential_src]\n",
    "            dst_node_embs = updated_embeddings[dst_type][potential_dst]\n",
    "            \n",
    "            # Compute relevance scores (with gradients if training)\n",
    "            if training:\n",
    "                relevance_scores = relevance_scorer(query_emb, src_node_embs, dst_node_embs)\n",
    "            else:\n",
    "                # During inference, use no_grad to avoid autograd tracking\n",
    "                with torch.no_grad():\n",
    "                    relevance_scores = relevance_scorer(query_emb, src_node_embs, dst_node_embs)\n",
    "            \n",
    "            # Compute attention weights (softmax over neighbors for each source node)\n",
    "            # Group by source node and compute softmax\n",
    "            if training:\n",
    "                attention_weights = relevance_scores.clone()\n",
    "                if len(potential_src) > 0:\n",
    "                    # Normalize attention scores per source node\n",
    "                    src_unique, src_inverse = torch.unique(potential_src, return_inverse=True)\n",
    "                    for src_idx in range(len(src_unique)):\n",
    "                        mask = (src_inverse == src_idx)\n",
    "                        if mask.sum() > 1:\n",
    "                            attention_weights[mask] = F.softmax(relevance_scores[mask] / 0.1, dim=0)\n",
    "                        else:\n",
    "                            attention_weights[mask] = 1.0\n",
    "            else:\n",
    "                # During inference, compute in no_grad context\n",
    "                with torch.no_grad():\n",
    "                    attention_weights = relevance_scores.clone()\n",
    "                    if len(potential_src) > 0:\n",
    "                        # Normalize attention scores per source node\n",
    "                        src_unique, src_inverse = torch.unique(potential_src, return_inverse=True)\n",
    "                        for src_idx in range(len(src_unique)):\n",
    "                            mask = (src_inverse == src_idx)\n",
    "                            if mask.sum() > 1:\n",
    "                                attention_weights[mask] = F.softmax(relevance_scores[mask] / 0.1, dim=0)\n",
    "                            else:\n",
    "                                attention_weights[mask] = 1.0\n",
    "            \n",
    "            # 5.3. Prune: Filter edges below threshold\n",
    "            relevant_mask = relevance_scores > relevance_threshold\n",
    "            \n",
    "            if relevant_mask.sum() == 0:\n",
    "                continue\n",
    "            \n",
    "            # Apply Gumbel-Softmax sampling if enabled\n",
    "            if use_gumbel_softmax and training:\n",
    "                # Compute triplet probabilities\n",
    "                triplet_logits = compute_triplet_probabilities(\n",
    "                    relevance_scores[relevant_mask],\n",
    "                    attention_weights[relevant_mask]\n",
    "                )\n",
    "                \n",
    "                # Sample using Gumbel-Softmax\n",
    "                sampled_mask = gumbel_softmax_sampling(\n",
    "                    triplet_logits,\n",
    "                    temperature=gumbel_temperature,\n",
    "                    hard=False,\n",
    "                    training=training\n",
    "                )\n",
    "                \n",
    "                # Further filter by sampled mask\n",
    "                sampled_indices = torch.where(sampled_mask > 0.5)[0]\n",
    "                if len(sampled_indices) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Get final retained edges\n",
    "                final_mask = torch.zeros_like(relevant_mask)\n",
    "                relevant_indices = torch.where(relevant_mask)[0]\n",
    "                final_mask[relevant_indices[sampled_indices]] = True\n",
    "            else:\n",
    "                # Standard thresholding (inference mode)\n",
    "                final_mask = relevant_mask\n",
    "                if use_gumbel_softmax and not training:\n",
    "                    # During inference, use hard thresholding\n",
    "                    triplet_logits = compute_triplet_probabilities(\n",
    "                        relevance_scores[relevant_mask],\n",
    "                        attention_weights[relevant_mask]\n",
    "                    )\n",
    "                    sampled_mask = gumbel_softmax_sampling(\n",
    "                        triplet_logits,\n",
    "                        temperature=gumbel_temperature,\n",
    "                        hard=True,\n",
    "                        training=False\n",
    "                    )\n",
    "                else:\n",
    "                    sampled_mask = None\n",
    "            \n",
    "            if final_mask.sum() == 0:\n",
    "                continue\n",
    "            \n",
    "            # Get retained edges\n",
    "            # Detach before converting to numpy if tensor requires grad\n",
    "            if training:\n",
    "                retained_src = potential_src[final_mask].detach().cpu().numpy()\n",
    "                retained_dst = potential_dst[final_mask].detach().cpu().numpy()\n",
    "                retained_scores = relevance_scores[final_mask].detach().cpu().numpy()\n",
    "            else:\n",
    "                retained_src = potential_src[final_mask].cpu().numpy()\n",
    "                retained_dst = potential_dst[final_mask].cpu().numpy()\n",
    "                retained_scores = relevance_scores[final_mask].cpu().numpy()\n",
    "            retained_attention = attention_weights[final_mask]\n",
    "            \n",
    "            # Optionally limit nodes per hop\n",
    "            if max_nodes_per_hop is not None:\n",
    "                top_k = min(max_nodes_per_hop, len(retained_scores))\n",
    "                # Use argsort and get top_k indices - ensure contiguous array\n",
    "                sorted_indices = np.argsort(retained_scores)\n",
    "                # Get top_k indices in descending order (largest first)\n",
    "                # Use np.flip to reverse, but ensure it's a copy, not a view\n",
    "                top_indices = np.flip(sorted_indices[-top_k:], axis=0).copy()  # Copy to avoid negative strides\n",
    "                # Ensure arrays are contiguous to avoid negative stride issues\n",
    "                retained_src = np.ascontiguousarray(retained_src[top_indices])\n",
    "                retained_dst = np.ascontiguousarray(retained_dst[top_indices])\n",
    "                retained_scores = np.ascontiguousarray(retained_scores[top_indices])\n",
    "                # For tensor, use indexing and ensure contiguous\n",
    "                retained_attention = retained_attention[top_indices]\n",
    "                if isinstance(retained_attention, torch.Tensor):\n",
    "                    retained_attention = retained_attention.contiguous()\n",
    "            \n",
    "            # Store retained edges with attention weights\n",
    "            # Convert attention to numpy and ensure it's contiguous\n",
    "            if isinstance(retained_attention, torch.Tensor):\n",
    "                attention_numpy = retained_attention.detach().cpu().numpy()\n",
    "                # Ensure contiguous array to avoid negative stride issues\n",
    "                attention_numpy = np.ascontiguousarray(attention_numpy)\n",
    "                attention_list = attention_numpy.tolist()\n",
    "            else:\n",
    "                attention_list = retained_attention.tolist() if hasattr(retained_attention, 'tolist') else list(retained_attention)\n",
    "            \n",
    "            edge_triples = list(zip(\n",
    "                retained_src.tolist(), \n",
    "                retained_dst.tolist(), \n",
    "                retained_scores.tolist(),\n",
    "                attention_list\n",
    "            ))\n",
    "            if etype not in retained_edges:\n",
    "                retained_edges[etype] = []\n",
    "            retained_edges[etype].extend([(s, d, sc) for s, d, sc, _ in edge_triples])\n",
    "            \n",
    "            # Store for entity embedding update\n",
    "            hop_edges[etype] = edge_triples\n",
    "            \n",
    "            # Store triplet logits and attention weights\n",
    "            if use_gumbel_softmax:\n",
    "                if etype not in all_triplet_logits:\n",
    "                    all_triplet_logits[etype] = []\n",
    "                all_triplet_logits[etype].append(triplet_logits if sampled_mask is None else triplet_logits)\n",
    "                all_attention_weights[etype] = retained_attention\n",
    "            \n",
    "            # 5.4. Update: Add destination nodes for next hop\n",
    "            newly_retained_nodes.setdefault(dst_type, set()).update(retained_dst.tolist())\n",
    "            all_retained_nodes.setdefault(dst_type, set()).update(retained_dst.tolist())\n",
    "        \n",
    "        # 5.5. ENTITY EMBEDDING UPDATE (Eq. 4) after each hop\n",
    "        if entity_updater is not None and hop < max_hops - 1:  # Don't update after last hop\n",
    "            for node_type in updated_embeddings.keys():\n",
    "                if node_type not in all_retained_nodes or len(all_retained_nodes[node_type]) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Collect edges involving this node type\n",
    "                node_edges = []\n",
    "                for etype, edge_list in hop_edges.items():\n",
    "                    src_type, rel_type, dst_type = etype\n",
    "                    \n",
    "                    if src_type == node_type:\n",
    "                        # Outgoing edges: node is source\n",
    "                        for src, dst, score, attn in edge_list:\n",
    "                            node_edges.append((src, dst, attn, 'out'))\n",
    "                    elif dst_type == node_type:\n",
    "                        # Incoming edges: node is destination\n",
    "                        for src, dst, score, attn in edge_list:\n",
    "                            node_edges.append((src, dst, attn, 'in'))\n",
    "                \n",
    "                if len(node_edges) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Prepare for message passing\n",
    "                node_indices_list = []\n",
    "                neighbor_emb_list = []\n",
    "                attention_list = []\n",
    "                \n",
    "                for src, dst, attn, direction in node_edges:\n",
    "                    if direction == 'out':\n",
    "                        # Node is source, neighbor is destination\n",
    "                        node_idx = src\n",
    "                        neighbor_idx = dst\n",
    "                        neighbor_type = dst_type\n",
    "                    else:\n",
    "                        # Node is destination, neighbor is source\n",
    "                        node_idx = dst\n",
    "                        neighbor_idx = src\n",
    "                        neighbor_type = src_type\n",
    "                    \n",
    "                    # Only process if node is in retained nodes\n",
    "                    # The neighbor should exist in the full graph embeddings\n",
    "                    if node_idx not in all_retained_nodes[node_type]:\n",
    "                        continue\n",
    "                    \n",
    "                    # Verify neighbor type and index are valid\n",
    "                    if neighbor_type not in updated_embeddings:\n",
    "                        continue\n",
    "                    \n",
    "                    embedding_size = updated_embeddings[neighbor_type].size(0)\n",
    "                    if neighbor_idx >= embedding_size:\n",
    "                        # Skip if neighbor index is out of bounds for this node type\n",
    "                        # This can happen if there's a mismatch between edge indices and node types\n",
    "                        continue\n",
    "                    \n",
    "                    # Get neighbor embedding safely (neighbor might not be in retained set yet,\n",
    "                    # but we can still access its embedding from the full graph)\n",
    "                    try:\n",
    "                        neighbor_emb = updated_embeddings[neighbor_type][neighbor_idx]\n",
    "                        \n",
    "                        node_indices_list.append(int(node_idx))  # Ensure Python int\n",
    "                        neighbor_emb_list.append(neighbor_emb)\n",
    "                        # Convert attn to Python float if it's a numpy scalar\n",
    "                        if isinstance(attn, (np.number, np.ndarray)):\n",
    "                            attention_list.append(float(attn))\n",
    "                        elif isinstance(attn, torch.Tensor):\n",
    "                            attention_list.append(float(attn.item()))\n",
    "                        else:\n",
    "                            attention_list.append(float(attn))\n",
    "                    except (IndexError, RuntimeError) as e:\n",
    "                        # Skip if there's any error accessing the embedding\n",
    "                        continue\n",
    "                \n",
    "                if len(node_indices_list) > 0:\n",
    "                    # Convert to tensors\n",
    "                    # Ensure node_indices_list contains Python ints, not numpy scalars\n",
    "                    node_indices_list_clean = [int(idx) for idx in node_indices_list]\n",
    "                    node_indices_tensor = torch.tensor(node_indices_list_clean, device=device, dtype=torch.long)\n",
    "                    neighbor_embs_tensor = torch.stack(neighbor_emb_list)\n",
    "                    # Ensure attention_list contains Python floats, not numpy scalars\n",
    "                    attention_list_clean = [float(attn) if isinstance(attn, (np.number, np.ndarray)) else attn for attn in attention_list]\n",
    "                    attention_tensor = torch.tensor(attention_list_clean, device=device, dtype=neighbor_embs_tensor.dtype)\n",
    "                    \n",
    "                    # Get current node embeddings\n",
    "                    current_node_indices = sorted(list(all_retained_nodes[node_type]))\n",
    "                    current_embs = updated_embeddings[node_type][current_node_indices]\n",
    "                    \n",
    "                    # Create mapping from node indices to position in current_embs\n",
    "                    idx_to_pos = {idx: pos for pos, idx in enumerate(current_node_indices)}\n",
    "                    node_positions = torch.tensor([idx_to_pos[idx] for idx in node_indices_list], device=device)\n",
    "                    \n",
    "                    # Update embeddings using EntityEmbeddingUpdater\n",
    "                    if training:\n",
    "                        updated_embs = entity_updater(\n",
    "                            current_embs,\n",
    "                            neighbor_embs_tensor,\n",
    "                            attention_tensor,\n",
    "                            node_positions\n",
    "                        )\n",
    "                    else:\n",
    "                        # During inference, use no_grad\n",
    "                        with torch.no_grad():\n",
    "                            updated_embs = entity_updater(\n",
    "                                current_embs,\n",
    "                                neighbor_embs_tensor,\n",
    "                                attention_tensor,\n",
    "                                node_positions\n",
    "                            )\n",
    "                    \n",
    "                    # Update stored embeddings\n",
    "                    # Since initial embeddings don't require grad, we can do in-place updates\n",
    "                    # The entity updater creates new tensors with gradients, which we store\n",
    "                    if not isinstance(current_node_indices, torch.Tensor):\n",
    "                        indices_tensor = torch.tensor(current_node_indices, device=device, dtype=torch.long)\n",
    "                    else:\n",
    "                        indices_tensor = current_node_indices.to(device)\n",
    "                    \n",
    "                    # Update embeddings (updated_embs from entity_updater has gradients if training)\n",
    "                    updated_embeddings[node_type][indices_tensor] = updated_embs\n",
    "        \n",
    "        # Merge newly retained nodes for next iteration\n",
    "        for node_type, new_nodes in newly_retained_nodes.items():\n",
    "            current_nodes.setdefault(node_type, set()).update(new_nodes)\n",
    "    \n",
    "    # 6. Construct subgraph from retained nodes and edges\n",
    "    subgraph_data = _construct_subgraph(full_data, all_retained_nodes, retained_edges, device)\n",
    "    \n",
    "    # Store updated embeddings in subgraph metadata\n",
    "    subgraph_data._updated_embeddings = updated_embeddings\n",
    "    subgraph_data._triplet_logits = all_triplet_logits\n",
    "    subgraph_data._attention_weights = all_attention_weights\n",
    "    \n",
    "    info_dict = {\n",
    "        'updated_embeddings': updated_embeddings,\n",
    "        'triplet_logits': all_triplet_logits,\n",
    "        'attention_weights': all_attention_weights\n",
    "    }\n",
    "    \n",
    "    return subgraph_data, info_dict\n",
    "\n",
    "print(\"Enhanced Algorithm 1 with Entity Updates and Gumbel-Softmax defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Enhanced Algorithm 1\n",
    "\n",
    "Test the enhanced Algorithm 1 with entity embedding updates and Gumbel-Softmax sampling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TESTING ENHANCED ALGORITHM 1: With Entity Updates & Gumbel-Softmax\n",
      "================================================================================\n",
      " All dependencies available\n",
      " Found query paper: 17396995 (index: 18104)\n",
      " Query text length: 645 characters\n",
      " Seed entities: {'paper': [18104]}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Test 1: Enhanced Algorithm 1 (Inference Mode)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      " ENHANCED ALGORITHM 1 TEST PASSED (Inference Mode)!\n",
      "================================================================================\n",
      "Retrieved subgraph statistics:\n",
      "  Node types: 5\n",
      "    paper: 201 nodes\n",
      "    author: 16 nodes\n",
      "    keyword: 28 nodes\n",
      "    venue: 2 nodes\n",
      "    pubDate: 1 nodes\n",
      "\n",
      "  Edge types: 8\n",
      "    ('paper', 'cites', 'paper'): 15 edges\n",
      "    ('paper', 'written_by', 'author'): 21 edges\n",
      "    ('paper', 'mentions', 'keyword'): 73 edges\n",
      "    ('paper', 'published_in', 'venue'): 3 edges\n",
      "    ('paper', 'publication_date', 'pubDate'): 3 edges\n",
      "    ('author', 'authored', 'paper'): 6 edges\n",
      "    ('keyword', 'appears_in', 'paper'): 100 edges\n",
      "    ('venue', 'published', 'paper'): 100 edges\n",
      "\n",
      "   Entity embeddings updated: 5 node types\n",
      "    paper: torch.Size([424278, 64])\n",
      "    author: torch.Size([307266, 64])\n",
      "    keyword: torch.Size([17207, 64])\n",
      "    venue: torch.Size([5501, 64])\n",
      "    pubDate: torch.Size([35, 64])\n",
      "\n",
      "   Triplet logits computed: 8 edge types\n",
      "\n",
      "  Comparison with full graph:\n",
      "    Full graph papers: 424,278\n",
      "    Retrieved papers: 201\n",
      "    Reduction: 99.95%\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Test 2: Enhanced Algorithm 1 (Training Mode)\n",
      "--------------------------------------------------------------------------------\n",
      " Training mode test passed!\n",
      "  Retrieved 4 papers in training mode\n",
      "   Gradients enabled for paper embeddings\n",
      "\n",
      " Enhanced Algorithm 1 is working correctly with Entity Updates and Gumbel-Softmax!\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# TEST: Enhanced Algorithm 1\n",
    "# =========================================\n",
    "print(\"=\"*80)\n",
    "print(\"TESTING ENHANCED ALGORITHM 1: With Entity Updates & Gumbel-Softmax\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check dependencies\n",
    "required = {\n",
    "    'model': model,\n",
    "    'data': data,\n",
    "    'paper2idx': paper2idx,\n",
    "    'all_df': all_df,\n",
    "    'query_encoder': query_encoder,\n",
    "    'relevance_scorer': relevance_scorer,\n",
    "    'entity_updater': entity_updater,\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "missing = [k for k, v in required.items() if v is None or k not in globals()]\n",
    "if missing:\n",
    "    print(f\"[ERROR] Missing dependencies: {missing}\")\n",
    "    print(\"Please run Part 1 and Part 2 initialization cells first.\")\n",
    "else:\n",
    "    print(\"All dependencies available\")\n",
    "    \n",
    "    # Test with a known paper ID\n",
    "    query_paper_id = \"17396995\"  # Example paper from our dataset\n",
    "    \n",
    "    try:\n",
    "        # Check if paper exists\n",
    "        if query_paper_id not in paper2idx:\n",
    "            print(f\"[ERROR] Paper {query_paper_id} not found in paper2idx\")\n",
    "        else:\n",
    "            query_paper_idx = paper2idx[query_paper_id]\n",
    "            print(f\"[OK] Found query paper: {query_paper_id} (index: {query_paper_idx})\")\n",
    "            \n",
    "            # Get query text\n",
    "            query_row = all_df[all_df['publication_ID'] == int(query_paper_id)]\n",
    "            if len(query_row) == 0:\n",
    "                print(f\"[ERROR] Paper {query_paper_id} not found in all_df\")\n",
    "            else:\n",
    "                query_text = query_row['text'].values[0]\n",
    "                print(f\"[OK] Query text length: {len(query_text)} characters\")\n",
    "                \n",
    "                # Define seed entities\n",
    "                query_seed_entities = {'paper': [query_paper_idx]}\n",
    "                print(f\"[OK] Seed entities: {query_seed_entities}\")\n",
    "                \n",
    "                # Test 1: Enhanced Algorithm 1 (inference mode)\n",
    "                print(\"\\n\" + \"-\"*80)\n",
    "                print(\"Test 1: Enhanced Algorithm 1 (Inference Mode)\")\n",
    "                print(\"-\"*80)\n",
    "                \n",
    "                retrieved_subgraph, info_dict = attention_based_graph_retriever_enhanced(\n",
    "                    query_text=query_text,\n",
    "                    query_seed_entities=query_seed_entities,\n",
    "                    gnn_model=model,\n",
    "                    full_data=data,\n",
    "                    query_encoder=query_encoder,\n",
    "                    relevance_scorer=relevance_scorer,\n",
    "                    entity_updater=entity_updater,\n",
    "                    max_hops=2,\n",
    "                    relevance_threshold=0.1,\n",
    "                    max_nodes_per_hop=100,\n",
    "                    use_gumbel_softmax=True,\n",
    "                    gumbel_temperature=1.0,\n",
    "                    training=False,  # Inference mode\n",
    "                    device=device\n",
    "                )\n",
    "                \n",
    "                # Verify results\n",
    "                print(\"\\n\" + \"=\"*80)\n",
    "                print(\"[OK] ENHANCED ALGORITHM 1 TEST PASSED (Inference Mode)!\")\n",
    "                print(\"=\"*80)\n",
    "                print(f\"Retrieved subgraph statistics:\")\n",
    "                print(f\"  Node types: {len(retrieved_subgraph.node_types)}\")\n",
    "                for node_type in retrieved_subgraph.node_types:\n",
    "                    num_nodes = retrieved_subgraph[node_type].num_nodes\n",
    "                    print(f\"    {node_type}: {num_nodes} nodes\")\n",
    "                \n",
    "                print(f\"\\n  Edge types: {len(retrieved_subgraph.edge_types)}\")\n",
    "                for etype in retrieved_subgraph.edge_types:\n",
    "                    num_edges = retrieved_subgraph[etype].edge_index.size(1) if hasattr(retrieved_subgraph[etype], 'edge_index') else 0\n",
    "                    print(f\"    {etype}: {num_edges} edges\")\n",
    "                \n",
    "                # Check for updated embeddings\n",
    "                if hasattr(retrieved_subgraph, '_updated_embeddings'):\n",
    "                    print(f\"\\n  [OK] Entity embeddings updated: {len(retrieved_subgraph._updated_embeddings)} node types\")\n",
    "                    for node_type, emb in retrieved_subgraph._updated_embeddings.items():\n",
    "                        print(f\"    {node_type}: {emb.shape}\")\n",
    "                \n",
    "                # Check for triplet logits (Gumbel-Softmax)\n",
    "                if hasattr(retrieved_subgraph, '_triplet_logits'):\n",
    "                    print(f\"\\n  [OK] Triplet logits computed: {len(retrieved_subgraph._triplet_logits)} edge types\")\n",
    "                \n",
    "                # Compare with full graph\n",
    "                print(f\"\\n  Comparison with full graph:\")\n",
    "                print(f\"    Full graph papers: {data['paper'].num_nodes:,}\")\n",
    "                print(f\"    Retrieved papers: {retrieved_subgraph['paper'].num_nodes:,}\")\n",
    "                print(f\"    Reduction: {(1 - retrieved_subgraph['paper'].num_nodes / data['paper'].num_nodes) * 100:.2f}%\")\n",
    "                \n",
    "                # Test 2: Training mode (with gradients)\n",
    "                print(\"\\n\" + \"-\"*80)\n",
    "                print(\"Test 2: Enhanced Algorithm 1 (Training Mode)\")\n",
    "                print(\"-\"*80)\n",
    "                \n",
    "                # Enable training mode\n",
    "                entity_updater.train()\n",
    "                relevance_scorer.train()\n",
    "                \n",
    "                retrieved_subgraph_train, info_dict_train = attention_based_graph_retriever_enhanced(\n",
    "                    query_text=query_text,\n",
    "                    query_seed_entities=query_seed_entities,\n",
    "                    gnn_model=model,\n",
    "                    full_data=data,\n",
    "                    query_encoder=query_encoder,\n",
    "                    relevance_scorer=relevance_scorer,\n",
    "                    entity_updater=entity_updater,\n",
    "                    max_hops=2,\n",
    "                    relevance_threshold=0.1,\n",
    "                    max_nodes_per_hop=50,  # Smaller for training test\n",
    "                    use_gumbel_softmax=True,\n",
    "                    gumbel_temperature=1.0,\n",
    "                    training=True,  # Training mode\n",
    "                    device=device\n",
    "                )\n",
    "                \n",
    "                print(\"[OK] Training mode test passed!\")\n",
    "                print(f\"  Retrieved {retrieved_subgraph_train['paper'].num_nodes} papers in training mode\")\n",
    "                \n",
    "                # Verify gradients can flow\n",
    "                if hasattr(retrieved_subgraph_train, '_updated_embeddings'):\n",
    "                    # Check if embeddings require grad\n",
    "                    for node_type, emb in retrieved_subgraph_train._updated_embeddings.items():\n",
    "                        if emb.requires_grad:\n",
    "                            print(f\"  [OK] Gradients enabled for {node_type} embeddings\")\n",
    "                            break\n",
    "                \n",
    "                print(\"\\n[OK] Enhanced Algorithm 1 is working correctly with Entity Updates and Gumbel-Softmax!\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] ERROR during Enhanced Algorithm 1 test:\")\n",
    "        print(f\"   {type(e).__name__}: {str(e)}\")\n",
    "        import traceback\n",
    "        print(\"\\nFull traceback:\")\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ComplexityAssessmentModule class defined\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# COMPLEXITY ASSESSMENT MODULE (CAM)\n",
    "# =========================================\n",
    "class ComplexityAssessmentModule(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP classifier that predicts question complexity (number of hops).\n",
    "    Output determines retrieval budget: number of triplets = 5 × predicted_hops\n",
    "    \"\"\"\n",
    "    def __init__(self, query_dim: int, hidden_dim: int = 256, max_hops: int = 4):\n",
    "        super().__init__()\n",
    "        self.query_dim = query_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.max_hops = max_hops\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(query_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, max_hops + 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, query_emb: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Predict complexity (number of hops).\"\"\"\n",
    "        return self.mlp(query_emb)\n",
    "    \n",
    "    def predict_hops(self, query_emb: torch.Tensor) -> int:\n",
    "        \"\"\"Predict number of hops for a query.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(query_emb)\n",
    "            if logits.dim() == 1:\n",
    "                predicted = torch.argmax(logits, dim=0).item()\n",
    "            else:\n",
    "                predicted = torch.argmax(logits, dim=1).item()\n",
    "        return predicted\n",
    "    \n",
    "    def get_retrieval_budget(self, query_emb: torch.Tensor) -> int:\n",
    "        \"\"\"Get number of triplets to retrieve based on predicted complexity.\"\"\"\n",
    "        hops = self.predict_hops(query_emb)\n",
    "        return 5 * hops\n",
    "\n",
    "# Initialize CAM (will be trained separately or jointly)\n",
    "# cam = ComplexityAssessmentModule(query_dim=query_encoder.embedding_dim).to(device)\n",
    "print(\"ComplexityAssessmentModule class defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint Training Framework\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " JointTrainingLoss class defined\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# JOINT TRAINING ALGORITHM\n",
    "# =========================================\n",
    "class JointTrainingLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Joint loss for LLM and retriever optimization.\n",
    "    Implements Algorithm 4 from GRIL paper, Section 4.3.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha: float = 1.0, beta: float = 1.0, gamma: float = 0.1, \n",
    "                 use_graph_supervision: bool = False):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.use_graph_supervision = use_graph_supervision\n",
    "    \n",
    "    def forward(self, llm_logits: torch.Tensor, ground_truth: torch.Tensor,\n",
    "                triplet_probabilities: Optional[torch.Tensor] = None,\n",
    "                shortest_path_entities: Optional[torch.Tensor] = None,\n",
    "                retrieved_entities: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, Dict[str, float]]:\n",
    "        \"\"\"Compute joint training loss.\"\"\"\n",
    "        # 1. LLM Accuracy Loss\n",
    "        if llm_logits.dim() == 2 and llm_logits.size(1) > 1:\n",
    "            if ground_truth.dtype == torch.long:\n",
    "                accuracy_loss = F.cross_entropy(llm_logits, ground_truth)\n",
    "            else:\n",
    "                accuracy_loss = F.binary_cross_entropy_with_logits(llm_logits, ground_truth)\n",
    "        else:\n",
    "            accuracy_loss = F.mse_loss(llm_logits, ground_truth.float())\n",
    "        \n",
    "        # 2. Retriever Feedback Loss\n",
    "        retriever_feedback_loss = torch.tensor(0.0, device=llm_logits.device)\n",
    "        if triplet_probabilities is not None:\n",
    "            probs = F.softmax(triplet_probabilities, dim=-1)\n",
    "            entropy = -(probs * torch.log(probs + 1e-10)).sum(dim=-1).mean()\n",
    "            retriever_feedback_loss = -entropy\n",
    "        \n",
    "        # 3. Graph Supervision Loss\n",
    "        graph_supervision_loss = torch.tensor(0.0, device=llm_logits.device)\n",
    "        if self.use_graph_supervision and shortest_path_entities is not None:\n",
    "            if retrieved_entities is not None:\n",
    "                graph_supervision_loss = F.binary_cross_entropy_with_logits(\n",
    "                    retrieved_entities.float(), shortest_path_entities.float())\n",
    "        \n",
    "        total_loss = (self.alpha * accuracy_loss + self.beta * retriever_feedback_loss + \n",
    "                     self.gamma * graph_supervision_loss)\n",
    "        \n",
    "        loss_dict = {\n",
    "            'total_loss': total_loss.item(),\n",
    "            'accuracy_loss': accuracy_loss.item(),\n",
    "            'retriever_feedback_loss': retriever_feedback_loss.item(),\n",
    "            'graph_supervision_loss': graph_supervision_loss.item()\n",
    "        }\n",
    "        return total_loss, loss_dict\n",
    "\n",
    "print(\"JointTrainingLoss class defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAG (Self-Attention Graph) Pooling Layer (Algorithm 3)\n",
    "\n",
    "Implements Algorithm 3 from GRIL paper:\n",
    "- Computes self-attention scores A_s for entities in retrieved subgraph\n",
    "- Aggregates entity embeddings with attention weights\n",
    "- Projects to LLM embedding space via MLP\n",
    "- Outputs soft graph token h_GT for LLM input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SAGPooling initialized (node_dim=64, graph_token_dim=512)\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# SAG (SELF-ATTENTION GRAPH) POOLING LAYER (Algorithm 3)\n",
    "# =========================================\n",
    "class SAGPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements Algorithm 3: Self-Attention Graph Pooling\n",
    "    \n",
    "    Generates dense graph-level embedding (soft graph token) from retrieved subgraph.\n",
    "    Based on: Lee et al., \"Self-attention graph pooling\" (ICML 2019)\n",
    "    \n",
    "    Equation: h_GT = MLP(Σ_{ei∈Gs} A_si · h'_ei)\n",
    "    where:\n",
    "    - A_si: Self-attention score for entity e_i\n",
    "    - h'_ei: Updated entity embedding from Algorithm 1\n",
    "    - h_GT: Graph token embedding for LLM input\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 node_dim: int,\n",
    "                 graph_token_dim: int = 512,\n",
    "                 hidden_dim: int = 256,\n",
    "                 dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            node_dim: Dimension of node embeddings (from entity updater)\n",
    "            graph_token_dim: Output dimension for graph token (LLM embedding space)\n",
    "            hidden_dim: Hidden dimension for MLP\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.node_dim = node_dim\n",
    "        self.graph_token_dim = graph_token_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Self-attention mechanism: computes A_s ∈ R^{|Gs|×1}\n",
    "        # Single-layer MLP that outputs attention scores\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(node_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        # MLP to project aggregated embeddings to LLM embedding space\n",
    "        self.graph_token_mlp = nn.Sequential(\n",
    "            nn.Linear(node_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, graph_token_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, \n",
    "                node_embeddings: torch.Tensor,\n",
    "                node_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute graph token from node embeddings.\n",
    "        \n",
    "        Args:\n",
    "            node_embeddings: Entity embeddings [num_nodes, node_dim]\n",
    "                           Can be from updated_embeddings or initial GNN embeddings\n",
    "            node_mask: Optional mask to indicate which nodes to include [num_nodes]\n",
    "                      (1 for include, 0 for exclude)\n",
    "        \n",
    "        Returns:\n",
    "            graph_token: Graph-level embedding [graph_token_dim]\n",
    "        \"\"\"\n",
    "        if node_mask is not None:\n",
    "            # Apply mask: set masked nodes to zero\n",
    "            node_embeddings = node_embeddings * node_mask.unsqueeze(-1)\n",
    "            num_valid_nodes = node_mask.sum()\n",
    "        else:\n",
    "            num_valid_nodes = node_embeddings.size(0)\n",
    "        \n",
    "        # Step 1: Compute self-attention scores A_s ∈ R^{|Gs|×1}\n",
    "        attention_scores = self.attention(node_embeddings)  # [num_nodes, 1]\n",
    "        \n",
    "        # Apply mask to attention scores if provided\n",
    "        if node_mask is not None:\n",
    "            attention_scores = attention_scores * node_mask.unsqueeze(-1)\n",
    "            # Set masked nodes to very negative value for softmax\n",
    "            attention_scores = attention_scores + (1 - node_mask.unsqueeze(-1)) * (-1e9)\n",
    "        \n",
    "        # Normalize attention scores (softmax)\n",
    "        attention_weights = F.softmax(attention_scores, dim=0)  # [num_nodes, 1]\n",
    "        \n",
    "        # Step 2: Aggregate entity embeddings with attention weights\n",
    "        # Σ_{ei∈Gs} A_si · h'_ei\n",
    "        aggregated_embedding = (attention_weights * node_embeddings).sum(dim=0)  # [node_dim]\n",
    "        \n",
    "        # Step 3: Project to LLM embedding space via MLP\n",
    "        # h_GT = MLP(aggregated_embedding)\n",
    "        graph_token = self.graph_token_mlp(aggregated_embedding)  # [graph_token_dim]\n",
    "        \n",
    "        return graph_token, attention_weights.squeeze(-1)\n",
    "    \n",
    "    def compute_graph_token_from_subgraph(self,\n",
    "                                         subgraph: HeteroData,\n",
    "                                         updated_embeddings: Dict[str, torch.Tensor],\n",
    "                                         node_type: str = 'paper') -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Convenience method to compute graph token from a HeteroData subgraph.\n",
    "        \n",
    "        Args:\n",
    "            subgraph: Retrieved subgraph from Algorithm 1\n",
    "            updated_embeddings: Dictionary of updated embeddings from Algorithm 1\n",
    "            node_type: Primary node type to use for graph token (default: 'paper')\n",
    "        \n",
    "        Returns:\n",
    "            graph_token: Graph-level embedding [graph_token_dim]\n",
    "            attention_weights: Attention weights for each node [num_nodes]\n",
    "        \"\"\"\n",
    "        if node_type not in updated_embeddings:\n",
    "            # Fallback: use subgraph node features\n",
    "            if node_type in subgraph.node_types and hasattr(subgraph[node_type], 'x'):\n",
    "                node_embeddings = subgraph[node_type].x\n",
    "            else:\n",
    "                raise ValueError(f\"Node type {node_type} not found in updated_embeddings or subgraph\")\n",
    "        else:\n",
    "            # Get embeddings for nodes in subgraph\n",
    "            # Map subgraph node indices to full graph indices\n",
    "            if hasattr(subgraph[node_type], '_node_mapping'):\n",
    "                # Reverse mapping: subgraph index -> full graph index\n",
    "                subgraph_to_full = {v: k for k, v in subgraph[node_type]._node_mapping.items()}\n",
    "                num_subgraph_nodes = subgraph[node_type].num_nodes\n",
    "                node_embeddings = []\n",
    "                for i in range(num_subgraph_nodes):\n",
    "                    if i in subgraph_to_full:\n",
    "                        full_idx = subgraph_to_full[i]\n",
    "                        node_embeddings.append(updated_embeddings[node_type][full_idx])\n",
    "                    else:\n",
    "                        # Use subgraph features as fallback\n",
    "                        if hasattr(subgraph[node_type], 'x'):\n",
    "                            node_embeddings.append(subgraph[node_type].x[i])\n",
    "                        else:\n",
    "                            raise ValueError(f\"Cannot find embedding for subgraph node {i}\")\n",
    "                node_embeddings = torch.stack(node_embeddings)\n",
    "            else:\n",
    "                # No mapping available, use subgraph features directly\n",
    "                if hasattr(subgraph[node_type], 'x'):\n",
    "                    node_embeddings = subgraph[node_type].x\n",
    "                else:\n",
    "                    raise ValueError(f\"Subgraph node type {node_type} has no features\")\n",
    "        \n",
    "        return self.forward(node_embeddings)\n",
    "\n",
    "# Initialize SAG Pooling layer\n",
    "# node_dim should match the output dimension of EntityEmbeddingUpdater (64)\n",
    "sag_pooling = SAGPooling(\n",
    "    node_dim=64,  # From EntityEmbeddingUpdater output\n",
    "    graph_token_dim=512,  # LLM embedding dimension (will match LLM later)\n",
    "    hidden_dim=256,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "print(f\"SAGPooling initialized (node_dim=64, graph_token_dim=512)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: SAG Pooling with Algorithm 1 Output\n",
    "\n",
    "Test SAG Pooling layer with a retrieved subgraph from Algorithm 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TESTING SAG POOLING LAYER (Algorithm 3)\n",
      "================================================================================\n",
      " All dependencies available\n",
      " Found query paper: 17396995 (index: 18104)\n",
      " Query text length: 645 characters\n",
      " Seed entities: {'paper': [18104]}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Step 1: Running Enhanced Algorithm 1...\n",
      "--------------------------------------------------------------------------------\n",
      " Retrieved subgraph with 201 papers\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Step 2: Computing Graph Token with SAG Pooling...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      " SAG POOLING TEST PASSED!\n",
      "================================================================================\n",
      "Graph token statistics:\n",
      "  Graph token shape: torch.Size([512])\n",
      "  Graph token dimension: 512 (should match LLM embedding dim)\n",
      "  Attention weights shape: torch.Size([201])\n",
      "  Number of nodes: 201\n",
      "  Attention weights sum: 1.0000 (should be ~1.0)\n",
      "  Max attention weight: 0.0067\n",
      "  Min attention weight: 0.0034\n",
      "\n",
      "  Top 5 nodes by attention:\n",
      "    1. Node 20: attention = 0.0067\n",
      "    2. Node 111: attention = 0.0066\n",
      "    3. Node 28: attention = 0.0064\n",
      "    4. Node 36: attention = 0.0064\n",
      "    5. Node 60: attention = 0.0063\n",
      "\n",
      " SAG Pooling is working correctly!\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# TEST: SAG Pooling with Algorithm 1 Output\n",
    "# =========================================\n",
    "print(\"=\"*80)\n",
    "print(\"TESTING SAG POOLING LAYER (Algorithm 3)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check dependencies\n",
    "required = {\n",
    "    'model': model,\n",
    "    'data': data,\n",
    "    'paper2idx': paper2idx,\n",
    "    'all_df': all_df,\n",
    "    'query_encoder': query_encoder,\n",
    "    'relevance_scorer': relevance_scorer,\n",
    "    'entity_updater': entity_updater,\n",
    "    'sag_pooling': sag_pooling,\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "missing = [k for k, v in required.items() if v is None or k not in globals()]\n",
    "if missing:\n",
    "    print(f\"[ERROR] Missing dependencies: {missing}\")\n",
    "    print(\"Please run Part 1, Part 2, and SAG Pooling initialization cells first.\")\n",
    "else:\n",
    "    print(\"All dependencies available\")\n",
    "    \n",
    "    # Test with a known paper ID\n",
    "    query_paper_id = \"17396995\"  # Example paper from our dataset\n",
    "    \n",
    "    try:\n",
    "        # Check if paper exists\n",
    "        if query_paper_id not in paper2idx:\n",
    "            print(f\"[ERROR] Paper {query_paper_id} not found in paper2idx\")\n",
    "        else:\n",
    "            query_paper_idx = paper2idx[query_paper_id]\n",
    "            print(f\"[OK] Found query paper: {query_paper_id} (index: {query_paper_idx})\")\n",
    "            \n",
    "            # Get query text\n",
    "            query_row = all_df[all_df['publication_ID'] == int(query_paper_id)]\n",
    "            if len(query_row) == 0:\n",
    "                print(f\"[ERROR] Paper {query_paper_id} not found in all_df\")\n",
    "            else:\n",
    "                query_text = query_row['text'].values[0]\n",
    "                print(f\"[OK] Query text length: {len(query_text)} characters\")\n",
    "                \n",
    "                # Define seed entities\n",
    "                query_seed_entities = {'paper': [query_paper_idx]}\n",
    "                print(f\"[OK] Seed entities: {query_seed_entities}\")\n",
    "                \n",
    "                # Step 1: Run Enhanced Algorithm 1 to get subgraph\n",
    "                print(\"\\n\" + \"-\"*80)\n",
    "                print(\"Step 1: Running Enhanced Algorithm 1...\")\n",
    "                print(\"-\"*80)\n",
    "                \n",
    "                retrieved_subgraph, info_dict = attention_based_graph_retriever_enhanced(\n",
    "                    query_text=query_text,\n",
    "                    query_seed_entities=query_seed_entities,\n",
    "                    gnn_model=model,\n",
    "                    full_data=data,\n",
    "                    query_encoder=query_encoder,\n",
    "                    relevance_scorer=relevance_scorer,\n",
    "                    entity_updater=entity_updater,\n",
    "                    max_hops=2,\n",
    "                    relevance_threshold=0.1,\n",
    "                    max_nodes_per_hop=100,\n",
    "                    use_gumbel_softmax=True,\n",
    "                    gumbel_temperature=1.0,\n",
    "                    training=False,  # Inference mode\n",
    "                    device=device\n",
    "                )\n",
    "                \n",
    "                print(f\"[OK] Retrieved subgraph with {retrieved_subgraph['paper'].num_nodes} papers\")\n",
    "                \n",
    "                # Step 2: Compute graph token using SAG Pooling\n",
    "                print(\"\\n\" + \"-\"*80)\n",
    "                print(\"Step 2: Computing Graph Token with SAG Pooling...\")\n",
    "                print(\"-\"*80)\n",
    "                \n",
    "                # Get updated embeddings from Algorithm 1\n",
    "                updated_embeddings = info_dict.get('updated_embeddings', {})\n",
    "                \n",
    "                # Compute graph token\n",
    "                graph_token, attention_weights = sag_pooling.compute_graph_token_from_subgraph(\n",
    "                    subgraph=retrieved_subgraph,\n",
    "                    updated_embeddings=updated_embeddings,\n",
    "                    node_type='paper'\n",
    "                )\n",
    "                \n",
    "                # Verify results\n",
    "                print(\"\\n\" + \"=\"*80)\n",
    "                print(\"[OK] SAG POOLING TEST PASSED!\")\n",
    "                print(\"=\"*80)\n",
    "                print(f\"Graph token statistics:\")\n",
    "                print(f\"  Graph token shape: {graph_token.shape}\")\n",
    "                print(f\"  Graph token dimension: {graph_token.shape[0]} (should match LLM embedding dim)\")\n",
    "                print(f\"  Attention weights shape: {attention_weights.shape}\")\n",
    "                print(f\"  Number of nodes: {attention_weights.shape[0]}\")\n",
    "                print(f\"  Attention weights sum: {attention_weights.sum().item():.4f} (should be ~1.0)\")\n",
    "                print(f\"  Max attention weight: {attention_weights.max().item():.4f}\")\n",
    "                print(f\"  Min attention weight: {attention_weights.min().item():.4f}\")\n",
    "                \n",
    "                # Show top nodes by attention\n",
    "                top_k = min(5, len(attention_weights))\n",
    "                top_indices = torch.argsort(attention_weights, descending=True)[:top_k]\n",
    "                print(f\"\\n  Top {top_k} nodes by attention:\")\n",
    "                for i, idx in enumerate(top_indices):\n",
    "                    print(f\"    {i+1}. Node {idx.item()}: attention = {attention_weights[idx].item():.4f}\")\n",
    "                \n",
    "                print(\"\\n[OK] SAG Pooling is working correctly!\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] ERROR during SAG Pooling test:\")\n",
    "        print(f\"   {type(e).__name__}: {str(e)}\")\n",
    "        import traceback\n",
    "        print(\"\\nFull traceback:\")\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Integration (Llama3-8B + LoRA)\n",
    "\n",
    "This section implements:\n",
    "- **Verbalization**: Convert graph triples to natural language text\n",
    "- **LLM Integration**: Load Llama3-8B with LoRA fine-tuning\n",
    "- **Input Formatting**: Combine Graph Token + Verbalized Triples + Question\n",
    "- **Joint Training Support**: Enable end-to-end training with retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# INSTALL REQUIRED PACKAGES FOR LLM\n",
    "# =========================================\n",
    "# !pip install -U transformers peft accelerate bitsandbytes huggingface_hub\n",
    "\n",
    "try:\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "    from peft import LoraConfig, get_peft_model, TaskType\n",
    "    from huggingface_hub import login\n",
    "    print(\"Required libraries available\")\n",
    "except ImportError:\n",
    "    print(\"[WARNING]  Please install: pip install -U transformers peft accelerate bitsandbytes huggingface_hub\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verbalization: Convert Triples to Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# VERBALIZATION: Convert Graph Triples to Text\n",
    "# =========================================\n",
    "from typing import List, Tuple, Dict\n",
    "import torch\n",
    "\n",
    "def verbalize_triples(\n",
    "    subgraph: HeteroData,\n",
    "    updated_embeddings: Dict[str, torch.Tensor],\n",
    "    all_df: 'pd.DataFrame',\n",
    "    paper2idx: Dict[str, int],\n",
    "    author2idx: Dict[str, int] = None,\n",
    "    keyword2idx: Dict[str, int] = None,\n",
    "    venue2idx: Dict[str, int] = None,\n",
    "    max_triples: int = 50,\n",
    "    relevance_scores: Dict[Tuple[str, str, str], torch.Tensor] = None\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Convert graph triples to natural language text format.\n",
    "    \n",
    "    Format: \"<entity_source → relation → entity_target>\"\n",
    "    \n",
    "    Args:\n",
    "        subgraph: Retrieved subgraph from Algorithm 1\n",
    "        updated_embeddings: Updated embeddings from Algorithm 1\n",
    "        all_df: DataFrame with paper metadata\n",
    "        paper2idx: Paper ID to index mapping\n",
    "        author2idx: Author ID to index mapping (optional)\n",
    "        keyword2idx: Keyword to index mapping (optional)\n",
    "        venue2idx: Venue to index mapping (optional)\n",
    "        max_triples: Maximum number of triples to include\n",
    "        relevance_scores: Optional relevance scores for ranking triples\n",
    "    \n",
    "    Returns:\n",
    "        List of verbalized triple strings\n",
    "    \"\"\"\n",
    "    verbalized_triples = []\n",
    "    triple_scores = []\n",
    "    \n",
    "    # Helper function to get entity name from index\n",
    "    def get_entity_name(node_type: str, idx: int) -> str:\n",
    "        if node_type == 'paper':\n",
    "            # Find paper ID from index\n",
    "            paper_id = None\n",
    "            for pid, i in paper2idx.items():\n",
    "                if i == idx:\n",
    "                    paper_id = pid\n",
    "                    break\n",
    "            if paper_id:\n",
    "                row = all_df[all_df['publication_ID'] == int(paper_id)]\n",
    "                if len(row) > 0:\n",
    "                    title = row['title'].values[0] if 'title' in row.columns else f\"Paper {paper_id}\"\n",
    "                    return title[:100]  # Truncate long titles\n",
    "            return f\"Paper_{idx}\"\n",
    "        elif node_type == 'author' and author2idx:\n",
    "            for aid, i in author2idx.items():\n",
    "                if i == idx:\n",
    "                    return str(aid)\n",
    "            return f\"Author_{idx}\"\n",
    "        elif node_type == 'keyword' and keyword2idx:\n",
    "            for kw, i in keyword2idx.items():\n",
    "                if i == idx:\n",
    "                    return kw\n",
    "            return f\"Keyword_{idx}\"\n",
    "        elif node_type == 'venue' and venue2idx:\n",
    "            for v, i in venue2idx.items():\n",
    "                if i == idx:\n",
    "                    return v\n",
    "            return f\"Venue_{idx}\"\n",
    "        elif node_type == 'pubDate':\n",
    "            return f\"Year_{idx}\"\n",
    "        else:\n",
    "            return f\"{node_type}_{idx}\"\n",
    "    \n",
    "    # Process each edge type\n",
    "    for etype in subgraph.edge_types:\n",
    "        if etype not in subgraph.edge_types:\n",
    "            continue\n",
    "        \n",
    "        src_type, rel_type, dst_type = etype\n",
    "        edge_index = subgraph[etype].edge_index\n",
    "        \n",
    "        if edge_index.size(1) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Get relevance scores for this edge type if available\n",
    "        edge_scores = None\n",
    "        if relevance_scores and etype in relevance_scores:\n",
    "            edge_scores = relevance_scores[etype]\n",
    "        \n",
    "        # Process each edge\n",
    "        for i in range(edge_index.size(1)):\n",
    "            src_idx = edge_index[0, i].item()\n",
    "            dst_idx = edge_index[1, i].item()\n",
    "            \n",
    "            # Map subgraph indices to full graph indices if mapping exists\n",
    "            if hasattr(subgraph[src_type], '_node_mapping'):\n",
    "                src_mapping = {v: k for k, v in subgraph[src_type]._node_mapping.items()}\n",
    "                if src_idx in src_mapping:\n",
    "                    src_full_idx = src_mapping[src_idx]\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                src_full_idx = src_idx\n",
    "            \n",
    "            if hasattr(subgraph[dst_type], '_node_mapping'):\n",
    "                dst_mapping = {v: k for k, v in subgraph[dst_type]._node_mapping.items()}\n",
    "                if dst_idx in dst_mapping:\n",
    "                    dst_full_idx = dst_mapping[dst_idx]\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                dst_full_idx = dst_idx\n",
    "            \n",
    "            # Get entity names\n",
    "            src_name = get_entity_name(src_type, src_full_idx)\n",
    "            dst_name = get_entity_name(dst_type, dst_full_idx)\n",
    "            rel_name = rel_type.replace('_', ' ').title()\n",
    "            \n",
    "            # Create verbalized triple\n",
    "            triple_text = f\"<{src_name} → {rel_name} → {dst_name}>\"\n",
    "            \n",
    "            # Get score for ranking\n",
    "            score = 1.0\n",
    "            if edge_scores is not None:\n",
    "                if isinstance(edge_scores, torch.Tensor) and i < len(edge_scores):\n",
    "                    score = edge_scores[i].item() if isinstance(edge_scores[i], torch.Tensor) else edge_scores[i]\n",
    "            \n",
    "            verbalized_triples.append(triple_text)\n",
    "            triple_scores.append(score)\n",
    "    \n",
    "    # Sort by relevance score (if available) and take top max_triples\n",
    "    if triple_scores:\n",
    "        sorted_indices = sorted(range(len(triple_scores)), key=lambda i: triple_scores[i], reverse=True)\n",
    "        verbalized_triples = [verbalized_triples[i] for i in sorted_indices[:max_triples]]\n",
    "    else:\n",
    "        verbalized_triples = verbalized_triples[:max_triples]\n",
    "    \n",
    "    return verbalized_triples\n",
    "\n",
    "print(\"Verbalization function defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Integration: Llama3-8B with LoRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# LLM INTEGRATION: Llama3-8B with LoRA\n",
    "# =========================================\n",
    "class GRIL_LLM(nn.Module):\n",
    "    \"\"\"\n",
    "    LLM Reasoner with Graph Token Integration for GRIL.\n",
    "    \n",
    "    Architecture:\n",
    "    - Base Model: Llama3-8B\n",
    "    - Fine-tuning: LoRA (rank 8)\n",
    "    - Input Format: [Graph Token] + Reasoning Paths + Question\n",
    "    - Output: Answer logits P_{φ,ψ}(a|Gs, q)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 model_name: str = \"meta-llama/Meta-Llama-3-8B\",\n",
    "                 use_4bit: bool = True,\n",
    "                 lora_rank: int = 8,\n",
    "                 lora_alpha: int = 16,\n",
    "                 lora_dropout: float = 0.1,\n",
    "                 device: torch.device = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_name: HuggingFace model name for Llama3-8B\n",
    "            use_4bit: Whether to use 4-bit quantization (saves memory)\n",
    "            lora_rank: LoRA rank (default: 8 as per GRIL paper)\n",
    "            lora_alpha: LoRA alpha parameter\n",
    "            lora_dropout: LoRA dropout rate\n",
    "            device: Device to run model on\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.use_4bit = use_4bit\n",
    "        \n",
    "        # Load tokenizer\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            print(f\"[OK] Tokenizer loaded: {model_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING]  Could not load tokenizer: {e}\")\n",
    "            print(\"   Make sure you've authenticated with HuggingFace and have access to the model\")\n",
    "            self.tokenizer = None\n",
    "        \n",
    "        # Load model with quantization if requested\n",
    "        try:\n",
    "            if use_4bit:\n",
    "                quantization_config = BitsAndBytesConfig(\n",
    "                    load_in_4bit=True,\n",
    "                    bnb_4bit_compute_dtype=torch.float16,\n",
    "                    bnb_4bit_use_double_quant=True,\n",
    "                    bnb_4bit_quant_type=\"nf4\"\n",
    "                )\n",
    "            else:\n",
    "                quantization_config = None\n",
    "            \n",
    "            self.base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                quantization_config=quantization_config,\n",
    "                device_map=\"auto\" if use_4bit else None,\n",
    "                trust_remote_code=True,\n",
    "                torch_dtype=torch.float16 if use_4bit else torch.float32\n",
    "            )\n",
    "            \n",
    "            if not use_4bit:\n",
    "                self.base_model = self.base_model.to(self.device)\n",
    "            \n",
    "            print(f\"[OK] Base model loaded: {model_name}\")\n",
    "            \n",
    "            # Apply LoRA\n",
    "            lora_config = LoraConfig(\n",
    "                task_type=TaskType.CAUSAL_LM,\n",
    "                r=lora_rank,\n",
    "                lora_alpha=lora_alpha,\n",
    "                lora_dropout=lora_dropout,\n",
    "                target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "                bias=\"none\"\n",
    "            )\n",
    "            \n",
    "            self.model = get_peft_model(self.base_model, lora_config)\n",
    "            print(f\"[OK] LoRA applied (rank={lora_rank}, alpha={lora_alpha})\")\n",
    "            \n",
    "            # Get embedding dimension\n",
    "            self.embedding_dim = self.model.get_input_embeddings().embedding_dim\n",
    "            print(f\"[OK] Model embedding dimension: {self.embedding_dim}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING]  Could not load model: {e}\")\n",
    "            print(\"   Make sure you've authenticated and have access to meta-llama/Meta-Llama-3-8B\")\n",
    "            self.model = None\n",
    "            self.embedding_dim = 512\n",
    "    \n",
    "    def format_input(self,\n",
    "                    graph_token: torch.Tensor,\n",
    "                    verbalized_triples: List[str],\n",
    "                    question: str,\n",
    "                    max_length: int = 2048) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Format input for LLM: [Graph Token] + Reasoning Paths + Question\n",
    "        \n",
    "        Args:\n",
    "            graph_token: Graph token embedding from SAG [graph_token_dim]\n",
    "            verbalized_triples: List of verbalized triple strings\n",
    "            question: Question text\n",
    "            max_length: Maximum sequence length\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with input_ids and attention_mask\n",
    "        \"\"\"\n",
    "        if self.tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer not loaded. Cannot format input.\")\n",
    "        \n",
    "        # Create reasoning paths text\n",
    "        reasoning_paths = \"\\\\n\".join(verbalized_triples)\n",
    "        \n",
    "        # Format prompt according to GRIL specification\n",
    "        prompt = f\"[Graph Token] Based on the following reasoning paths, please answer the given question.\\\\n\\\\nReasoning Paths: {reasoning_paths}\\\\n\\\\nQuestion: {question}\\\\n\\\\nAnswer:\"\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        return inputs, prompt\n",
    "    \n",
    "    def forward(self,\n",
    "                graph_token: torch.Tensor,\n",
    "                verbalized_triples: List[str],\n",
    "                question: str,\n",
    "                labels: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass through LLM with graph token integration.\n",
    "        \n",
    "        Args:\n",
    "            graph_token: Graph token embedding from SAG [graph_token_dim]\n",
    "            verbalized_triples: List of verbalized triple strings\n",
    "            question: Question text\n",
    "            labels: Optional labels for training [batch_size, seq_len]\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with logits and loss (if labels provided)\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not loaded. Cannot perform forward pass.\")\n",
    "        \n",
    "        # Format input\n",
    "        inputs, prompt = self.format_input(graph_token, verbalized_triples, question)\n",
    "        \n",
    "        # Get input embeddings\n",
    "        input_embeddings = self.model.get_input_embeddings()(inputs['input_ids'])\n",
    "        \n",
    "        # Project graph token to model embedding space if needed\n",
    "        if graph_token.shape[0] != self.embedding_dim:\n",
    "            # Use a simple linear projection (could be made learnable)\n",
    "            if not hasattr(self, 'graph_token_proj'):\n",
    "                self.graph_token_proj = nn.Linear(graph_token.shape[0], self.embedding_dim).to(self.device)\n",
    "            graph_token_proj = self.graph_token_proj(graph_token.unsqueeze(0))  # [1, embedding_dim]\n",
    "        else:\n",
    "            graph_token_proj = graph_token.unsqueeze(0)  # [1, embedding_dim]\n",
    "        \n",
    "        # Find where to insert graph token (at the beginning, before [Graph Token] token)\n",
    "        # For simplicity, prepend to input embeddings\n",
    "        # In practice, you'd find the [Graph Token] token position and replace it\n",
    "        batch_size = input_embeddings.size(0)\n",
    "        graph_token_expanded = graph_token_proj.expand(batch_size, -1)  # [batch_size, embedding_dim]\n",
    "        \n",
    "        # Prepend graph token to input embeddings\n",
    "        # This is a simplified approach - in practice, you'd replace the [Graph Token] token embedding\n",
    "        combined_embeddings = torch.cat([graph_token_expanded, input_embeddings], dim=1)  # [batch_size, seq_len+1, embedding_dim]\n",
    "        \n",
    "        # Adjust attention mask\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        graph_token_mask = torch.ones(batch_size, 1, device=self.device, dtype=attention_mask.dtype)\n",
    "        combined_attention_mask = torch.cat([graph_token_mask, attention_mask], dim=1)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = self.model(\n",
    "            inputs_embeds=combined_embeddings,\n",
    "            attention_mask=combined_attention_mask,\n",
    "            labels=labels,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'logits': outputs.logits,\n",
    "            'loss': outputs.loss if labels is not None else None,\n",
    "            'prompt': prompt\n",
    "        }\n",
    "    \n",
    "    def generate(self,\n",
    "                graph_token: torch.Tensor,\n",
    "                verbalized_triples: List[str],\n",
    "                question: str,\n",
    "                max_new_tokens: int = 100,\n",
    "                temperature: float = 0.7) -> str:\n",
    "        \"\"\"\n",
    "        Generate answer from graph token and question.\n",
    "        \n",
    "        Args:\n",
    "            graph_token: Graph token embedding from SAG\n",
    "            verbalized_triples: List of verbalized triple strings\n",
    "            question: Question text\n",
    "            max_new_tokens: Maximum number of tokens to generate\n",
    "            temperature: Sampling temperature\n",
    "        \n",
    "        Returns:\n",
    "            Generated answer text\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not loaded. Cannot generate.\")\n",
    "        \n",
    "        # Format input\n",
    "        inputs, _ = self.format_input(graph_token, verbalized_triples, question)\n",
    "        \n",
    "        # Get input embeddings\n",
    "        input_embeddings = self.model.get_input_embeddings()(inputs['input_ids'])\n",
    "        \n",
    "        # Project and prepend graph token (same as forward)\n",
    "        if graph_token.shape[0] != self.embedding_dim:\n",
    "            if not hasattr(self, 'graph_token_proj'):\n",
    "                self.graph_token_proj = nn.Linear(graph_token.shape[0], self.embedding_dim).to(self.device)\n",
    "            graph_token_proj = self.graph_token_proj(graph_token.unsqueeze(0))\n",
    "        else:\n",
    "            graph_token_proj = graph_token.unsqueeze(0)\n",
    "        \n",
    "        batch_size = input_embeddings.size(0)\n",
    "        graph_token_expanded = graph_token_proj.expand(batch_size, -1)\n",
    "        combined_embeddings = torch.cat([graph_token_expanded, input_embeddings], dim=1)\n",
    "        \n",
    "        attention_mask = inputs['attention_mask']\n",
    "        graph_token_mask = torch.ones(batch_size, 1, device=self.device, dtype=attention_mask.dtype)\n",
    "        combined_attention_mask = torch.cat([graph_token_mask, attention_mask], dim=1)\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                inputs_embeds=combined_embeddings,\n",
    "                attention_mask=combined_attention_mask,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract answer (text after \"Answer:\")\n",
    "        if \"Answer:\" in generated_text:\n",
    "            answer = generated_text.split(\"Answer:\")[-1].strip()\n",
    "        else:\n",
    "            answer = generated_text\n",
    "        \n",
    "        return answer\n",
    "\n",
    "# =========================================\n",
    "# HUGGINGFACE AUTHENTICATION\n",
    "# =========================================\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"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\")\n",
    "print(\"[OK] HuggingFace authenticated\")\n",
    "\n",
    "print(\"GRIL_LLM class defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# TEST: LLM Integration (Full Pipeline)\n",
    "# =========================================\n",
    "print(\"=\"*80)\n",
    "print(\"TESTING LLM INTEGRATION (Full GRIL Pipeline)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check dependencies\n",
    "required = {\n",
    "    'model': model,\n",
    "    'data': data,\n",
    "    'paper2idx': paper2idx,\n",
    "    'all_df': all_df,\n",
    "    'query_encoder': query_encoder,\n",
    "    'relevance_scorer': relevance_scorer,\n",
    "    'entity_updater': entity_updater,\n",
    "    'sag_pooling': sag_pooling,\n",
    "    'author2idx': author2idx,\n",
    "    'keyword2idx': keyword2idx,\n",
    "    'venue2idx': venue2idx,\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "missing = [k for k, v in required.items() if v is None or k not in globals()]\n",
    "if missing:\n",
    "    print(f\"[ERROR] Missing dependencies: {missing}\")\n",
    "else:\n",
    "    print(\"All dependencies available\")\n",
    "    \n",
    "    # Initialize LLM (this will attempt to load the model)\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"Initializing LLM (Llama3-8B + LoRA)...\")\n",
    "    print(\"-\"*80)\n",
    "    print(\"[WARNING]  This requires HuggingFace authentication and model access\")\n",
    "    \n",
    "    try:\n",
    "        llm = GRIL_LLM(\n",
    "            model_name=\"meta-llama/Meta-Llama-3-8B\",\n",
    "            use_4bit=True,  # Use 4-bit quantization to save memory\n",
    "            lora_rank=8,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        if llm.model is None:\n",
    "            print(\"[WARNING]  Model not loaded. Skipping LLM test.\")\n",
    "            print(\"   Please authenticate and request access to Llama3-8B\")\n",
    "        else:\n",
    "            # Test with a query\n",
    "            query_paper_id = \"17396995\"\n",
    "            query_paper_idx = paper2idx[query_paper_id]\n",
    "            query_row = all_df[all_df['publication_ID'] == int(query_paper_id)]\n",
    "            query_text = query_row['text'].values[0]\n",
    "            question = f\"What papers are related to: {query_row['title'].values[0]}?\"\n",
    "            \n",
    "            print(f\"\\n[OK] Query paper: {query_paper_id}\")\n",
    "            print(f\"[OK] Question: {question}\")\n",
    "            \n",
    "            # Step 1: Run Algorithm 1\n",
    "            print(\"\\n\" + \"-\"*80)\n",
    "            print(\"Step 1: Running Algorithm 1...\")\n",
    "            print(\"-\"*80)\n",
    "            \n",
    "            retrieved_subgraph, info_dict = attention_based_graph_retriever_enhanced(\n",
    "                query_text=query_text,\n",
    "                query_seed_entities={'paper': [query_paper_idx]},\n",
    "                gnn_model=model,\n",
    "                full_data=data,\n",
    "                query_encoder=query_encoder,\n",
    "                relevance_scorer=relevance_scorer,\n",
    "                entity_updater=entity_updater,\n",
    "                max_hops=2,\n",
    "                relevance_threshold=0.1,\n",
    "                max_nodes_per_hop=50,  # Smaller for testing\n",
    "                use_gumbel_softmax=True,\n",
    "                training=False,\n",
    "                device=device\n",
    "            )\n",
    "            \n",
    "            print(f\"[OK] Retrieved {retrieved_subgraph['paper'].num_nodes} papers\")\n",
    "            \n",
    "            # Step 2: Compute Graph Token with SAG\n",
    "            print(\"\\n\" + \"-\"*80)\n",
    "            print(\"Step 2: Computing Graph Token with SAG...\")\n",
    "            print(\"-\"*80)\n",
    "            \n",
    "            updated_embeddings = info_dict.get('updated_embeddings', {})\n",
    "            graph_token, attention_weights = sag_pooling.compute_graph_token_from_subgraph(\n",
    "                subgraph=retrieved_subgraph,\n",
    "                updated_embeddings=updated_embeddings,\n",
    "                node_type='paper'\n",
    "            )\n",
    "            \n",
    "            print(f\"[OK] Graph token computed: {graph_token.shape}\")\n",
    "            \n",
    "            # Step 3: Verbalize Triples\n",
    "            print(\"\\n\" + \"-\"*80)\n",
    "            print(\"Step 3: Verbalizing Triples...\")\n",
    "            print(\"-\"*80)\n",
    "            \n",
    "            verbalized_triples = verbalize_triples(\n",
    "                subgraph=retrieved_subgraph,\n",
    "                updated_embeddings=updated_embeddings,\n",
    "                all_df=all_df,\n",
    "                paper2idx=paper2idx,\n",
    "                author2idx=author2idx,\n",
    "                keyword2idx=keyword2idx,\n",
    "                venue2idx=venue2idx,\n",
    "                max_triples=20  # Limit for testing\n",
    "            )\n",
    "            \n",
    "            print(f\"[OK] Verbalized {len(verbalized_triples)} triples\")\n",
    "            print(f\"   Example: {verbalized_triples[0] if verbalized_triples else 'None'}\")\n",
    "            \n",
    "            # Step 4: LLM Forward Pass\n",
    "            print(\"\\n\" + \"-\"*80)\n",
    "            print(\"Step 4: LLM Forward Pass...\")\n",
    "            print(\"-\"*80)\n",
    "            \n",
    "            outputs = llm.forward(\n",
    "                graph_token=graph_token,\n",
    "                verbalized_triples=verbalized_triples,\n",
    "                question=question\n",
    "            )\n",
    "            \n",
    "            print(f\"[OK] LLM forward pass completed\")\n",
    "            print(f\"   Logits shape: {outputs['logits'].shape}\")\n",
    "            \n",
    "            # Step 5: Generate Answer (optional, can be slow)\n",
    "            print(\"\\n\" + \"-\"*80)\n",
    "            print(\"Step 5: Generating Answer (this may take a while)...\")\n",
    "            print(\"-\"*80)\n",
    "            \n",
    "            answer = llm.generate(\n",
    "                graph_token=graph_token,\n",
    "                verbalized_triples=verbalized_triples,\n",
    "                question=question,\n",
    "                max_new_tokens=50,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            \n",
    "            print(f\"\\n[OK] Generated Answer:\")\n",
    "            print(f\"   {answer}\")\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"[OK] FULL GRIL PIPELINE TEST PASSED!\")\n",
    "            print(\"=\"*80)\n",
    "            print(\"Pipeline: Algorithm 1 → SAG → Verbalization → LLM\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] ERROR during LLM integration test:\")\n",
    "        print(f\"   {type(e).__name__}: {str(e)}\")\n",
    "        import traceback\n",
    "        print(\"\\nFull traceback:\")\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint Training Framework (Algorithm 4)\n",
    "\n",
    "Complete implementation of end-to-end training with:\n",
    "- LLM accuracy loss\n",
    "- Retriever feedback loss (with stop-gradient)\n",
    "- Graph supervision loss (optional)\n",
    "- Separate optimizers for retriever and LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# ENHANCED JOINT TRAINING LOSS (Algorithm 4)\n",
    "# =========================================\n",
    "class JointTrainingLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Enhanced Joint Training Loss with proper stop-gradient mechanism.\n",
    "    \n",
    "    Implements Algorithm 4 from GRIL paper:\n",
    "    L_joint = L_accuracy + L_retriever + L_supervision\n",
    "    \n",
    "    Key: Stop-gradient on LLM/SAG when updating retriever\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha: float = 1.0, beta: float = 1.0, gamma: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # Weight for LLM accuracy loss\n",
    "        self.beta = beta    # Weight for retriever feedback loss\n",
    "        self.gamma = gamma  # Weight for graph supervision loss\n",
    "    \n",
    "    def forward(self,\n",
    "                llm_logits: torch.Tensor,\n",
    "                ground_truth: torch.Tensor,\n",
    "                triplet_logits: Optional[torch.Tensor] = None,\n",
    "                shortest_path_mask: Optional[torch.Tensor] = None,\n",
    "                retrieved_mask: Optional[torch.Tensor] = None,\n",
    "                use_stop_gradient: bool = True) -> Tuple[torch.Tensor, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Compute joint training loss with stop-gradient mechanism.\n",
    "        \n",
    "        Args:\n",
    "            llm_logits: LLM output logits [batch_size, vocab_size] or [batch_size, seq_len, vocab_size]\n",
    "            ground_truth: Ground truth labels [batch_size] or [batch_size, seq_len]\n",
    "            triplet_logits: Triplet probability logits from retriever [num_triplets]\n",
    "            shortest_path_mask: Binary mask for entities on shortest paths [num_entities]\n",
    "            retrieved_mask: Binary mask for retrieved entities [num_entities]\n",
    "            use_stop_gradient: Whether to use stop-gradient for retriever feedback\n",
    "        \n",
    "        Returns:\n",
    "            total_loss: Combined loss tensor\n",
    "            loss_dict: Dictionary with individual loss components\n",
    "        \"\"\"\n",
    "        # 1. LLM Accuracy Loss: L_accuracy = -log P_{φ,ψ}(a|Gs, q)\n",
    "        if llm_logits.dim() == 3:  # [batch, seq_len, vocab_size]\n",
    "            # Shift logits and labels for language modeling\n",
    "            shift_logits = llm_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = ground_truth[..., 1:].contiguous()\n",
    "            accuracy_loss = F.cross_entropy(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                shift_labels.view(-1),\n",
    "                ignore_index=-100\n",
    "            )\n",
    "        elif llm_logits.dim() == 2:\n",
    "            if ground_truth.dtype == torch.long:\n",
    "                accuracy_loss = F.cross_entropy(llm_logits, ground_truth)\n",
    "            else:\n",
    "                accuracy_loss = F.binary_cross_entropy_with_logits(llm_logits, ground_truth)\n",
    "        else:\n",
    "            accuracy_loss = F.mse_loss(llm_logits, ground_truth.float())\n",
    "        \n",
    "        # 2. Retriever Feedback Loss: L_retriever = -log(P_{φ,ψ}(a|Gs, q) · P_θ(Gs|q))\n",
    "        # Uses LLM logits as implicit feedback with stop-gradient\n",
    "        retriever_feedback_loss = torch.tensor(0.0, device=llm_logits.device)\n",
    "        if triplet_logits is not None:\n",
    "            # Get LLM probability (stop-gradient if requested)\n",
    "            if use_stop_gradient:\n",
    "                llm_probs = F.softmax(llm_logits.detach(), dim=-1)\n",
    "            else:\n",
    "                llm_probs = F.softmax(llm_logits, dim=-1)\n",
    "            \n",
    "            # Use max probability as feedback signal\n",
    "            max_llm_prob = llm_probs.max(dim=-1)[0].mean()\n",
    "            \n",
    "            # Triplet probabilities from retriever\n",
    "            triplet_probs = F.softmax(triplet_logits, dim=-1)\n",
    "            \n",
    "            # Combined probability: P(a|Gs, q) · P(Gs|q)\n",
    "            # Use negative log-likelihood of combined probability\n",
    "            combined_prob = max_llm_prob * triplet_probs.mean()\n",
    "            retriever_feedback_loss = -torch.log(combined_prob + 1e-10)\n",
    "        \n",
    "        # 3. Graph Supervision Loss: L_supervision = BCE(Gs covers P(q,a))\n",
    "        graph_supervision_loss = torch.tensor(0.0, device=llm_logits.device)\n",
    "        if shortest_path_mask is not None and retrieved_mask is not None:\n",
    "            # Binary cross-entropy: supervise retrieved entities to cover shortest path entities\n",
    "            graph_supervision_loss = F.binary_cross_entropy_with_logits(\n",
    "                retrieved_mask.float(),\n",
    "                shortest_path_mask.float()\n",
    "            )\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = (self.alpha * accuracy_loss + \n",
    "                     self.beta * retriever_feedback_loss + \n",
    "                     self.gamma * graph_supervision_loss)\n",
    "        \n",
    "        loss_dict = {\n",
    "            'total_loss': total_loss.item(),\n",
    "            'accuracy_loss': accuracy_loss.item(),\n",
    "            'retriever_feedback_loss': retriever_feedback_loss.item(),\n",
    "            'graph_supervision_loss': graph_supervision_loss.item()\n",
    "        }\n",
    "        \n",
    "        return total_loss, loss_dict\n",
    "\n",
    "print(\"Enhanced JointTrainingLoss class defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Supervision: Shortest Path Extraction (Algorithm 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# GRAPH SUPERVISION: Shortest Path Extraction (Algorithm 5)\n",
    "# =========================================\n",
    "from collections import deque\n",
    "from typing import Set\n",
    "\n",
    "def extract_shortest_paths(\n",
    "    query_entities: Set[int],\n",
    "    answer_entities: Set[int],\n",
    "    full_data: HeteroData,\n",
    "    node_type: str = 'paper'\n",
    ") -> Set[int]:\n",
    "    \"\"\"\n",
    "    Extract entities on shortest paths between query and answer entities.\n",
    "    \n",
    "    Implements Algorithm 5: P(q, a) = entities on shortest paths\n",
    "    \n",
    "    Args:\n",
    "        query_entities: Set of query entity indices\n",
    "        answer_entities: Set of answer entity indices\n",
    "        full_data: Full HeteroData graph\n",
    "        node_type: Node type to consider (default: 'paper')\n",
    "    \n",
    "    Returns:\n",
    "        Set of entity indices on shortest paths\n",
    "    \"\"\"\n",
    "    if node_type not in full_data.node_types:\n",
    "        return set()\n",
    "    \n",
    "    # Get edge index for the node type (paper-cites-paper)\n",
    "    edge_index = None\n",
    "    for etype in full_data.edge_types:\n",
    "        if etype == (node_type, 'cites', node_type):\n",
    "            edge_index = full_data[etype].edge_index\n",
    "            break\n",
    "    \n",
    "    if edge_index is None:\n",
    "        return set()\n",
    "    \n",
    "    # Build adjacency list\n",
    "    num_nodes = full_data[node_type].num_nodes\n",
    "    adj_list = [[] for _ in range(num_nodes)]\n",
    "    for i in range(edge_index.size(1)):\n",
    "        src = edge_index[0, i].item()\n",
    "        dst = edge_index[1, i].item()\n",
    "        adj_list[src].append(dst)\n",
    "    \n",
    "    # BFS to find shortest paths from query entities to answer entities\n",
    "    path_entities = set()\n",
    "    \n",
    "    for query_entity in query_entities:\n",
    "        if query_entity >= num_nodes:\n",
    "            continue\n",
    "        \n",
    "        # BFS from query entity\n",
    "        queue = deque([(query_entity, [query_entity])])\n",
    "        visited = {query_entity}\n",
    "        found_paths = []\n",
    "        \n",
    "        while queue:\n",
    "            current, path = queue.popleft()\n",
    "            \n",
    "            # Check if we reached an answer entity\n",
    "            if current in answer_entities:\n",
    "                found_paths.append(path)\n",
    "                # Don't continue from answer entities\n",
    "                continue\n",
    "            \n",
    "            # Explore neighbors\n",
    "            for neighbor in adj_list[current]:\n",
    "                if neighbor not in visited:\n",
    "                    visited.add(neighbor)\n",
    "                    queue.append((neighbor, path + [neighbor]))\n",
    "        \n",
    "        # Collect all entities on shortest paths\n",
    "        for path in found_paths:\n",
    "            path_entities.update(path)\n",
    "    \n",
    "    return path_entities\n",
    "\n",
    "def create_entity_mask(\n",
    "    path_entities: Set[int],\n",
    "    all_entities: Set[int],\n",
    "    num_entities: int\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create binary mask for entities on shortest paths.\n",
    "    \n",
    "    Args:\n",
    "        path_entities: Set of entity indices on shortest paths\n",
    "        all_entities: Set of all entity indices to consider\n",
    "        num_entities: Total number of entities\n",
    "    \n",
    "    Returns:\n",
    "        Binary mask tensor [num_entities] (1 for path entities, 0 otherwise)\n",
    "    \"\"\"\n",
    "    mask = torch.zeros(num_entities, dtype=torch.float)\n",
    "    for entity_idx in path_entities:\n",
    "        if entity_idx < num_entities:\n",
    "            mask[entity_idx] = 1.0\n",
    "    return mask\n",
    "\n",
    "print(\"Graph supervision functions defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint Training Loop\n",
    "\n",
    "Complete training loop that:\n",
    "- Runs forward pass through full pipeline\n",
    "- Computes joint loss with stop-gradient\n",
    "- Updates retriever and LLM separately\n",
    "- Includes checkpointing and evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# JOINT TRAINING LOOP\n",
    "# =========================================\n",
    "class GRILTrainer:\n",
    "    \"\"\"\n",
    "    Complete joint training framework for GRIL.\n",
    "    \n",
    "    Manages:\n",
    "    - Forward pass through Algorithm 1 → SAG → LLM\n",
    "    - Joint loss computation with stop-gradient\n",
    "    - Separate optimizers for retriever and LLM\n",
    "    - Checkpointing and evaluation\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 gnn_model: nn.Module,\n",
    "                 query_encoder: QueryEncoder,\n",
    "                 relevance_scorer: RelevanceScorer,\n",
    "                 entity_updater: EntityEmbeddingUpdater,\n",
    "                 sag_pooling: SAGPooling,\n",
    "                 llm: GRIL_LLM,\n",
    "                 full_data: HeteroData,\n",
    "                 joint_loss: JointTrainingLoss,\n",
    "                 all_df: 'pd.DataFrame' = None,\n",
    "                 paper2idx: Dict[str, int] = None,\n",
    "                 author2idx: Dict[str, int] = None,\n",
    "                 keyword2idx: Dict[str, int] = None,\n",
    "                 venue2idx: Dict[str, int] = None,\n",
    "                 retriever_lr: float = 1e-4,\n",
    "                 llm_lr: float = 1e-5,\n",
    "                 device: torch.device = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            gnn_model: Pre-trained HeteroGNN (frozen)\n",
    "            query_encoder: Query encoder (frozen or trainable)\n",
    "            relevance_scorer: Relevance scorer (trainable)\n",
    "            entity_updater: Entity embedding updater (trainable)\n",
    "            sag_pooling: SAG pooling layer (trainable)\n",
    "            llm: LLM with LoRA (trainable)\n",
    "            full_data: Full graph data\n",
    "            joint_loss: Joint training loss function\n",
    "            retriever_lr: Learning rate for retriever components\n",
    "            llm_lr: Learning rate for LLM\n",
    "            device: Device to run on\n",
    "        \"\"\"\n",
    "        self.gnn_model = gnn_model\n",
    "        self.query_encoder = query_encoder\n",
    "        self.relevance_scorer = relevance_scorer\n",
    "        self.entity_updater = entity_updater\n",
    "        self.sag_pooling = sag_pooling\n",
    "        self.llm = llm\n",
    "        self.full_data = full_data\n",
    "        self.joint_loss = joint_loss\n",
    "        self.all_df = all_df\n",
    "        self.paper2idx = paper2idx\n",
    "        self.author2idx = author2idx\n",
    "        self.keyword2idx = keyword2idx\n",
    "        self.venue2idx = venue2idx\n",
    "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Freeze GNN (pre-trained)\n",
    "        for param in self.gnn_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Collect trainable parameters\n",
    "        retriever_params = []\n",
    "        retriever_params.extend(list(self.relevance_scorer.parameters()))\n",
    "        retriever_params.extend(list(self.entity_updater.parameters()))\n",
    "        retriever_params.extend(list(self.sag_pooling.parameters()))\n",
    "        \n",
    "        llm_params = []\n",
    "        if self.llm.model is not None:\n",
    "            # Only LoRA parameters are trainable\n",
    "            llm_params.extend([p for p in self.llm.model.parameters() if p.requires_grad])\n",
    "            if hasattr(self.llm, 'graph_token_proj'):\n",
    "                llm_params.extend(list(self.llm.graph_token_proj.parameters()))\n",
    "        \n",
    "        # Create optimizers\n",
    "        self.retriever_optimizer = torch.optim.Adam(retriever_params, lr=retriever_lr)\n",
    "        self.llm_optimizer = torch.optim.Adam(llm_params, lr=llm_lr) if llm_params else None\n",
    "        \n",
    "        print(f\"GRILTrainer initialized\")\n",
    "        print(f\"   Retriever parameters: {sum(p.numel() for p in retriever_params):,}\")\n",
    "        print(f\"   LLM parameters: {sum(p.numel() for p in llm_params):,}\")\n",
    "    \n",
    "    def forward_pass(self,\n",
    "                    query_text: str,\n",
    "                    query_seed_entities: Dict[str, List[int]],\n",
    "                    question: str,\n",
    "                    ground_truth: Optional[torch.Tensor] = None,\n",
    "                    answer_entities: Optional[Set[int]] = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Complete forward pass through GRIL pipeline.\n",
    "        \n",
    "        Args:\n",
    "            query_text: Query text for retrieval\n",
    "            query_seed_entities: Seed entities for Algorithm 1\n",
    "            question: Question text for LLM\n",
    "            ground_truth: Ground truth labels for training\n",
    "            answer_entities: Answer entity indices (for graph supervision)\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with outputs and intermediate results\n",
    "        \"\"\"\n",
    "        # Step 1: Algorithm 1 - Retrieve subgraph\n",
    "        retrieved_subgraph, info_dict = attention_based_graph_retriever_enhanced(\n",
    "            query_text=query_text,\n",
    "            query_seed_entities=query_seed_entities,\n",
    "            gnn_model=self.gnn_model,\n",
    "            full_data=self.full_data,\n",
    "            query_encoder=self.query_encoder,\n",
    "            relevance_scorer=self.relevance_scorer,\n",
    "            entity_updater=self.entity_updater,\n",
    "            max_hops=2,\n",
    "            relevance_threshold=0.1,\n",
    "            max_nodes_per_hop=100,\n",
    "            use_gumbel_softmax=True,\n",
    "            gumbel_temperature=1.0,\n",
    "            training=True,  # Enable gradients\n",
    "            device=self.device\n",
    "        )\n",
    "        \n",
    "        # Step 2: SAG Pooling - Compute graph token\n",
    "        updated_embeddings = info_dict.get('updated_embeddings', {})\n",
    "        graph_token, attention_weights = self.sag_pooling.compute_graph_token_from_subgraph(\n",
    "            subgraph=retrieved_subgraph,\n",
    "            updated_embeddings=updated_embeddings,\n",
    "            node_type='paper'\n",
    "        )\n",
    "        \n",
    "        # Step 3: Verbalize triples\n",
    "        verbalized_triples = verbalize_triples(\n",
    "            subgraph=retrieved_subgraph,\n",
    "            updated_embeddings=updated_embeddings,\n",
    "            all_df=self.all_df,\n",
    "            paper2idx=self.paper2idx,\n",
    "            author2idx=self.author2idx,\n",
    "            keyword2idx=self.keyword2idx,\n",
    "            venue2idx=self.venue2idx,\n",
    "            max_triples=50\n",
    "        )\n",
    "        \n",
    "        # Step 4: LLM forward pass\n",
    "        if self.llm.model is None:\n",
    "            # Mock LLM output for testing\n",
    "            llm_outputs = {\n",
    "                'logits': torch.randn(1, 100, 32000, device=self.device),  # Mock logits\n",
    "                'loss': None\n",
    "            }\n",
    "        else:\n",
    "            llm_outputs = self.llm.forward(\n",
    "                graph_token=graph_token,\n",
    "                verbalized_triples=verbalized_triples,\n",
    "                question=question,\n",
    "                labels=ground_truth\n",
    "            )\n",
    "        \n",
    "        # Step 5: Extract triplet logits for retriever feedback\n",
    "        triplet_logits = None\n",
    "        if 'triplet_logits' in info_dict:\n",
    "            # Concatenate all triplet logits\n",
    "            all_logits = []\n",
    "            for etype, logits_list in info_dict['triplet_logits'].items():\n",
    "                if isinstance(logits_list, list) and len(logits_list) > 0:\n",
    "                    if isinstance(logits_list[0], torch.Tensor):\n",
    "                        all_logits.append(torch.cat(logits_list))\n",
    "            if all_logits:\n",
    "                triplet_logits = torch.cat(all_logits)\n",
    "        \n",
    "        # Step 6: Graph supervision (if answer entities provided)\n",
    "        shortest_path_mask = None\n",
    "        retrieved_mask = None\n",
    "        if answer_entities is not None:\n",
    "            query_entity_set = set(query_seed_entities.get('paper', []))\n",
    "            path_entities = extract_shortest_paths(\n",
    "                query_entities=query_entity_set,\n",
    "                answer_entities=answer_entities,\n",
    "                full_data=self.full_data,\n",
    "                node_type='paper'\n",
    "            )\n",
    "            \n",
    "            # Create masks\n",
    "            num_papers = self.full_data['paper'].num_nodes\n",
    "            shortest_path_mask = create_entity_mask(path_entities, path_entities, num_papers).to(self.device)\n",
    "            \n",
    "            # Get retrieved entities\n",
    "            retrieved_papers = set()\n",
    "            if 'paper' in retrieved_subgraph.node_types:\n",
    "                if hasattr(retrieved_subgraph['paper'], '_node_mapping'):\n",
    "                    retrieved_papers = set(retrieved_subgraph['paper']._node_mapping.keys())\n",
    "            retrieved_mask = create_entity_mask(retrieved_papers, retrieved_papers, num_papers).to(self.device)\n",
    "        \n",
    "        return {\n",
    "            'llm_logits': llm_outputs['logits'],\n",
    "            'graph_token': graph_token,\n",
    "            'verbalized_triples': verbalized_triples,\n",
    "            'retrieved_subgraph': retrieved_subgraph,\n",
    "            'triplet_logits': triplet_logits,\n",
    "            'shortest_path_mask': shortest_path_mask,\n",
    "            'retrieved_mask': retrieved_mask,\n",
    "            'info_dict': info_dict\n",
    "        }\n",
    "    \n",
    "    def train_step(self,\n",
    "                   query_text: str,\n",
    "                   query_seed_entities: Dict[str, List[int]],\n",
    "                   question: str,\n",
    "                   ground_truth: torch.Tensor,\n",
    "                   answer_entities: Optional[Set[int]] = None) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Single training step.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with loss values\n",
    "        \"\"\"\n",
    "        # Forward pass\n",
    "        outputs = self.forward_pass(\n",
    "            query_text=query_text,\n",
    "            query_seed_entities=query_seed_entities,\n",
    "            question=question,\n",
    "            ground_truth=ground_truth,\n",
    "            answer_entities=answer_entities\n",
    "        )\n",
    "        \n",
    "        # Compute joint loss\n",
    "        total_loss, loss_dict = self.joint_loss(\n",
    "            llm_logits=outputs['llm_logits'],\n",
    "            ground_truth=ground_truth,\n",
    "            triplet_logits=outputs['triplet_logits'],\n",
    "            shortest_path_mask=outputs['shortest_path_mask'],\n",
    "            retrieved_mask=outputs['retrieved_mask'],\n",
    "            use_stop_gradient=True\n",
    "        )\n",
    "        \n",
    "        # Backward pass with separate optimizers\n",
    "        # 1. Update LLM (full gradient)\n",
    "        if self.llm_optimizer is not None:\n",
    "            self.llm_optimizer.zero_grad()\n",
    "        \n",
    "        # 2. Update Retriever (with stop-gradient on LLM)\n",
    "        self.retriever_optimizer.zero_grad()\n",
    "        \n",
    "        # Compute retriever loss separately (with stop-gradient)\n",
    "        if outputs['triplet_logits'] is not None:\n",
    "            # Retriever feedback loss\n",
    "            llm_probs = F.softmax(outputs['llm_logits'].detach(), dim=-1)\n",
    "            max_llm_prob = llm_probs.max(dim=-1)[0].mean()\n",
    "            triplet_probs = F.softmax(outputs['triplet_logits'], dim=-1)\n",
    "            combined_prob = max_llm_prob * triplet_probs.mean()\n",
    "            retriever_loss = -torch.log(combined_prob + 1e-10)\n",
    "            \n",
    "            # Graph supervision loss\n",
    "            if outputs['shortest_path_mask'] is not None:\n",
    "                graph_sup_loss = F.binary_cross_entropy_with_logits(\n",
    "                    outputs['retrieved_mask'],\n",
    "                    outputs['shortest_path_mask']\n",
    "                )\n",
    "                retriever_loss = retriever_loss + 0.1 * graph_sup_loss\n",
    "            \n",
    "            retriever_loss.backward(retain_graph=True)\n",
    "            self.retriever_optimizer.step()\n",
    "        \n",
    "        # LLM loss (accuracy loss)\n",
    "        accuracy_loss = loss_dict['accuracy_loss']\n",
    "        if self.llm_optimizer is not None and outputs['llm_logits'].requires_grad:\n",
    "            # Re-compute accuracy loss for LLM update\n",
    "            if outputs['llm_logits'].dim() == 3:\n",
    "                shift_logits = outputs['llm_logits'][..., :-1, :].contiguous()\n",
    "                shift_labels = ground_truth[..., 1:].contiguous()\n",
    "                llm_loss = F.cross_entropy(\n",
    "                    shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                    shift_labels.view(-1),\n",
    "                    ignore_index=-100\n",
    "                )\n",
    "            else:\n",
    "                llm_loss = F.cross_entropy(outputs['llm_logits'], ground_truth)\n",
    "            \n",
    "            llm_loss.backward()\n",
    "            self.llm_optimizer.step()\n",
    "        \n",
    "        return loss_dict\n",
    "\n",
    "print(\"GRILTrainer class defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# CHECKPOINTING AND TRAINING UTILITIES\n",
    "# =========================================\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def save_checkpoint(trainer: GRILTrainer,\n",
    "                   epoch: int,\n",
    "                   loss_history: List[float],\n",
    "                   checkpoint_dir: Path,\n",
    "                   best_loss: float = float('inf')):\n",
    "    \"\"\"Save training checkpoint.\"\"\"\n",
    "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'loss_history': loss_history,\n",
    "        'best_loss': best_loss,\n",
    "        'relevance_scorer_state': trainer.relevance_scorer.state_dict(),\n",
    "        'entity_updater_state': trainer.entity_updater.state_dict(),\n",
    "        'sag_pooling_state': trainer.sag_pooling.state_dict(),\n",
    "        'retriever_optimizer_state': trainer.retriever_optimizer.state_dict(),\n",
    "    }\n",
    "    \n",
    "    if trainer.llm_optimizer is not None:\n",
    "        checkpoint['llm_optimizer_state'] = trainer.llm_optimizer.state_dict()\n",
    "    \n",
    "    if trainer.llm.model is not None:\n",
    "        checkpoint['llm_state'] = trainer.llm.model.state_dict()\n",
    "    \n",
    "    # Save latest\n",
    "    torch.save(checkpoint, checkpoint_dir / 'latest_checkpoint.pt')\n",
    "    \n",
    "    # Save best\n",
    "    if loss_history and loss_history[-1] < best_loss:\n",
    "        torch.save(checkpoint, checkpoint_dir / 'best_checkpoint.pt')\n",
    "        return loss_history[-1]\n",
    "    \n",
    "    return best_loss\n",
    "\n",
    "def load_checkpoint(trainer: GRILTrainer, checkpoint_path: Path):\n",
    "    \"\"\"Load training checkpoint.\"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=trainer.device)\n",
    "    \n",
    "    trainer.relevance_scorer.load_state_dict(checkpoint['relevance_scorer_state'])\n",
    "    trainer.entity_updater.load_state_dict(checkpoint['entity_updater_state'])\n",
    "    trainer.sag_pooling.load_state_dict(checkpoint['sag_pooling_state'])\n",
    "    trainer.retriever_optimizer.load_state_dict(checkpoint['retriever_optimizer_state'])\n",
    "    \n",
    "    if 'llm_optimizer_state' in checkpoint and trainer.llm_optimizer is not None:\n",
    "        trainer.llm_optimizer.load_state_dict(checkpoint['llm_optimizer_state'])\n",
    "    \n",
    "    if 'llm_state' in checkpoint and trainer.llm.model is not None:\n",
    "        trainer.llm.model.load_state_dict(checkpoint['llm_state'])\n",
    "    \n",
    "    return checkpoint.get('epoch', 0), checkpoint.get('loss_history', []), checkpoint.get('best_loss', float('inf'))\n",
    "\n",
    "print(\"Checkpointing utilities defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Training Loop\n",
    "\n",
    "Full training loop with:\n",
    "- Epoch iteration\n",
    "- Loss tracking\n",
    "- Checkpointing\n",
    "- Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# COMPLETE TRAINING LOOP\n",
    "# =========================================\n",
    "def train_gril(trainer: GRILTrainer,\n",
    "               train_data: List[Dict],\n",
    "               val_data: List[Dict] = None,\n",
    "               num_epochs: int = 10,\n",
    "               checkpoint_dir: Path = None,\n",
    "               eval_interval: int = 1,\n",
    "               save_interval: int = 5):\n",
    "    \"\"\"\n",
    "    Complete training loop for GRIL.\n",
    "    \n",
    "    Args:\n",
    "        trainer: GRILTrainer instance\n",
    "        train_data: List of training examples, each with:\n",
    "                   - 'query_text': str\n",
    "                   - 'query_seed_entities': Dict[str, List[int]]\n",
    "                   - 'question': str\n",
    "                   - 'ground_truth': torch.Tensor (tokenized answer)\n",
    "                   - 'answer_entities': Optional[Set[int]] (for graph supervision)\n",
    "        val_data: Validation data (same format)\n",
    "        num_epochs: Number of training epochs\n",
    "        checkpoint_dir: Directory to save checkpoints\n",
    "        eval_interval: Evaluate every N epochs\n",
    "        save_interval: Save checkpoint every N epochs\n",
    "    \"\"\"\n",
    "    if checkpoint_dir is None:\n",
    "        checkpoint_dir = Path('checkpoints') / 'gril_training'\n",
    "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    loss_history = []\n",
    "    best_loss = float('inf')\n",
    "    start_epoch = 0\n",
    "    \n",
    "    # Try to load existing checkpoint\n",
    "    latest_checkpoint = checkpoint_dir / 'latest_checkpoint.pt'\n",
    "    if latest_checkpoint.exists():\n",
    "        print(f\"Loading checkpoint from {latest_checkpoint}\")\n",
    "        start_epoch, loss_history, best_loss = load_checkpoint(trainer, latest_checkpoint)\n",
    "        print(f\"Resuming from epoch {start_epoch}, best loss: {best_loss:.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Starting GRIL Training\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Training examples: {len(train_data)}\")\n",
    "    print(f\"Validation examples: {len(val_data) if val_data else 0}\")\n",
    "    print(f\"Epochs: {num_epochs}\")\n",
    "    print(f\"Device: {trainer.device}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Training mode\n",
    "        trainer.relevance_scorer.train()\n",
    "        trainer.entity_updater.train()\n",
    "        trainer.sag_pooling.train()\n",
    "        if trainer.llm.model is not None:\n",
    "            trainer.llm.model.train()\n",
    "        \n",
    "        epoch_losses = []\n",
    "        epoch_loss_components = {'accuracy_loss': [], 'retriever_feedback_loss': [], 'graph_supervision_loss': []}\n",
    "        \n",
    "        # Training loop\n",
    "        for i, example in enumerate(train_data):\n",
    "            try:\n",
    "                # Get data\n",
    "                query_text = example['query_text']\n",
    "                query_seed_entities = example['query_seed_entities']\n",
    "                question = example['question']\n",
    "                ground_truth = example['ground_truth'].to(trainer.device)\n",
    "                answer_entities = example.get('answer_entities', None)\n",
    "                \n",
    "                # Training step\n",
    "                loss_dict = trainer.train_step(\n",
    "                    query_text=query_text,\n",
    "                    query_seed_entities=query_seed_entities,\n",
    "                    question=question,\n",
    "                    ground_truth=ground_truth,\n",
    "                    answer_entities=answer_entities\n",
    "                )\n",
    "                \n",
    "                epoch_losses.append(loss_dict['total_loss'])\n",
    "                epoch_loss_components['accuracy_loss'].append(loss_dict.get('accuracy_loss', 0))\n",
    "                epoch_loss_components['retriever_feedback_loss'].append(loss_dict.get('retriever_feedback_loss', 0))\n",
    "                epoch_loss_components['graph_supervision_loss'].append(loss_dict.get('graph_supervision_loss', 0))\n",
    "                \n",
    "                # Print progress\n",
    "                if (i + 1) % 10 == 0:\n",
    "                    avg_loss = np.mean(epoch_losses[-10:])\n",
    "                    print(f\"  Step {i+1}/{len(train_data)}: Loss = {avg_loss:.4f}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"  Error in step {i+1}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Epoch statistics\n",
    "        avg_epoch_loss = np.mean(epoch_losses) if epoch_losses else 0.0\n",
    "        loss_history.append(avg_epoch_loss)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "        print(f\"  Average Loss: {avg_epoch_loss:.4f}\")\n",
    "        if epoch_losses:\n",
    "            print(f\"  Accuracy Loss: {np.mean(epoch_loss_components['accuracy_loss']):.4f}\")\n",
    "            print(f\"  Retriever Feedback Loss: {np.mean(epoch_loss_components['retriever_feedback_loss']):.4f}\")\n",
    "            print(f\"  Graph Supervision Loss: {np.mean(epoch_loss_components['graph_supervision_loss']):.4f}\")\n",
    "        \n",
    "        # Evaluation\n",
    "        if val_data and (epoch + 1) % eval_interval == 0:\n",
    "            print(f\"\\nEvaluating on validation set...\")\n",
    "            val_loss = evaluate_gril(trainer, val_data)\n",
    "            print(f\"  Validation Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (epoch + 1) % save_interval == 0:\n",
    "            best_loss = save_checkpoint(trainer, epoch + 1, loss_history, checkpoint_dir, best_loss)\n",
    "            print(f\"  Checkpoint saved (best loss: {best_loss:.4f})\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # Final checkpoint\n",
    "    best_loss = save_checkpoint(trainer, num_epochs, loss_history, checkpoint_dir, best_loss)\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Training Complete!\")\n",
    "    print(f\"Final best loss: {best_loss:.4f}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "def evaluate_gril(trainer: GRILTrainer, val_data: List[Dict]) -> float:\n",
    "    \"\"\"Evaluate GRIL on validation data.\"\"\"\n",
    "    trainer.relevance_scorer.eval()\n",
    "    trainer.entity_updater.eval()\n",
    "    trainer.sag_pooling.eval()\n",
    "    if trainer.llm.model is not None:\n",
    "        trainer.llm.model.eval()\n",
    "    \n",
    "    val_losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for example in val_data:\n",
    "            try:\n",
    "                query_text = example['query_text']\n",
    "                query_seed_entities = example['query_seed_entities']\n",
    "                question = example['question']\n",
    "                ground_truth = example['ground_truth'].to(trainer.device)\n",
    "                answer_entities = example.get('answer_entities', None)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = trainer.forward_pass(\n",
    "                    query_text=query_text,\n",
    "                    query_seed_entities=query_seed_entities,\n",
    "                    question=question,\n",
    "                    ground_truth=ground_truth,\n",
    "                    answer_entities=answer_entities\n",
    "                )\n",
    "                \n",
    "                # Compute loss\n",
    "                _, loss_dict = trainer.joint_loss(\n",
    "                    llm_logits=outputs['llm_logits'],\n",
    "                    ground_truth=ground_truth,\n",
    "                    triplet_logits=outputs['triplet_logits'],\n",
    "                    shortest_path_mask=outputs['shortest_path_mask'],\n",
    "                    retrieved_mask=outputs['retrieved_mask'],\n",
    "                    use_stop_gradient=False  # No gradients needed for eval\n",
    "                )\n",
    "                \n",
    "                val_losses.append(loss_dict['total_loss'])\n",
    "            \n",
    "            except Exception as e:\n",
    "                continue\n",
    "    \n",
    "    return np.mean(val_losses) if val_losses else float('inf')\n",
    "\n",
    "print(\"Training loop functions defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Complete GRIL Implementation\n",
    "\n",
    "**All components are now implemented:**\n",
    "\n",
    "- **Algorithm 1**: Enhanced with Entity Updates (Eq. 4) and Gumbel-Softmax (Eq. 5)\n",
    "- **Algorithm 3**: SAG Pooling Layer for graph token generation\n",
    "- **Verbalization**: Triple-to-text conversion\n",
    "- **LLM Integration**: Llama3-8B with LoRA fine-tuning\n",
    "- **Algorithm 4**: Joint Training Framework with stop-gradient\n",
    "- **Algorithm 5**: Graph Supervision (shortest path extraction)\n",
    "- **Training Loop**: Complete with checkpointing and evaluation\n",
    "\n",
    "**Ready for end-to-end training!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup Guide\n",
    "\n",
    "This section shows how to prepare data and start training the GRIL model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# PREPARE TRAINING DATA FOR GRIL\n",
    "# =========================================\n",
    "from transformers import AutoTokenizer\n",
    "from typing import List, Dict, Set, Optional\n",
    "\n",
    "def prepare_gril_training_data(\n",
    "    raw_train_data: List[Dict],\n",
    "    all_df: 'pd.DataFrame',\n",
    "    paper2idx: Dict[str, int],\n",
    "    tokenizer: AutoTokenizer = None,\n",
    "    max_answer_length: int = 100,\n",
    "    include_answer_entities: bool = False\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Convert raw training data to GRIL training format.\n",
    "    \n",
    "    Args:\n",
    "        raw_train_data: List of dicts with keys like 'publication_ID', 'title', 'abstract', 'Citations'\n",
    "        all_df: DataFrame with paper metadata\n",
    "        paper2idx: Mapping from paper ID to index\n",
    "        tokenizer: Tokenizer for encoding answers (if None, will create simple token IDs)\n",
    "        max_answer_length: Maximum length for answer tokens\n",
    "        include_answer_entities: Whether to extract answer entities for graph supervision\n",
    "    \n",
    "    Returns:\n",
    "        List of training examples in GRIL format:\n",
    "        {\n",
    "            'query_text': str,\n",
    "            'query_seed_entities': Dict[str, List[int]],\n",
    "            'question': str,\n",
    "            'ground_truth': torch.Tensor,\n",
    "            'answer_entities': Optional[Set[int]]\n",
    "        }\n",
    "    \"\"\"\n",
    "    gril_data = []\n",
    "    \n",
    "    # Initialize tokenizer if not provided\n",
    "    if tokenizer is None:\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "        except:\n",
    "            print(\"Warning: Could not load tokenizer. Using simple token IDs.\")\n",
    "            tokenizer = None\n",
    "    \n",
    "    for item in raw_train_data:\n",
    "        try:\n",
    "            # Get query paper info\n",
    "            paper_id = str(item.get('publication_ID', ''))\n",
    "            if paper_id not in paper2idx:\n",
    "                continue\n",
    "            \n",
    "            query_paper_idx = paper2idx[paper_id]\n",
    "            \n",
    "            # Get query text (title + abstract)\n",
    "            query_row = all_df[all_df['publication_ID'] == int(paper_id)]\n",
    "            if len(query_row) == 0:\n",
    "                continue\n",
    "            \n",
    "            query_text = query_row['text'].values[0] if 'text' in query_row.columns else ''\n",
    "            if not query_text or len(query_text) < 10:\n",
    "                continue\n",
    "            \n",
    "            # Create question\n",
    "            title = item.get('title', 'this paper')\n",
    "            question = f\"What papers are related to: {title}?\"\n",
    "            \n",
    "            # Get answer (citations)\n",
    "            citations = item.get('Citations', [])\n",
    "            if not citations:\n",
    "                # If no citations, use empty answer\n",
    "                answer_text = \"No related papers found.\"\n",
    "            else:\n",
    "                # Create answer from citations\n",
    "                citation_titles = []\n",
    "                for cit_id in citations[:5]:  # Limit to 5 citations\n",
    "                    cit_row = all_df[all_df['publication_ID'] == int(cit_id)]\n",
    "                    if len(cit_row) > 0 and 'title' in cit_row.columns:\n",
    "                        citation_titles.append(cit_row['title'].values[0])\n",
    "                \n",
    "                if citation_titles:\n",
    "                    answer_text = \"Related papers: \" + \"; \".join(citation_titles)\n",
    "                else:\n",
    "                    answer_text = \"No related papers found.\"\n",
    "            \n",
    "            # Tokenize answer\n",
    "            if tokenizer is not None:\n",
    "                answer_tokens = tokenizer(\n",
    "                    answer_text,\n",
    "                    return_tensors='pt',\n",
    "                    padding='max_length',\n",
    "                    truncation=True,\n",
    "                    max_length=max_answer_length\n",
    "                )['input_ids'].squeeze(0)\n",
    "            else:\n",
    "                # Simple token IDs (just use character codes)\n",
    "                answer_tokens = torch.tensor([ord(c) % 1000 for c in answer_text[:max_answer_length]], dtype=torch.long)\n",
    "                if len(answer_tokens) < max_answer_length:\n",
    "                    answer_tokens = torch.cat([answer_tokens, torch.zeros(max_answer_length - len(answer_tokens), dtype=torch.long)])\n",
    "            \n",
    "            # Extract answer entities for graph supervision (optional)\n",
    "            answer_entities = None\n",
    "            if include_answer_entities and citations:\n",
    "                answer_entities = set()\n",
    "                for cit_id in citations:\n",
    "                    cit_str = str(cit_id)\n",
    "                    if cit_str in paper2idx:\n",
    "                        answer_entities.add(paper2idx[cit_str])\n",
    "            \n",
    "            # Create GRIL training example\n",
    "            gril_example = {\n",
    "                'query_text': query_text,\n",
    "                'query_seed_entities': {'paper': [query_paper_idx]},\n",
    "                'question': question,\n",
    "                'ground_truth': answer_tokens,\n",
    "                'answer_entities': answer_entities\n",
    "            }\n",
    "            \n",
    "            gril_data.append(gril_example)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing item: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Prepared {len(gril_data)} training examples from {len(raw_train_data)} raw examples\")\n",
    "    return gril_data\n",
    "\n",
    "print(\"Training data preparation function defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize GRIL Trainer\n",
    "\n",
    "Set up all components and create the trainer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# INITIALIZE GRIL TRAINER\n",
    "# =========================================\n",
    "\n",
    "# Check that all required components are available\n",
    "required_components = {\n",
    "    'model': model,\n",
    "    'data': data,\n",
    "    'query_encoder': query_encoder,\n",
    "    'relevance_scorer': relevance_scorer,\n",
    "    'entity_updater': entity_updater,\n",
    "    'sag_pooling': sag_pooling,\n",
    "    'all_df': all_df,\n",
    "    'paper2idx': paper2idx,\n",
    "    'author2idx': author2idx,\n",
    "    'keyword2idx': keyword2idx,\n",
    "    'venue2idx': venue2idx,\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "missing = [k for k, v in required_components.items() if v is None or k not in globals()]\n",
    "if missing:\n",
    "    print(f\"[ERROR] Missing components: {missing}\")\n",
    "    print(\"Please run all previous cells to initialize components.\")\n",
    "else:\n",
    "    print(\"All required components available\")\n",
    "    \n",
    "    # Initialize LLM (optional - can train without it using mock outputs)\n",
    "    print(\"\\nInitializing LLM...\")\n",
    "    try:\n",
    "        llm = GRIL_LLM(\n",
    "            model_name=\"meta-llama/Meta-Llama-3-8B\",\n",
    "            use_4bit=True,\n",
    "            lora_rank=8,\n",
    "            device=device\n",
    "        )\n",
    "        if llm.model is None:\n",
    "            print(\"LLM not loaded. Training will use mock LLM outputs.\")\n",
    "        else:\n",
    "            print(\"LLM loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load LLM: {e}\")\n",
    "        print(\"Training will use mock LLM outputs.\")\n",
    "        llm = GRIL_LLM(model_name=None, device=device)  # Mock LLM\n",
    "    \n",
    "    # Initialize joint loss\n",
    "    joint_loss = JointTrainingLoss(alpha=1.0, beta=1.0, gamma=0.1)\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = GRILTrainer(\n",
    "        gnn_model=model,\n",
    "        query_encoder=query_encoder,\n",
    "        relevance_scorer=relevance_scorer,\n",
    "        entity_updater=entity_updater,\n",
    "        sag_pooling=sag_pooling,\n",
    "        llm=llm,\n",
    "        full_data=data,\n",
    "        joint_loss=joint_loss,\n",
    "        all_df=all_df,\n",
    "        paper2idx=paper2idx,\n",
    "        author2idx=author2idx,\n",
    "        keyword2idx=keyword2idx,\n",
    "        venue2idx=venue2idx,\n",
    "        retriever_lr=1e-4,\n",
    "        llm_lr=1e-5,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    print(\"\\nGRIL Trainer initialized and ready for training!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data\n",
    "\n",
    "Convert raw training data to GRIL format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# PREPARE TRAINING AND VALIDATION DATA\n",
    "# =========================================\n",
    "\n",
    "# Use a subset for faster training (adjust as needed)\n",
    "TRAIN_SUBSET_SIZE = 100  # Use first 100 examples for quick testing\n",
    "VAL_SUBSET_SIZE = 20     # Use first 20 examples for validation\n",
    "\n",
    "print(\"Preparing training data...\")\n",
    "gril_train_data = prepare_gril_training_data(\n",
    "    raw_train_data=train_data[:TRAIN_SUBSET_SIZE],\n",
    "    all_df=all_df,\n",
    "    paper2idx=paper2idx,\n",
    "    tokenizer=None,  # Will use simple token IDs\n",
    "    max_answer_length=50,\n",
    "    include_answer_entities=False  # Set to True if you have citation data\n",
    ")\n",
    "\n",
    "print(\"\\nPreparing validation data...\")\n",
    "gril_val_data = prepare_gril_training_data(\n",
    "    raw_train_data=val_data[:VAL_SUBSET_SIZE],\n",
    "    all_df=all_df,\n",
    "    paper2idx=paper2idx,\n",
    "    tokenizer=None,\n",
    "    max_answer_length=50,\n",
    "    include_answer_entities=False\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining data: {len(gril_train_data)} examples\")\n",
    "print(f\"Validation data: {len(gril_val_data)} examples\")\n",
    "print(\"\\nSample training example:\")\n",
    "if gril_train_data:\n",
    "    sample = gril_train_data[0]\n",
    "    print(f\"  Query text length: {len(sample['query_text'])} chars\")\n",
    "    print(f\"  Seed entities: {sample['query_seed_entities']}\")\n",
    "    print(f\"  Question: {sample['question']}\")\n",
    "    print(f\"  Ground truth shape: {sample['ground_truth'].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training\n",
    "\n",
    "Run the training loop. Adjust parameters as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# START GRIL TRAINING\n",
    "# =========================================\n",
    "\n",
    "# Training parameters\n",
    "NUM_EPOCHS = 5           # Number of training epochs\n",
    "EVAL_INTERVAL = 1        # Evaluate every N epochs\n",
    "SAVE_INTERVAL = 2        # Save checkpoint every N epochs\n",
    "CHECKPOINT_DIR = Path('checkpoints') / 'gril_training'\n",
    "\n",
    "print(\"Starting GRIL training...\")\n",
    "print(f\"Training examples: {len(gril_train_data)}\")\n",
    "print(f\"Validation examples: {len(gril_val_data)}\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(\"\\nNote: This may take a while. Each example runs the full pipeline:\")\n",
    "print(\"  Algorithm 1 → SAG Pooling → Verbalization → LLM\")\n",
    "\n",
    "# Start training\n",
    "train_gril(\n",
    "    trainer=trainer,\n",
    "    train_data=gril_train_data,\n",
    "    val_data=gril_val_data,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    checkpoint_dir=CHECKPOINT_DIR,\n",
    "    eval_interval=EVAL_INTERVAL,\n",
    "    save_interval=SAVE_INTERVAL\n",
    ")\n",
    "\n",
    "print(\"\\nTraining complete! Checkpoints saved in:\", CHECKPOINT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Instructions Summary\n",
    "\n",
    "**To train the GRIL model, follow these steps:**\n",
    "\n",
    "1. **Run all previous cells** to initialize:\n",
    "   - GNN model (pre-trained)\n",
    "   - Graph data and mappings\n",
    "   - All GRIL components (Algorithm 1, SAG, LLM, etc.)\n",
    "\n",
    "2. **Run the \"Initialize GRIL Trainer\" cell** above to create the trainer\n",
    "\n",
    "3. **Run the \"Prepare Training Data\" cell** to convert your data to GRIL format\n",
    "\n",
    "4. **Run the \"Start Training\" cell** to begin training\n",
    "\n",
    "**Training Parameters (adjust as needed):**\n",
    "- `NUM_EPOCHS`: Number of training epochs (default: 5)\n",
    "- `TRAIN_SUBSET_SIZE`: Number of training examples (default: 100 for testing)\n",
    "- `VAL_SUBSET_SIZE`: Number of validation examples (default: 20)\n",
    "- `retriever_lr`: Learning rate for retriever (default: 1e-4)\n",
    "- `llm_lr`: Learning rate for LLM (default: 1e-5)\n",
    "\n",
    "**Checkpoints:**\n",
    "- Saved in `checkpoints/gril_training/`\n",
    "- `latest_checkpoint.pt`: Most recent checkpoint\n",
    "- `best_checkpoint.pt`: Best model (lowest loss)\n",
    "\n",
    "**Note:** Training can be slow because each example runs the full pipeline. Start with a small subset to test, then scale up.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "017455a50103431abe967ce3edd08634": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "02555f1425ab46b8baa9e22850916f4d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0cd24ded990047c8ae343b8880533320",
      "max": 112,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5b63ec87dc9943d497afe1c8b46b99c9",
      "value": 112
     }
    },
    "0360e5b4587d498ca89b86ff2e2627eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1937e59c1cc544dea15e8be6e9ec1ca1",
       "IPY_MODEL_fc99d2bf92a2458eac7fb4980cd5f757",
       "IPY_MODEL_3be7f5eaca0544388784367f2ce1a060"
      ],
      "layout": "IPY_MODEL_6a9bba08586b494694abab655f49206c"
     }
    },
    "067c551f00aa4990a09cb142a00f7e95": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0cb8cb1304e444a9acd83297793c0f4c",
      "placeholder": "​",
      "style": "IPY_MODEL_47742bb797fb42ad956c3a2cdbea641c",
      "value": " 190/190 [00:00&lt;00:00, 21.3kB/s]"
     }
    },
    "06991822b23d417496e937f977d45eee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0aa9b9cbccda4016949857b1274affc7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0af323a9de65474b848871745f3b25bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0b6c70e751cf457ea318988e335e61c4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0cb8cb1304e444a9acd83297793c0f4c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0cd24ded990047c8ae343b8880533320": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0cf3809d637342a984191c9b50596e1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fd902d39b5b54f8980c3e04229fcab6b",
      "max": 612,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4d4e4172637142289c8d73894ddb9f74",
      "value": 612
     }
    },
    "0e156ad0bafc491da0f24570a8216c86": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "11a3b79c2d5d46968ae3c81a790f6fb7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_686e2ecbc65d4050b74710b68c5fef6a",
       "IPY_MODEL_aa701f7b487640fdbb063f02a3f49ec5",
       "IPY_MODEL_939e1a0773b249c18bc7b033a828733d"
      ],
      "layout": "IPY_MODEL_5b23b0c34399491aa548b0a040b7d868"
     }
    },
    "1439582734794fa68ba068ca39e94914": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "157b9cc8da2f4236bb40a63cffee6f1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1937e59c1cc544dea15e8be6e9ec1ca1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_72a54e97a5054b709bcbf72f4dd85fe3",
      "placeholder": "​",
      "style": "IPY_MODEL_ab619af6632945939eebf8be0338b8e0",
      "value": "modules.json: 100%"
     }
    },
    "19d93a1f31714989bf347beba37d22e1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1df0a564094142a8992835bdc0ba74d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_891ee13ba6ec4f81a78e5b8e67aba3f3",
      "max": 350,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5101f6c585da4e7db8e5e548f0d9ec01",
      "value": 350
     }
    },
    "1f95847ea47e4f0b852366d5d599d62e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_64a5b02f63b24a2b8cdebb7a519df606",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6a05296b063e4fc890c82e9d56446e85",
      "value": 1
     }
    },
    "1f97fc92dfcc436ebad18f9e1d4f016a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_24182075e4974186a6bed5072f67601c",
      "placeholder": "​",
      "style": "IPY_MODEL_4e845b095fa04a8e8f5017af6f1fbee4",
      "value": "config_sentence_transformers.json: 100%"
     }
    },
    "20aadd9f73464590ad8eb87f9b3830f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0e156ad0bafc491da0f24570a8216c86",
      "placeholder": "​",
      "style": "IPY_MODEL_4c8fe2e1058945fb80dd41cf4eb5df49",
      "value": " 10.5k/? [00:00&lt;00:00, 1.22MB/s]"
     }
    },
    "24182075e4974186a6bed5072f67601c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "24f5e74118dc40d5a6f0371a447b8bce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "25c406ed33ed4cf49116cb20ddb50256": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fe2e3ea421364debbc32539439311f8f",
      "max": 116,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3e7d07734800439c9798b91545b94004",
      "value": 116
     }
    },
    "29019e98cfbc46ef99417ae67b7ced63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b4ea04a3bdb745b1941d3d1572c974c0",
       "IPY_MODEL_02555f1425ab46b8baa9e22850916f4d",
       "IPY_MODEL_8532ab20f3164335b698ce83849e773b"
      ],
      "layout": "IPY_MODEL_67e708f3806a4d15aaeeb6ed5fdf2e2a"
     }
    },
    "2b248e547bbb4b628f9db01278ec77e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "30a9eeefa4f9475393dfcce81dd8e67b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33a12990d5614c688524a08e8c1f7db7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7ce1faec9e74445384c9e7b1de703fca",
       "IPY_MODEL_d7cfac6c24834ebbb533b8ca430d6a45",
       "IPY_MODEL_067c551f00aa4990a09cb142a00f7e95"
      ],
      "layout": "IPY_MODEL_d8b3611eb2ec41d18c0c8ff2cfa3d047"
     }
    },
    "357ba5868921480684827c0a1ea2dd30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_971fd59a043946588ebe2b6de97a8826",
      "placeholder": "​",
      "style": "IPY_MODEL_7975159d86fb4893a9151e72fb5bbc9d",
      "value": " 53.0/53.0 [00:00&lt;00:00, 6.97kB/s]"
     }
    },
    "358f31c9a08247c29827809f618d0430": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "36eac627bd2544f48fb79cce1e21228f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "37887e11ef214661bd6f74a32766274b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "38355623054d4841851240b8c70d3974": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4767f185a682488b82304a7613a43aa9",
       "IPY_MODEL_4ab9f52cb4a241a6a00f5ea5d3e5de45",
       "IPY_MODEL_913c20aadf9e41ec8c315f68edca9a9c"
      ],
      "layout": "IPY_MODEL_ab4c744c07e74f4a99839d6fa2eaa6b0"
     }
    },
    "3be7f5eaca0544388784367f2ce1a060": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_37887e11ef214661bd6f74a32766274b",
      "placeholder": "​",
      "style": "IPY_MODEL_a1056d170b63463db495247c0679851f",
      "value": " 349/349 [00:00&lt;00:00, 44.0kB/s]"
     }
    },
    "3e523b4a8d59446cb216b324bef1d013": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9e79df59bb11405393af04047f9a36fe",
      "placeholder": "​",
      "style": "IPY_MODEL_2b248e547bbb4b628f9db01278ec77e6",
      "value": "config.json: 100%"
     }
    },
    "3e7d07734800439c9798b91545b94004": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3f42509651084f00b9496ce73f800248": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3f7d30cdae054944ab41b17f9cf3f8e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "40aae11b392142f2abf6b38e1d06e6dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "41d096868a964d0cab765f0326d8d4c1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "429ebf9726104edda7d71ca248d16667": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "44958190405b4ba6b0ff0ff7302a3e4d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bcabc31828684b17b882ec17978d45c9",
       "IPY_MODEL_1f95847ea47e4f0b852366d5d599d62e",
       "IPY_MODEL_453eea4d42b842f1b9fea4ecd3b6904d"
      ],
      "layout": "IPY_MODEL_ddfb5aab135945f1bccfb624d794dff7"
     }
    },
    "453eea4d42b842f1b9fea4ecd3b6904d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c0276f556e424d3187b32e271dcec9f8",
      "placeholder": "​",
      "style": "IPY_MODEL_bf83fe16987243d39854e5a8e30aa6da",
      "value": " 466k/? [00:00&lt;00:00, 35.7MB/s]"
     }
    },
    "460e13836be34983bd59a6539147d614": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4767f185a682488b82304a7613a43aa9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7d8bd1eb3cf640d69c332b52bb21e71b",
      "placeholder": "​",
      "style": "IPY_MODEL_4d089500a48f49d78fe1b247e222d0cf",
      "value": "vocab.txt: "
     }
    },
    "47742bb797fb42ad956c3a2cdbea641c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4ab9f52cb4a241a6a00f5ea5d3e5de45": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6e6a0a87b09e44e0abb1ce7b3cc9ae38",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_859d78a445264529979b61a291283f70",
      "value": 1
     }
    },
    "4c8fe2e1058945fb80dd41cf4eb5df49": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4d089500a48f49d78fe1b247e222d0cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4d4e4172637142289c8d73894ddb9f74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4e845b095fa04a8e8f5017af6f1fbee4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5101f6c585da4e7db8e5e548f0d9ec01": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5527913dc1d644bc8b00ec1b7f0a17f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fd5d944f1dde4da28bfe38bd36780fe2",
       "IPY_MODEL_569cbef7cc634d72875ef40731022e83",
       "IPY_MODEL_357ba5868921480684827c0a1ea2dd30"
      ],
      "layout": "IPY_MODEL_017455a50103431abe967ce3edd08634"
     }
    },
    "569cbef7cc634d72875ef40731022e83": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_61d1ecc3c8a44351b7c25174c487cbf7",
      "max": 53,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f5bdbe6669ec43c08b70e34b1a01c430",
      "value": 53
     }
    },
    "58ef2ad1f1004bfbb1e2c15b25818de8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5b23b0c34399491aa548b0a040b7d868": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b63ec87dc9943d497afe1c8b46b99c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5b9fed553c5b4c0d83ff80a09d048a19": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "614c75ab42f44852aa81a85b2236c92c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_24f5e74118dc40d5a6f0371a447b8bce",
      "placeholder": "​",
      "style": "IPY_MODEL_c6cf4ef1267d47d09e2d20d9894e63fb",
      "value": " 116/116 [00:00&lt;00:00, 14.9kB/s]"
     }
    },
    "61d1ecc3c8a44351b7c25174c487cbf7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "61fd545ee64545c3b19aa462d1dfd3e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "640aa67869574234acd8ec7655bb9ce2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6417d9300f894bbd91f9561755cee093": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "64a5b02f63b24a2b8cdebb7a519df606": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "65afb9893b9d4feca7456726dc890ba1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "65fe6277c7bd4911b779b9d3eae1cb80": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_460e13836be34983bd59a6539147d614",
      "placeholder": "​",
      "style": "IPY_MODEL_640aa67869574234acd8ec7655bb9ce2",
      "value": " 350/350 [00:00&lt;00:00, 47.3kB/s]"
     }
    },
    "67e708f3806a4d15aaeeb6ed5fdf2e2a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "686e2ecbc65d4050b74710b68c5fef6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a446cbe7ec7042c885433d78ac42948b",
      "placeholder": "​",
      "style": "IPY_MODEL_977accd634cd479f89e0800da49bae1d",
      "value": "model.safetensors: 100%"
     }
    },
    "6a05296b063e4fc890c82e9d56446e85": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6a4c68b9acc84133bab07a8fbf33b4e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f3fca04e4dd44c97ad4312fdec3f5884",
       "IPY_MODEL_d4cb152409894f6aa39434bfa1f6c3de",
       "IPY_MODEL_79bf615cfdff43eda64c409bd547f375"
      ],
      "layout": "IPY_MODEL_358f31c9a08247c29827809f618d0430"
     }
    },
    "6a9bba08586b494694abab655f49206c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6e6a0a87b09e44e0abb1ce7b3cc9ae38": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "72a54e97a5054b709bcbf72f4dd85fe3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7908bdd58499455bbf3bdaa064333370": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7975159d86fb4893a9151e72fb5bbc9d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "79bf615cfdff43eda64c409bd547f375": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7908bdd58499455bbf3bdaa064333370",
      "placeholder": "​",
      "style": "IPY_MODEL_157b9cc8da2f4236bb40a63cffee6f1e",
      "value": " 6630/6630 [01:28&lt;00:00, 141.09it/s]"
     }
    },
    "7ce1faec9e74445384c9e7b1de703fca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_429ebf9726104edda7d71ca248d16667",
      "placeholder": "​",
      "style": "IPY_MODEL_61fd545ee64545c3b19aa462d1dfd3e6",
      "value": "config.json: 100%"
     }
    },
    "7d8bd1eb3cf640d69c332b52bb21e71b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7ece3b1aeecb46df8d295f2a2fcb4c1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_df7ebab2501d454583a478dd79f5cb89",
       "IPY_MODEL_1df0a564094142a8992835bdc0ba74d1",
       "IPY_MODEL_65fe6277c7bd4911b779b9d3eae1cb80"
      ],
      "layout": "IPY_MODEL_5b9fed553c5b4c0d83ff80a09d048a19"
     }
    },
    "7f726a871fcf40a78160fe8e3964c063": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_30a9eeefa4f9475393dfcce81dd8e67b",
      "placeholder": "​",
      "style": "IPY_MODEL_06991822b23d417496e937f977d45eee",
      "value": "README.md: "
     }
    },
    "828a177687cd41ea913f43852be16943": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8405d2700fe748d183cf8df08cab2f50": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8532ab20f3164335b698ce83849e773b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6417d9300f894bbd91f9561755cee093",
      "placeholder": "​",
      "style": "IPY_MODEL_a0ad946dc86e4d159361dc78f6987e6c",
      "value": " 112/112 [00:00&lt;00:00, 14.4kB/s]"
     }
    },
    "859d78a445264529979b61a291283f70": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "891ee13ba6ec4f81a78e5b8e67aba3f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "913c20aadf9e41ec8c315f68edca9a9c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9d01ac43fc0c4888bfeed063e73e1df1",
      "placeholder": "​",
      "style": "IPY_MODEL_0aa9b9cbccda4016949857b1274affc7",
      "value": " 232k/? [00:00&lt;00:00, 20.3MB/s]"
     }
    },
    "91d13cc582be4b988fd958bea1450329": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3e523b4a8d59446cb216b324bef1d013",
       "IPY_MODEL_0cf3809d637342a984191c9b50596e1d",
       "IPY_MODEL_ebf22edcfced42a695ed615d42ada934"
      ],
      "layout": "IPY_MODEL_e70be053fa334155879e66718c90339a"
     }
    },
    "939e1a0773b249c18bc7b033a828733d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_df289a4a4aeb41e4922f98d2677eba00",
      "placeholder": "​",
      "style": "IPY_MODEL_9ea91c6d49b344189431e2602904a375",
      "value": " 90.9M/90.9M [00:01&lt;00:00, 407kB/s]"
     }
    },
    "971fd59a043946588ebe2b6de97a8826": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "977accd634cd479f89e0800da49bae1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "984531e0cc5f4125b5464805e8976cc5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9d01ac43fc0c4888bfeed063e73e1df1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9e79df59bb11405393af04047f9a36fe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9ea91c6d49b344189431e2602904a375": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a0ad946dc86e4d159361dc78f6987e6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a1056d170b63463db495247c0679851f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a446cbe7ec7042c885433d78ac42948b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a4d93ac76a6a401e99d8ef262a908c37": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aa701f7b487640fdbb063f02a3f49ec5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_40aae11b392142f2abf6b38e1d06e6dd",
      "max": 90868376,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3f7d30cdae054944ab41b17f9cf3f8e6",
      "value": 90868376
     }
    },
    "ab4c744c07e74f4a99839d6fa2eaa6b0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ab619af6632945939eebf8be0338b8e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b4341b18cee846bcb73ca665705464cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_65afb9893b9d4feca7456726dc890ba1",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b73c8ac3c072401dbdd65fcea79b9af4",
      "value": 1
     }
    },
    "b4ea04a3bdb745b1941d3d1572c974c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a4d93ac76a6a401e99d8ef262a908c37",
      "placeholder": "​",
      "style": "IPY_MODEL_58ef2ad1f1004bfbb1e2c15b25818de8",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "b73c8ac3c072401dbdd65fcea79b9af4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b837acaa30664304ae8786d38d794c84": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bcabc31828684b17b882ec17978d45c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_41d096868a964d0cab765f0326d8d4c1",
      "placeholder": "​",
      "style": "IPY_MODEL_0af323a9de65474b848871745f3b25bf",
      "value": "tokenizer.json: "
     }
    },
    "bf83fe16987243d39854e5a8e30aa6da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c0276f556e424d3187b32e271dcec9f8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c6cf4ef1267d47d09e2d20d9894e63fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ce226ae29fe148808b9b7ecb2858fa2c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d4cb152409894f6aa39434bfa1f6c3de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce226ae29fe148808b9b7ecb2858fa2c",
      "max": 6630,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_36eac627bd2544f48fb79cce1e21228f",
      "value": 6630
     }
    },
    "d714eaa9296d461ab37f0e3e69c65bc0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7f726a871fcf40a78160fe8e3964c063",
       "IPY_MODEL_b4341b18cee846bcb73ca665705464cf",
       "IPY_MODEL_20aadd9f73464590ad8eb87f9b3830f8"
      ],
      "layout": "IPY_MODEL_0b6c70e751cf457ea318988e335e61c4"
     }
    },
    "d7cfac6c24834ebbb533b8ca430d6a45": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e82ea687032b4778b87cc9e6037fe0ac",
      "max": 190,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1439582734794fa68ba068ca39e94914",
      "value": 190
     }
    },
    "d8b3611eb2ec41d18c0c8ff2cfa3d047": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ddfb5aab135945f1bccfb624d794dff7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "df289a4a4aeb41e4922f98d2677eba00": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "df7ebab2501d454583a478dd79f5cb89": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e868b25a6a9b43b0a8aecdef8794006c",
      "placeholder": "​",
      "style": "IPY_MODEL_ff5d1103c2e1427a87183a9fbb6f6f0b",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "e3ffc470112c4727991e2a8bb004dd0f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e70be053fa334155879e66718c90339a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e82ea687032b4778b87cc9e6037fe0ac": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e868b25a6a9b43b0a8aecdef8794006c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eb91300c22514b27b5350e6808460b52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1f97fc92dfcc436ebad18f9e1d4f016a",
       "IPY_MODEL_25c406ed33ed4cf49116cb20ddb50256",
       "IPY_MODEL_614c75ab42f44852aa81a85b2236c92c"
      ],
      "layout": "IPY_MODEL_b837acaa30664304ae8786d38d794c84"
     }
    },
    "ebf22edcfced42a695ed615d42ada934": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_19d93a1f31714989bf347beba37d22e1",
      "placeholder": "​",
      "style": "IPY_MODEL_8405d2700fe748d183cf8df08cab2f50",
      "value": " 612/612 [00:00&lt;00:00, 72.2kB/s]"
     }
    },
    "f3fca04e4dd44c97ad4312fdec3f5884": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fcdb2ebed9ce4da284d429dd54e74cde",
      "placeholder": "​",
      "style": "IPY_MODEL_828a177687cd41ea913f43852be16943",
      "value": "Batches: 100%"
     }
    },
    "f5bdbe6669ec43c08b70e34b1a01c430": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fb21fa57fba941dda7d460f27343d7dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fc99d2bf92a2458eac7fb4980cd5f757": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e3ffc470112c4727991e2a8bb004dd0f",
      "max": 349,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fb21fa57fba941dda7d460f27343d7dd",
      "value": 349
     }
    },
    "fcdb2ebed9ce4da284d429dd54e74cde": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fd5d944f1dde4da28bfe38bd36780fe2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_984531e0cc5f4125b5464805e8976cc5",
      "placeholder": "​",
      "style": "IPY_MODEL_3f42509651084f00b9496ce73f800248",
      "value": "sentence_bert_config.json: 100%"
     }
    },
    "fd902d39b5b54f8980c3e04229fcab6b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fe2e3ea421364debbc32539439311f8f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff5d1103c2e1427a87183a9fbb6f6f0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
